INFO:root:the deviced being used is cpu
INFO:root:Start validation
INFO:root:statistics used: {'mean': tensor([0.3073, 0.0613, 0.1427]), 'std': tensor([0.1653, 0.1623, 0.1262])}
INFO:root:train dataset: 68201, test dataset: 6454
INFO:root:used only channels: []; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.04876486
INFO:root:[1,   100] training loss: 0.04781373
INFO:root:[1,   150] training loss: 0.03675491
INFO:root:[1,   200] training loss: 0.03423247
INFO:root:[1,   250] training loss: 0.03270832
INFO:root:[1,   300] training loss: 0.03118324
INFO:root:[1,   350] training loss: 0.03268123
INFO:root:[1,   400] training loss: 0.00251825
INFO:root:[1,   450] training loss: 0.00016410
INFO:root:[1,   500] training loss: 0.00803000
INFO:root:[1,   550] training loss: 0.00690066
INFO:root:[1,   600] training loss: 0.02658401
INFO:root:[1,   650] training loss: 0.00002231
INFO:root:[1,   700] training loss: 0.00001394
INFO:root:[1,   750] training loss: 0.00001416
INFO:root:[1,   800] training loss: 0.00001019
INFO:root:[1,   850] training loss: 0.00001010
INFO:root:[1,   900] training loss: 0.07413917
INFO:root:[1,   950] training loss: 0.02120808
INFO:root:[1,  1000] training loss: 0.00054070
INFO:root:[1,  1050] training loss: 0.00022512
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.0000    0.0000    0.0000        10
          G1     0.0000    0.0000    0.0000        74
   Metaphase     0.2629    1.0000    0.4164      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.08036618
INFO:root:[2,   100] training loss: 0.03785478
INFO:root:[2,   150] training loss: 0.02724706
INFO:root:[2,   200] training loss: 0.02734241
INFO:root:[2,   250] training loss: 0.02801977
INFO:root:[2,   300] training loss: 0.02595593
INFO:root:[2,   350] training loss: 0.02979881
INFO:root:[2,   400] training loss: 0.00356893
INFO:root:[2,   450] training loss: 0.00016267
INFO:root:[2,   500] training loss: 0.00786651
INFO:root:[2,   550] training loss: 0.00721066
INFO:root:[2,   600] training loss: 0.03031940
INFO:root:[2,   650] training loss: 0.00017798
INFO:root:[2,   700] training loss: 0.00013818
INFO:root:[2,   750] training loss: 0.00011234
INFO:root:[2,   800] training loss: 0.00009309
INFO:root:[2,   850] training loss: 0.00008050
INFO:root:[2,   900] training loss: 0.06194471
INFO:root:[2,   950] training loss: 0.02570239
INFO:root:[2,  1000] training loss: 0.00100740
INFO:root:[2,  1050] training loss: 0.00038425
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.0000    0.0000    0.0000        10
          G1     0.0000    0.0000    0.0000        74
   Metaphase     0.2629    1.0000    0.4164      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.07195864
INFO:root:[3,   100] training loss: 0.03362033
INFO:root:[3,   150] training loss: 0.04340020
INFO:root:[3,   200] training loss: 0.03333091
INFO:root:[3,   250] training loss: 0.03245928
INFO:root:[3,   300] training loss: 0.02523275
INFO:root:[3,   350] training loss: 0.03635505
INFO:root:[3,   400] training loss: 0.00166340
INFO:root:[3,   450] training loss: 0.00024975
INFO:root:[3,   500] training loss: 0.00578495
INFO:root:[3,   550] training loss: 0.00345183
INFO:root:[3,   600] training loss: 0.02129732
INFO:root:[3,   650] training loss: 0.00000824
INFO:root:[3,   700] training loss: 0.00000815
INFO:root:[3,   750] training loss: 0.00000620
INFO:root:[3,   800] training loss: 0.00000494
INFO:root:[3,   850] training loss: 0.00000578
INFO:root:[3,   900] training loss: 0.05723710
INFO:root:[3,   950] training loss: 0.01892127
INFO:root:[3,  1000] training loss: 0.00016961
INFO:root:[3,  1050] training loss: 0.00010595
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.0000    0.0000    0.0000        10
          G1     0.0000    0.0000    0.0000        74
   Metaphase     0.2629    1.0000    0.4164      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.05930757
INFO:root:[4,   100] training loss: 0.02890297
INFO:root:[4,   150] training loss: 0.02839182
INFO:root:[4,   200] training loss: 0.02547475
INFO:root:[4,   250] training loss: 0.02792054
INFO:root:[4,   300] training loss: 0.02273888
INFO:root:[4,   350] training loss: 0.03206780
INFO:root:[4,   400] training loss: 0.00052773
INFO:root:[4,   450] training loss: 0.00020471
INFO:root:[4,   500] training loss: 0.00531163
INFO:root:[4,   550] training loss: 0.00484671
INFO:root:[4,   600] training loss: 0.03243306
INFO:root:[4,   650] training loss: 0.00006367
INFO:root:[4,   700] training loss: 0.00005516
INFO:root:[4,   750] training loss: 0.00004744
INFO:root:[4,   800] training loss: 0.00004277
INFO:root:[4,   850] training loss: 0.00004121
INFO:root:[4,   900] training loss: 0.05127609
INFO:root:[4,   950] training loss: 0.01885214
INFO:root:[4,  1000] training loss: 0.00062350
INFO:root:[4,  1050] training loss: 0.00027287
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4000    0.2000    0.2667        10
          G1     0.0000    0.0000    0.0000        74
   Metaphase     0.2633    1.0000    0.4168      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2634      3872
   macro avg     0.0948    0.1714    0.0976      3872
weighted avg     0.0702    0.2634    0.1103      3872

INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.05762699
INFO:root:[5,   100] training loss: 0.02578025
INFO:root:[5,   150] training loss: 0.03406503
INFO:root:[5,   200] training loss: 0.02384491
INFO:root:[5,   250] training loss: 0.02394855
INFO:root:[5,   300] training loss: 0.02171873
INFO:root:[5,   350] training loss: 0.02641019
INFO:root:[5,   400] training loss: 0.00055916
INFO:root:[5,   450] training loss: 0.00034141
INFO:root:[5,   500] training loss: 0.00555516
INFO:root:[5,   550] training loss: 0.00547386
INFO:root:[5,   600] training loss: 0.02607820
INFO:root:[5,   650] training loss: 0.00002582
INFO:root:[5,   700] training loss: 0.00002315
INFO:root:[5,   750] training loss: 0.00002067
INFO:root:[5,   800] training loss: 0.00001874
INFO:root:[5,   850] training loss: 0.00001757
INFO:root:[5,   900] training loss: 0.05609788
INFO:root:[5,   950] training loss: 0.02461256
INFO:root:[5,  1000] training loss: 0.00045756
INFO:root:[5,  1050] training loss: 0.00018561
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.0000    0.0000    0.0000        10
          G1     0.0000    0.0000    0.0000        74
   Metaphase     0.2629    1.0000    0.4164      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.05894218
INFO:root:[6,   100] training loss: 0.02532494
INFO:root:[6,   150] training loss: 0.03036192
INFO:root:[6,   200] training loss: 0.02451668
INFO:root:[6,   250] training loss: 0.02203707
INFO:root:[6,   300] training loss: 0.02102566
INFO:root:[6,   350] training loss: 0.02929143
INFO:root:[6,   400] training loss: 0.00073122
INFO:root:[6,   450] training loss: 0.00025333
INFO:root:[6,   500] training loss: 0.00811982
INFO:root:[6,   550] training loss: 0.00458790
INFO:root:[6,   600] training loss: 0.03067694
INFO:root:[6,   650] training loss: 0.00008117
INFO:root:[6,   700] training loss: 0.00006297
INFO:root:[6,   750] training loss: 0.00004990
INFO:root:[6,   800] training loss: 0.00004414
INFO:root:[6,   850] training loss: 0.00003756
INFO:root:[6,   900] training loss: 0.05376767
INFO:root:[6,   950] training loss: 0.02277459
INFO:root:[6,  1000] training loss: 0.00041690
INFO:root:[6,  1050] training loss: 0.00020110
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.2800    0.7000    0.4000        10
          G1     0.8182    0.1216    0.2118        74
   Metaphase     0.2654    1.0000    0.4194      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2670      3872
   macro avg     0.1948    0.2602    0.1473      3872
weighted avg     0.0861    0.2670    0.1154      3872

INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.05336631
INFO:root:[7,   100] training loss: 0.02457694
INFO:root:[7,   150] training loss: 0.03109241
INFO:root:[7,   200] training loss: 0.02096042
INFO:root:[7,   250] training loss: 0.01830737
INFO:root:[7,   300] training loss: 0.01849997
INFO:root:[7,   350] training loss: 0.02950415
INFO:root:[7,   400] training loss: 0.00075372
INFO:root:[7,   450] training loss: 0.00012139
INFO:root:[7,   500] training loss: 0.00497415
INFO:root:[7,   550] training loss: 0.00576547
INFO:root:[7,   600] training loss: 0.02507446
INFO:root:[7,   650] training loss: 0.00005430
INFO:root:[7,   700] training loss: 0.00004753
INFO:root:[7,   750] training loss: 0.00004352
INFO:root:[7,   800] training loss: 0.00003661
INFO:root:[7,   850] training loss: 0.00003109
INFO:root:[7,   900] training loss: 0.04601219
INFO:root:[7,   950] training loss: 0.01887875
INFO:root:[7,  1000] training loss: 0.00042838
INFO:root:[7,  1050] training loss: 0.00021514
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3214    0.9000    0.4737        10
          G1     0.6923    0.1216    0.2069        74
   Metaphase     0.2652    0.9980    0.4191      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2670      3872
   macro avg     0.1827    0.2885    0.1571      3872
weighted avg     0.0838    0.2670    0.1154      3872

INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.05184712
INFO:root:[8,   100] training loss: 0.02207005
INFO:root:[8,   150] training loss: 0.02837613
INFO:root:[8,   200] training loss: 0.01917563
INFO:root:[8,   250] training loss: 0.01960615
INFO:root:[8,   300] training loss: 0.01890914
INFO:root:[8,   350] training loss: 0.02692307
INFO:root:[8,   400] training loss: 0.00040984
INFO:root:[8,   450] training loss: 0.00010396
INFO:root:[8,   500] training loss: 0.00499802
INFO:root:[8,   550] training loss: 0.00564618
INFO:root:[8,   600] training loss: 0.02570173
INFO:root:[8,   650] training loss: 0.00004922
INFO:root:[8,   700] training loss: 0.00004327
INFO:root:[8,   750] training loss: 0.00004238
INFO:root:[8,   800] training loss: 0.00003756
INFO:root:[8,   850] training loss: 0.00003194
INFO:root:[8,   900] training loss: 0.04291667
INFO:root:[8,   950] training loss: 0.01853117
INFO:root:[8,  1000] training loss: 0.00028763
INFO:root:[8,  1050] training loss: 0.00017799
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3750    0.9000    0.5294        10
          G1     0.8750    0.2838    0.4286        74
   Metaphase     0.2660    0.9990    0.4201      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2704      3872
   macro avg     0.2166    0.3118    0.1969      3872
weighted avg     0.0876    0.2704    0.1200      3872

INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.04919948
INFO:root:[9,   100] training loss: 0.02053030
INFO:root:[9,   150] training loss: 0.02658820
INFO:root:[9,   200] training loss: 0.02162102
INFO:root:[9,   250] training loss: 0.01996278
INFO:root:[9,   300] training loss: 0.01913389
INFO:root:[9,   350] training loss: 0.02593571
INFO:root:[9,   400] training loss: 0.00073643
INFO:root:[9,   450] training loss: 0.00021998
INFO:root:[9,   500] training loss: 0.00505878
INFO:root:[9,   550] training loss: 0.00637577
INFO:root:[9,   600] training loss: 0.02509283
INFO:root:[9,   650] training loss: 0.00004125
INFO:root:[9,   700] training loss: 0.00004260
INFO:root:[9,   750] training loss: 0.00003751
INFO:root:[9,   800] training loss: 0.00003136
INFO:root:[9,   850] training loss: 0.00002294
INFO:root:[9,   900] training loss: 0.04284776
INFO:root:[9,   950] training loss: 0.01605077
INFO:root:[9,  1000] training loss: 0.00029420
INFO:root:[9,  1050] training loss: 0.00016584
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5000    0.1000    0.1667        10
          G1     0.6667    0.1622    0.2609        74
   Metaphase     0.2643    1.0000    0.4181      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2663      3872
   macro avg     0.2044    0.1803    0.1208      3872
weighted avg     0.0835    0.2663    0.1153      3872

INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.04654929
INFO:root:[10,   100] training loss: 0.02157976
INFO:root:[10,   150] training loss: 0.01969358
INFO:root:[10,   200] training loss: 0.02147363
INFO:root:[10,   250] training loss: 0.02033037
INFO:root:[10,   300] training loss: 0.02098757
INFO:root:[10,   350] training loss: 0.02225491
INFO:root:[10,   400] training loss: 0.00056579
INFO:root:[10,   450] training loss: 0.00029639
INFO:root:[10,   500] training loss: 0.00533821
INFO:root:[10,   550] training loss: 0.00732569
INFO:root:[10,   600] training loss: 0.02662262
INFO:root:[10,   650] training loss: 0.00010218
INFO:root:[10,   700] training loss: 0.00007408
INFO:root:[10,   750] training loss: 0.00006926
INFO:root:[10,   800] training loss: 0.00006058
INFO:root:[10,   850] training loss: 0.00004621
INFO:root:[10,   900] training loss: 0.04050911
INFO:root:[10,   950] training loss: 0.01790431
INFO:root:[10,  1000] training loss: 0.00034454
INFO:root:[10,  1050] training loss: 0.00020700
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4167    1.0000    0.5882        10
          G1     0.6122    0.4054    0.4878        74
   Metaphase     0.2674    0.9980    0.4218      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2727      3872
   macro avg     0.1852    0.3433    0.2140      3872
weighted avg     0.0831    0.2727    0.1217      3872

INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.04579304
INFO:root:[11,   100] training loss: 0.01935971
INFO:root:[11,   150] training loss: 0.02605525
INFO:root:[11,   200] training loss: 0.02024891
INFO:root:[11,   250] training loss: 0.01703101
INFO:root:[11,   300] training loss: 0.01711832
INFO:root:[11,   350] training loss: 0.02148149
INFO:root:[11,   400] training loss: 0.00043606
INFO:root:[11,   450] training loss: 0.00018092
INFO:root:[11,   500] training loss: 0.00445832
INFO:root:[11,   550] training loss: 0.00646588
INFO:root:[11,   600] training loss: 0.02714518
INFO:root:[11,   650] training loss: 0.00012758
INFO:root:[11,   700] training loss: 0.00009993
INFO:root:[11,   750] training loss: 0.00008924
INFO:root:[11,   800] training loss: 0.00007990
INFO:root:[11,   850] training loss: 0.00006581
INFO:root:[11,   900] training loss: 0.03777861
INFO:root:[11,   950] training loss: 0.01670832
INFO:root:[11,  1000] training loss: 0.00039076
INFO:root:[11,  1050] training loss: 0.00023200
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4286    0.9000    0.5806        10
          G1     0.6889    0.4189    0.5210        74
   Metaphase     0.2671    0.9980    0.4214      1018
   Telophase     1.0000    0.3333    0.5000         6

    accuracy                         0.2732      3872
   macro avg     0.3406    0.3786    0.2890      3872
weighted avg     0.0860    0.2732    0.1230      3872

INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.04434463
INFO:root:[12,   100] training loss: 0.01881709
INFO:root:[12,   150] training loss: 0.01700279
INFO:root:[12,   200] training loss: 0.01794776
INFO:root:[12,   250] training loss: 0.01899788
INFO:root:[12,   300] training loss: 0.01829402
INFO:root:[12,   350] training loss: 0.01985211
INFO:root:[12,   400] training loss: 0.00027661
INFO:root:[12,   450] training loss: 0.00013722
INFO:root:[12,   500] training loss: 0.00479452
INFO:root:[12,   550] training loss: 0.00730031
INFO:root:[12,   600] training loss: 0.02881633
INFO:root:[12,   650] training loss: 0.00019533
INFO:root:[12,   700] training loss: 0.00012825
INFO:root:[12,   750] training loss: 0.00010527
INFO:root:[12,   800] training loss: 0.00008992
INFO:root:[12,   850] training loss: 0.00007962
INFO:root:[12,   900] training loss: 0.03739475
INFO:root:[12,   950] training loss: 0.01667922
INFO:root:[12,  1000] training loss: 0.00043387
INFO:root:[12,  1050] training loss: 0.00024515
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3810    0.8000    0.5161        10
          G1     0.6923    0.3649    0.4779        74
   Metaphase     0.2669    0.9980    0.4211      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2727      3872
   macro avg     0.3343    0.4280    0.3320      3872
weighted avg     0.0859    0.2727    0.1226      3872

INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.04473767
INFO:root:[13,   100] training loss: 0.02017810
INFO:root:[13,   150] training loss: 0.02154262
INFO:root:[13,   200] training loss: 0.02090412
INFO:root:[13,   250] training loss: 0.01680587
INFO:root:[13,   300] training loss: 0.01605181
INFO:root:[13,   350] training loss: 0.01752784
INFO:root:[13,   400] training loss: 0.00033707
INFO:root:[13,   450] training loss: 0.00010717
INFO:root:[13,   500] training loss: 0.00452603
INFO:root:[13,   550] training loss: 0.00699592
INFO:root:[13,   600] training loss: 0.02576017
INFO:root:[13,   650] training loss: 0.00020167
INFO:root:[13,   700] training loss: 0.00014514
INFO:root:[13,   750] training loss: 0.00012763
INFO:root:[13,   800] training loss: 0.00010979
INFO:root:[13,   850] training loss: 0.00009548
INFO:root:[13,   900] training loss: 0.03495608
INFO:root:[13,   950] training loss: 0.01703696
INFO:root:[13,  1000] training loss: 0.00045145
INFO:root:[13,  1050] training loss: 0.00021024
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4211    0.8000    0.5517        10
          G1     0.7895    0.2027    0.3226        74
   Metaphase     0.2653    0.9990    0.4192      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2686      3872
   macro avg     0.2108    0.2860    0.1848      3872
weighted avg     0.0859    0.2686    0.1178      3872

INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.05306831
INFO:root:[14,   100] training loss: 0.02182656
INFO:root:[14,   150] training loss: 0.01906727
INFO:root:[14,   200] training loss: 0.02050058
INFO:root:[14,   250] training loss: 0.01820836
INFO:root:[14,   300] training loss: 0.01771161
INFO:root:[14,   350] training loss: 0.01903864
INFO:root:[14,   400] training loss: 0.00014546
INFO:root:[14,   450] training loss: 0.00008211
INFO:root:[14,   500] training loss: 0.00508685
INFO:root:[14,   550] training loss: 0.00707251
INFO:root:[14,   600] training loss: 0.02543786
INFO:root:[14,   650] training loss: 0.00017759
INFO:root:[14,   700] training loss: 0.00012741
INFO:root:[14,   750] training loss: 0.00012067
INFO:root:[14,   800] training loss: 0.00009703
INFO:root:[14,   850] training loss: 0.00009335
INFO:root:[14,   900] training loss: 0.03920342
INFO:root:[14,   950] training loss: 0.01602014
INFO:root:[14,  1000] training loss: 0.00056568
INFO:root:[14,  1050] training loss: 0.00024923
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3529    0.6000    0.4444        10
          G1     0.5370    0.3919    0.4531        74
   Metaphase     0.2674    0.9980    0.4218      1018
   Telophase     1.0000    0.1667    0.2857         6

    accuracy                         0.2717      3872
   macro avg     0.3082    0.3081    0.2293      3872
weighted avg     0.0830    0.2717    0.1211      3872

INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.04884786
INFO:root:[15,   100] training loss: 0.02042248
INFO:root:[15,   150] training loss: 0.01899150
INFO:root:[15,   200] training loss: 0.01853203
INFO:root:[15,   250] training loss: 0.02007515
INFO:root:[15,   300] training loss: 0.02061353
INFO:root:[15,   350] training loss: 0.01725413
INFO:root:[15,   400] training loss: 0.00021379
INFO:root:[15,   450] training loss: 0.00025130
INFO:root:[15,   500] training loss: 0.00467032
INFO:root:[15,   550] training loss: 0.00763143
INFO:root:[15,   600] training loss: 0.02719741
INFO:root:[15,   650] training loss: 0.00023420
INFO:root:[15,   700] training loss: 0.00017384
INFO:root:[15,   750] training loss: 0.00014717
INFO:root:[15,   800] training loss: 0.00011004
INFO:root:[15,   850] training loss: 0.00010334
INFO:root:[15,   900] training loss: 0.03752052
INFO:root:[15,   950] training loss: 0.01778953
INFO:root:[15,  1000] training loss: 0.00054097
INFO:root:[15,  1050] training loss: 0.00026905
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3103    0.9000    0.4615        10
          G1     0.3553    0.3649    0.3600        74
   Metaphase     0.2703    0.9990    0.4254      1018
   Telophase     1.0000    0.6667    0.8000         6

    accuracy                         0.2730      3872
   macro avg     0.2766    0.4186    0.2924      3872
weighted avg     0.0802    0.2730    0.1212      3872

INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.04713413
INFO:root:[16,   100] training loss: 0.01873842
INFO:root:[16,   150] training loss: 0.01735104
INFO:root:[16,   200] training loss: 0.01958416
INFO:root:[16,   250] training loss: 0.02074797
INFO:root:[16,   300] training loss: 0.01812060
INFO:root:[16,   350] training loss: 0.01790380
INFO:root:[16,   400] training loss: 0.00029422
INFO:root:[16,   450] training loss: 0.00013363
INFO:root:[16,   500] training loss: 0.00452467
INFO:root:[16,   550] training loss: 0.00792633
INFO:root:[16,   600] training loss: 0.02830060
INFO:root:[16,   650] training loss: 0.00027520
INFO:root:[16,   700] training loss: 0.00019538
INFO:root:[16,   750] training loss: 0.00016763
INFO:root:[16,   800] training loss: 0.00013713
INFO:root:[16,   850] training loss: 0.00011360
INFO:root:[16,   900] training loss: 0.03615089
INFO:root:[16,   950] training loss: 0.01852598
INFO:root:[16,  1000] training loss: 0.00053793
INFO:root:[16,  1050] training loss: 0.00026512
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3750    0.9000    0.5294        10
          G1     0.4943    0.5811    0.5342        74
   Metaphase     0.2694    0.9941    0.4239      1018
   Telophase     1.0000    0.6667    0.8000         6

    accuracy                         0.2758      3872
   macro avg     0.3055    0.4488    0.3268      3872
weighted avg     0.0828    0.2758    0.1243      3872

INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.04695249
INFO:root:[17,   100] training loss: 0.02021629
INFO:root:[17,   150] training loss: 0.01952123
INFO:root:[17,   200] training loss: 0.02054741
INFO:root:[17,   250] training loss: 0.01678039
INFO:root:[17,   300] training loss: 0.01643892
INFO:root:[17,   350] training loss: 0.01688686
INFO:root:[17,   400] training loss: 0.00036000
INFO:root:[17,   450] training loss: 0.00010495
INFO:root:[17,   500] training loss: 0.00453754
INFO:root:[17,   550] training loss: 0.00894449
INFO:root:[17,   600] training loss: 0.02966958
INFO:root:[17,   650] training loss: 0.00036444
INFO:root:[17,   700] training loss: 0.00023422
INFO:root:[17,   750] training loss: 0.00019834
INFO:root:[17,   800] training loss: 0.00014667
INFO:root:[17,   850] training loss: 0.00012481
INFO:root:[17,   900] training loss: 0.03418843
INFO:root:[17,   950] training loss: 0.02086009
INFO:root:[17,  1000] training loss: 0.00061812
INFO:root:[17,  1050] training loss: 0.00030528
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4444    0.4000    0.4211        10
          G1     0.4423    0.6216    0.5169        74
   Metaphase     0.2699    0.9951    0.4246      1018
   Telophase     1.0000    0.6667    0.8000         6

    accuracy                         0.2756      3872
   macro avg     0.3081    0.3833    0.3089      3872
weighted avg     0.0821    0.2756    0.1239      3872

INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.04903549
INFO:root:[18,   100] training loss: 0.01833791
INFO:root:[18,   150] training loss: 0.01929480
INFO:root:[18,   200] training loss: 0.01944222
INFO:root:[18,   250] training loss: 0.02655069
INFO:root:[18,   300] training loss: 0.02469149
INFO:root:[18,   350] training loss: 0.01966051
INFO:root:[18,   400] training loss: 0.00015952
INFO:root:[18,   450] training loss: 0.00011059
INFO:root:[18,   500] training loss: 0.00432872
INFO:root:[18,   550] training loss: 0.00771919
INFO:root:[18,   600] training loss: 0.02788219
INFO:root:[18,   650] training loss: 0.00021860
INFO:root:[18,   700] training loss: 0.00016290
INFO:root:[18,   750] training loss: 0.00014928
INFO:root:[18,   800] training loss: 0.00013399
INFO:root:[18,   850] training loss: 0.00010526
INFO:root:[18,   900] training loss: 0.04537035
INFO:root:[18,   950] training loss: 0.02093024
INFO:root:[18,  1000] training loss: 0.00092034
INFO:root:[18,  1050] training loss: 0.00036179
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.2308    0.3000    0.2609        10
          G1     0.8182    0.1216    0.2118        74
   Metaphase     0.2645    0.9990    0.4183      1018
   Telophase     1.0000    0.5000    0.6667         6

    accuracy                         0.2665      3872
   macro avg     0.3305    0.2744    0.2225      3872
weighted avg     0.0873    0.2665    0.1157      3872

INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.05130458
INFO:root:[19,   100] training loss: 0.02030311
INFO:root:[19,   150] training loss: 0.02109098
INFO:root:[19,   200] training loss: 0.02130641
INFO:root:[19,   250] training loss: 0.01767546
INFO:root:[19,   300] training loss: 0.01831938
INFO:root:[19,   350] training loss: 0.01929403
INFO:root:[19,   400] training loss: 0.00024422
INFO:root:[19,   450] training loss: 0.00013437
INFO:root:[19,   500] training loss: 0.00456971
INFO:root:[19,   550] training loss: 0.00952196
INFO:root:[19,   600] training loss: 0.02869352
INFO:root:[19,   650] training loss: 0.00047525
INFO:root:[19,   700] training loss: 0.00029532
INFO:root:[19,   750] training loss: 0.00022128
INFO:root:[19,   800] training loss: 0.00017935
INFO:root:[19,   850] training loss: 0.00014411
INFO:root:[19,   900] training loss: 0.04100499
INFO:root:[19,   950] training loss: 0.01975667
INFO:root:[19,  1000] training loss: 0.00093635
INFO:root:[19,  1050] training loss: 0.00038997
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3704    1.0000    0.5405        10
          G1     0.4394    0.3919    0.4143        74
   Metaphase     0.2684    0.9951    0.4228      1018
   Telophase     0.7500    0.5000    0.6000         6

    accuracy                         0.2725      3872
   macro avg     0.2612    0.4124    0.2825      3872
weighted avg     0.0811    0.2725    0.1214      3872

INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.04891536
INFO:root:[20,   100] training loss: 0.02016435
INFO:root:[20,   150] training loss: 0.01786405
INFO:root:[20,   200] training loss: 0.01902349
INFO:root:[20,   250] training loss: 0.01850465
INFO:root:[20,   300] training loss: 0.01921517
INFO:root:[20,   350] training loss: 0.01550779
INFO:root:[20,   400] training loss: 0.00034229
INFO:root:[20,   450] training loss: 0.00029417
INFO:root:[20,   500] training loss: 0.00469635
INFO:root:[20,   550] training loss: 0.00898003
INFO:root:[20,   600] training loss: 0.02945275
INFO:root:[20,   650] training loss: 0.00046527
INFO:root:[20,   700] training loss: 0.00029542
INFO:root:[20,   750] training loss: 0.00024369
INFO:root:[20,   800] training loss: 0.00019636
INFO:root:[20,   850] training loss: 0.00015950
INFO:root:[20,   900] training loss: 0.03967343
INFO:root:[20,   950] training loss: 0.01830815
INFO:root:[20,  1000] training loss: 0.00088652
INFO:root:[20,  1050] training loss: 0.00040777
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4737    0.9000    0.6207        10
          G1     0.3740    0.6216    0.4670        74
   Metaphase     0.2715    0.9921    0.4263      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.2769      3872
   macro avg     0.3385    0.5496    0.3959      3872
weighted avg     0.0813    0.2769    0.1243      3872

INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.04615854
INFO:root:[21,   100] training loss: 0.02067148
INFO:root:[21,   150] training loss: 0.01604714
INFO:root:[21,   200] training loss: 0.01924248
INFO:root:[21,   250] training loss: 0.01638165
INFO:root:[21,   300] training loss: 0.01655020
INFO:root:[21,   350] training loss: 0.01807080
INFO:root:[21,   400] training loss: 0.00022658
INFO:root:[21,   450] training loss: 0.00011107
INFO:root:[21,   500] training loss: 0.00470573
INFO:root:[21,   550] training loss: 0.00969639
INFO:root:[21,   600] training loss: 0.02932909
INFO:root:[21,   650] training loss: 0.00064254
INFO:root:[21,   700] training loss: 0.00037200
INFO:root:[21,   750] training loss: 0.00027305
INFO:root:[21,   800] training loss: 0.00020547
INFO:root:[21,   850] training loss: 0.00017327
INFO:root:[21,   900] training loss: 0.03640292
INFO:root:[21,   950] training loss: 0.02005724
INFO:root:[21,  1000] training loss: 0.00084224
INFO:root:[21,  1050] training loss: 0.00037821
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4118    0.7000    0.5185        10
          G1     0.4409    0.5541    0.4910        74
   Metaphase     0.2707    0.9990    0.4260      1018
   Telophase     1.0000    0.6667    0.8000         6

    accuracy                         0.2761      3872
   macro avg     0.3033    0.4171    0.3194      3872
weighted avg     0.0822    0.2761    0.1240      3872

INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.04711148
INFO:root:[22,   100] training loss: 0.01902247
INFO:root:[22,   150] training loss: 0.01760061
INFO:root:[22,   200] training loss: 0.01930448
INFO:root:[22,   250] training loss: 0.01628163
INFO:root:[22,   300] training loss: 0.01823059
INFO:root:[22,   350] training loss: 0.01495643
INFO:root:[22,   400] training loss: 0.00023400
INFO:root:[22,   450] training loss: 0.00011225
INFO:root:[22,   500] training loss: 0.00459780
INFO:root:[22,   550] training loss: 0.00907990
INFO:root:[22,   600] training loss: 0.02927637
INFO:root:[22,   650] training loss: 0.00059155
INFO:root:[22,   700] training loss: 0.00035544
INFO:root:[22,   750] training loss: 0.00026002
INFO:root:[22,   800] training loss: 0.00020399
INFO:root:[22,   850] training loss: 0.00016621
INFO:root:[22,   900] training loss: 0.03674068
INFO:root:[22,   950] training loss: 0.02175510
INFO:root:[22,  1000] training loss: 0.00095695
INFO:root:[22,  1050] training loss: 0.00044081
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4286    0.6000    0.5000        10
          G1     0.5000    0.5676    0.5316        74
   Metaphase     0.2691    0.9971    0.4238      1018
   Telophase     1.0000    0.3333    0.5000         6

    accuracy                         0.2751      3872
   macro avg     0.3140    0.3569    0.2793      3872
weighted avg     0.0830    0.2751    0.1236      3872

INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.04677108
INFO:root:[23,   100] training loss: 0.01989819
INFO:root:[23,   150] training loss: 0.01768857
INFO:root:[23,   200] training loss: 0.01777922
INFO:root:[23,   250] training loss: 0.01736910
INFO:root:[23,   300] training loss: 0.01708134
INFO:root:[23,   350] training loss: 0.01481756
INFO:root:[23,   400] training loss: 0.00019054
INFO:root:[23,   450] training loss: 0.00014276
INFO:root:[23,   500] training loss: 0.00467350
INFO:root:[23,   550] training loss: 0.00978450
INFO:root:[23,   600] training loss: 0.03105547
INFO:root:[23,   650] training loss: 0.00062063
INFO:root:[23,   700] training loss: 0.00036251
INFO:root:[23,   750] training loss: 0.00028502
INFO:root:[23,   800] training loss: 0.00021255
INFO:root:[23,   850] training loss: 0.00017513
INFO:root:[23,   900] training loss: 0.03718582
INFO:root:[23,   950] training loss: 0.02078862
INFO:root:[23,  1000] training loss: 0.00109172
INFO:root:[23,  1050] training loss: 0.00047465
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4091    0.9000    0.5625        10
          G1     0.4312    0.6351    0.5137        74
   Metaphase     0.2710    0.9941    0.4258      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2771      3872
   macro avg     0.3016    0.4804    0.3444      3872
weighted avg     0.0821    0.2771    0.1246      3872

INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.04760470
INFO:root:[24,   100] training loss: 0.02259273
INFO:root:[24,   150] training loss: 0.01870828
INFO:root:[24,   200] training loss: 0.01856602
INFO:root:[24,   250] training loss: 0.01780793
INFO:root:[24,   300] training loss: 0.01699041
INFO:root:[24,   350] training loss: 0.01489139
INFO:root:[24,   400] training loss: 0.00012248
INFO:root:[24,   450] training loss: 0.00008860
INFO:root:[24,   500] training loss: 0.00451044
INFO:root:[24,   550] training loss: 0.00864093
INFO:root:[24,   600] training loss: 0.02970694
INFO:root:[24,   650] training loss: 0.00066795
INFO:root:[24,   700] training loss: 0.00036754
INFO:root:[24,   750] training loss: 0.00028079
INFO:root:[24,   800] training loss: 0.00021532
INFO:root:[24,   850] training loss: 0.00016798
INFO:root:[24,   900] training loss: 0.03397503
INFO:root:[24,   950] training loss: 0.02161668
INFO:root:[24,  1000] training loss: 0.00098275
INFO:root:[24,  1050] training loss: 0.00045506
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3043    0.7000    0.4242        10
          G1     0.4023    0.4730    0.4348        74
   Metaphase     0.2707    0.9990    0.4260      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2748      3872
   macro avg     0.2825    0.4293    0.3134      3872
weighted avg     0.0812    0.2748    0.1228      3872

INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.04779575
INFO:root:[25,   100] training loss: 0.02046847
INFO:root:[25,   150] training loss: 0.01654207
INFO:root:[25,   200] training loss: 0.01867527
INFO:root:[25,   250] training loss: 0.01512425
INFO:root:[25,   300] training loss: 0.01669485
INFO:root:[25,   350] training loss: 0.01506513
INFO:root:[25,   400] training loss: 0.00022756
INFO:root:[25,   450] training loss: 0.00009875
INFO:root:[25,   500] training loss: 0.00457895
INFO:root:[25,   550] training loss: 0.00897674
INFO:root:[25,   600] training loss: 0.02772461
INFO:root:[25,   650] training loss: 0.00058798
INFO:root:[25,   700] training loss: 0.00036354
INFO:root:[25,   750] training loss: 0.00028607
INFO:root:[25,   800] training loss: 0.00020411
INFO:root:[25,   850] training loss: 0.00017951
INFO:root:[25,   900] training loss: 0.04336457
INFO:root:[25,   950] training loss: 0.02575993
INFO:root:[25,  1000] training loss: 0.00130937
INFO:root:[25,  1050] training loss: 0.00047771
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.6250    0.5000    0.5556        10
          G1     0.4471    0.5135    0.4780        74
   Metaphase     0.2695    0.9990    0.4245      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2751      3872
   macro avg     0.3345    0.4066    0.3382      3872
weighted avg     0.0826    0.2751    0.1236      3872

INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.04610496
INFO:root:[26,   100] training loss: 0.02052303
INFO:root:[26,   150] training loss: 0.02419732
INFO:root:[26,   200] training loss: 0.01971256
INFO:root:[26,   250] training loss: 0.01761125
INFO:root:[26,   300] training loss: 0.01715472
INFO:root:[26,   350] training loss: 0.01615838
INFO:root:[26,   400] training loss: 0.00025399
INFO:root:[26,   450] training loss: 0.00026799
INFO:root:[26,   500] training loss: 0.00444875
INFO:root:[26,   550] training loss: 0.00918373
INFO:root:[26,   600] training loss: 0.03033244
INFO:root:[26,   650] training loss: 0.00075085
INFO:root:[26,   700] training loss: 0.00042312
INFO:root:[26,   750] training loss: 0.00033014
INFO:root:[26,   800] training loss: 0.00025461
INFO:root:[26,   850] training loss: 0.00019862
INFO:root:[26,   900] training loss: 0.04095505
INFO:root:[26,   950] training loss: 0.02298109
INFO:root:[26,  1000] training loss: 0.00102144
INFO:root:[26,  1050] training loss: 0.00041135
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.6667    0.5714         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5294    0.9000    0.6667        10
          G1     0.4306    0.4189    0.4247        74
   Metaphase     0.2682    0.9941    0.4224      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2735      3872
   macro avg     0.3897    0.5447    0.4277      3872
weighted avg     0.0820    0.2735    0.1227      3872

INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.05179457
INFO:root:[27,   100] training loss: 0.02132437
INFO:root:[27,   150] training loss: 0.02106086
INFO:root:[27,   200] training loss: 0.02400593
INFO:root:[27,   250] training loss: 0.02496492
INFO:root:[27,   300] training loss: 0.02525954
INFO:root:[27,   350] training loss: 0.01723496
INFO:root:[27,   400] training loss: 0.00023580
INFO:root:[27,   450] training loss: 0.00013050
INFO:root:[27,   500] training loss: 0.00467984
INFO:root:[27,   550] training loss: 0.01059181
INFO:root:[27,   600] training loss: 0.03103974
INFO:root:[27,   650] training loss: 0.00065502
INFO:root:[27,   700] training loss: 0.00039926
INFO:root:[27,   750] training loss: 0.00030629
INFO:root:[27,   800] training loss: 0.00024117
INFO:root:[27,   850] training loss: 0.00019414
INFO:root:[27,   900] training loss: 0.04415015
INFO:root:[27,   950] training loss: 0.02221477
INFO:root:[27,  1000] training loss: 0.00148927
INFO:root:[27,  1050] training loss: 0.00057831
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5000    0.9000    0.6429        10
          G1     0.5472    0.3919    0.4567        74
   Metaphase     0.2666    0.9941    0.4204      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2725      3872
   macro avg     0.3305    0.4456    0.3470      3872
weighted avg     0.0834    0.2725    0.1223      3872

INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.04823299
INFO:root:[28,   100] training loss: 0.01978361
INFO:root:[28,   150] training loss: 0.01703369
INFO:root:[28,   200] training loss: 0.02139499
INFO:root:[28,   250] training loss: 0.01930131
INFO:root:[28,   300] training loss: 0.01847263
INFO:root:[28,   350] training loss: 0.01618939
INFO:root:[28,   400] training loss: 0.00020537
INFO:root:[28,   450] training loss: 0.00013713
INFO:root:[28,   500] training loss: 0.00446353
INFO:root:[28,   550] training loss: 0.00978287
INFO:root:[28,   600] training loss: 0.02912946
INFO:root:[28,   650] training loss: 0.00092022
INFO:root:[28,   700] training loss: 0.00050556
INFO:root:[28,   750] training loss: 0.00039841
INFO:root:[28,   800] training loss: 0.00029658
INFO:root:[28,   850] training loss: 0.00023312
INFO:root:[28,   900] training loss: 0.04354793
INFO:root:[28,   950] training loss: 0.02119620
INFO:root:[28,  1000] training loss: 0.00117666
INFO:root:[28,  1050] training loss: 0.00047009
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5000    0.5000    0.5000        10
          G1     0.5000    0.3784    0.4308        74
   Metaphase     0.2674    0.9980    0.4218      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2722      3872
   macro avg     0.3239    0.3871    0.3231      3872
weighted avg     0.0827    0.2722    0.1218      3872

INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.04758575
INFO:root:[29,   100] training loss: 0.01978592
INFO:root:[29,   150] training loss: 0.01728910
INFO:root:[29,   200] training loss: 0.01880603
INFO:root:[29,   250] training loss: 0.01826062
INFO:root:[29,   300] training loss: 0.01983248
INFO:root:[29,   350] training loss: 0.01619865
INFO:root:[29,   400] training loss: 0.00027881
INFO:root:[29,   450] training loss: 0.00019965
INFO:root:[29,   500] training loss: 0.00414645
INFO:root:[29,   550] training loss: 0.00925766
INFO:root:[29,   600] training loss: 0.02913947
INFO:root:[29,   650] training loss: 0.00075877
INFO:root:[29,   700] training loss: 0.00045060
INFO:root:[29,   750] training loss: 0.00031402
INFO:root:[29,   800] training loss: 0.00024885
INFO:root:[29,   850] training loss: 0.00020622
INFO:root:[29,   900] training loss: 0.03690023
INFO:root:[29,   950] training loss: 0.02339579
INFO:root:[29,  1000] training loss: 0.00139261
INFO:root:[29,  1050] training loss: 0.00061027
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4615    0.6000    0.5217        10
          G1     0.4078    0.5676    0.4746        74
   Metaphase     0.2705    0.9961    0.4254      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2758      3872
   macro avg     0.3771    0.4758    0.3901      3872
weighted avg     0.0820    0.2758    0.1240      3872

INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.04668314
INFO:root:[30,   100] training loss: 0.01918264
INFO:root:[30,   150] training loss: 0.01660213
INFO:root:[30,   200] training loss: 0.01728865
INFO:root:[30,   250] training loss: 0.01599228
INFO:root:[30,   300] training loss: 0.02109844
INFO:root:[30,   350] training loss: 0.01517950
INFO:root:[30,   400] training loss: 0.00021568
INFO:root:[30,   450] training loss: 0.00020389
INFO:root:[30,   500] training loss: 0.00440012
INFO:root:[30,   550] training loss: 0.00981789
INFO:root:[30,   600] training loss: 0.02945715
INFO:root:[30,   650] training loss: 0.00103529
INFO:root:[30,   700] training loss: 0.00054685
INFO:root:[30,   750] training loss: 0.00040420
INFO:root:[30,   800] training loss: 0.00031613
INFO:root:[30,   850] training loss: 0.00023503
INFO:root:[30,   900] training loss: 0.06016309
INFO:root:[30,   950] training loss: 0.02188049
INFO:root:[30,  1000] training loss: 0.00034292
INFO:root:[30,  1050] training loss: 0.00018176
INFO:root:              precision    recall  f1-score   support

          G2     0.2500    0.3333    0.2857         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4000    0.4000    0.4000        10
          G1     0.0000    0.0000    0.0000        74
   Metaphase     0.2640    1.0000    0.4177      1018
   Telophase     1.0000    0.3333    0.5000         6

    accuracy                         0.2647      3872
   macro avg     0.2734    0.2952    0.2291      3872
weighted avg     0.0722    0.2647    0.1119      3872

INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.05018488
INFO:root:[31,   100] training loss: 0.02341831
INFO:root:[31,   150] training loss: 0.02495982
INFO:root:[31,   200] training loss: 0.02124806
INFO:root:[31,   250] training loss: 0.01875852
INFO:root:[31,   300] training loss: 0.01802002
INFO:root:[31,   350] training loss: 0.01898967
INFO:root:[31,   400] training loss: 0.00036003
INFO:root:[31,   450] training loss: 0.00018498
INFO:root:[31,   500] training loss: 0.00482844
INFO:root:[31,   550] training loss: 0.00778285
INFO:root:[31,   600] training loss: 0.02693460
INFO:root:[31,   650] training loss: 0.00038030
INFO:root:[31,   700] training loss: 0.00026692
INFO:root:[31,   750] training loss: 0.00022818
INFO:root:[31,   800] training loss: 0.00018312
INFO:root:[31,   850] training loss: 0.00016596
INFO:root:[31,   900] training loss: 0.03934719
INFO:root:[31,   950] training loss: 0.02192618
INFO:root:[31,  1000] training loss: 0.00097204
INFO:root:[31,  1050] training loss: 0.00044327
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4706    0.8000    0.5926        10
          G1     0.6842    0.1757    0.2796        74
   Metaphase     0.2653    0.9980    0.4191      1018
   Telophase     0.8000    0.6667    0.7273         6

    accuracy                         0.2689      3872
   macro avg     0.3172    0.3772    0.2884      3872
weighted avg     0.0853    0.2689    0.1182      3872

INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.04403519
INFO:root:[32,   100] training loss: 0.01976761
INFO:root:[32,   150] training loss: 0.01812293
INFO:root:[32,   200] training loss: 0.01901786
INFO:root:[32,   250] training loss: 0.01779970
INFO:root:[32,   300] training loss: 0.01733846
INFO:root:[32,   350] training loss: 0.01752667
INFO:root:[32,   400] training loss: 0.00015935
INFO:root:[32,   450] training loss: 0.00010188
INFO:root:[32,   500] training loss: 0.00451360
INFO:root:[32,   550] training loss: 0.00923025
INFO:root:[32,   600] training loss: 0.02764167
INFO:root:[32,   650] training loss: 0.00068116
INFO:root:[32,   700] training loss: 0.00041569
INFO:root:[32,   750] training loss: 0.00031874
INFO:root:[32,   800] training loss: 0.00025159
INFO:root:[32,   850] training loss: 0.00021212
INFO:root:[32,   900] training loss: 0.03633610
INFO:root:[32,   950] training loss: 0.02310120
INFO:root:[32,  1000] training loss: 0.00139243
INFO:root:[32,  1050] training loss: 0.00057896
INFO:root:              precision    recall  f1-score   support

          G2     0.6667    0.6667    0.6667         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5833    0.7000    0.6364        10
          G1     0.5778    0.3514    0.4370        74
   Metaphase     0.2668    0.9971    0.4209      1018
   Telophase     0.7143    0.8333    0.7692         6

    accuracy                         0.2725      3872
   macro avg     0.4013    0.5069    0.4186      3872
weighted avg     0.0843    0.2725    0.1224      3872

INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.04747396
INFO:root:[33,   100] training loss: 0.02004648
INFO:root:[33,   150] training loss: 0.02138542
INFO:root:[33,   200] training loss: 0.01943895
INFO:root:[33,   250] training loss: 0.03348374
INFO:root:[33,   300] training loss: 0.02269145
INFO:root:[33,   350] training loss: 0.01649465
INFO:root:[33,   400] training loss: 0.00015572
INFO:root:[33,   450] training loss: 0.00011185
INFO:root:[33,   500] training loss: 0.00464256
INFO:root:[33,   550] training loss: 0.00993009
INFO:root:[33,   600] training loss: 0.02948542
INFO:root:[33,   650] training loss: 0.00060710
INFO:root:[33,   700] training loss: 0.00038029
INFO:root:[33,   750] training loss: 0.00030768
INFO:root:[33,   800] training loss: 0.00024401
INFO:root:[33,   850] training loss: 0.00019933
INFO:root:[33,   900] training loss: 0.03790237
INFO:root:[33,   950] training loss: 0.02299418
INFO:root:[33,  1000] training loss: 0.00126230
INFO:root:[33,  1050] training loss: 0.00056373
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4286    0.9000    0.5806        10
          G1     0.4930    0.4730    0.4828        74
   Metaphase     0.2684    0.9941    0.4226      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.2743      3872
   macro avg     0.2771    0.4810    0.3347      3872
weighted avg     0.0822    0.2743    0.1232      3872

INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.04393286
INFO:root:[34,   100] training loss: 0.01843404
INFO:root:[34,   150] training loss: 0.01798153
INFO:root:[34,   200] training loss: 0.01708280
INFO:root:[34,   250] training loss: 0.01690122
INFO:root:[34,   300] training loss: 0.01786368
INFO:root:[34,   350] training loss: 0.01576893
INFO:root:[34,   400] training loss: 0.00011707
INFO:root:[34,   450] training loss: 0.00010915
INFO:root:[34,   500] training loss: 0.00457724
INFO:root:[34,   550] training loss: 0.01077533
INFO:root:[34,   600] training loss: 0.02983084
INFO:root:[34,   650] training loss: 0.00083138
INFO:root:[34,   700] training loss: 0.00046663
INFO:root:[34,   750] training loss: 0.00037716
INFO:root:[34,   800] training loss: 0.00028568
INFO:root:[34,   850] training loss: 0.00023642
INFO:root:[34,   900] training loss: 0.03603916
INFO:root:[34,   950] training loss: 0.02303088
INFO:root:[34,  1000] training loss: 0.00139779
INFO:root:[34,  1050] training loss: 0.00063050
INFO:root:              precision    recall  f1-score   support

          G2     0.6667    0.6667    0.6667         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.6154    0.8000    0.6957        10
          G1     0.4583    0.5946    0.5176        74
   Metaphase     0.2686    0.9902    0.4226      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.2758      3872
   macro avg     0.4094    0.5788    0.4608      3872
weighted avg     0.0828    0.2758    0.1247      3872

INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.04537522
INFO:root:[35,   100] training loss: 0.01859498
INFO:root:[35,   150] training loss: 0.01814628
INFO:root:[35,   200] training loss: 0.01931035
INFO:root:[35,   250] training loss: 0.01616052
INFO:root:[35,   300] training loss: 0.01707873
INFO:root:[35,   350] training loss: 0.01586419
INFO:root:[35,   400] training loss: 0.00021689
INFO:root:[35,   450] training loss: 0.00009626
INFO:root:[35,   500] training loss: 0.00459476
INFO:root:[35,   550] training loss: 0.01054400
INFO:root:[35,   600] training loss: 0.02900325
INFO:root:[35,   650] training loss: 0.00105827
INFO:root:[35,   700] training loss: 0.00057751
INFO:root:[35,   750] training loss: 0.00043145
INFO:root:[35,   800] training loss: 0.00033088
INFO:root:[35,   850] training loss: 0.00025703
INFO:root:[35,   900] training loss: 0.03737085
INFO:root:[35,   950] training loss: 0.02174300
INFO:root:[35,  1000] training loss: 0.00125328
INFO:root:[35,  1050] training loss: 0.00055158
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3750    0.6000    0.4615        10
          G1     0.4655    0.3649    0.4091        74
   Metaphase     0.2668    0.9931    0.4206      1018
   Telophase     0.7143    0.8333    0.7692         6

    accuracy                         0.2712      3872
   macro avg     0.3317    0.4464    0.3515      3872
weighted avg     0.0815    0.2712    0.1211      3872

INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.04486537
INFO:root:[36,   100] training loss: 0.01809593
INFO:root:[36,   150] training loss: 0.01697465
INFO:root:[36,   200] training loss: 0.01934573
INFO:root:[36,   250] training loss: 0.01556173
INFO:root:[36,   300] training loss: 0.01614702
INFO:root:[36,   350] training loss: 0.01511191
INFO:root:[36,   400] training loss: 0.00016802
INFO:root:[36,   450] training loss: 0.00011373
INFO:root:[36,   500] training loss: 0.00452568
INFO:root:[36,   550] training loss: 0.00974119
INFO:root:[36,   600] training loss: 0.02992081
INFO:root:[36,   650] training loss: 0.00094979
INFO:root:[36,   700] training loss: 0.00052245
INFO:root:[36,   750] training loss: 0.00041821
INFO:root:[36,   800] training loss: 0.00030708
INFO:root:[36,   850] training loss: 0.00024823
INFO:root:[36,   900] training loss: 0.03688120
INFO:root:[36,   950] training loss: 0.02593682
INFO:root:[36,  1000] training loss: 0.00149282
INFO:root:[36,  1050] training loss: 0.00071509
INFO:root:              precision    recall  f1-score   support

          G2     0.6667    0.6667    0.6667         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4211    0.8000    0.5517        10
          G1     0.4158    0.5676    0.4800        74
   Metaphase     0.2699    0.9921    0.4244      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.2758      3872
   macro avg     0.3758    0.5752    0.4351      3872
weighted avg     0.0818    0.2758    0.1241      3872

INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.04458232
INFO:root:[37,   100] training loss: 0.01852843
INFO:root:[37,   150] training loss: 0.01633879
INFO:root:[37,   200] training loss: 0.01824851
INFO:root:[37,   250] training loss: 0.01457516
INFO:root:[37,   300] training loss: 0.01728811
INFO:root:[37,   350] training loss: 0.01441161
INFO:root:[37,   400] training loss: 0.00010608
INFO:root:[37,   450] training loss: 0.00008623
INFO:root:[37,   500] training loss: 0.00451637
INFO:root:[37,   550] training loss: 0.00994446
INFO:root:[37,   600] training loss: 0.02902560
INFO:root:[37,   650] training loss: 0.00107864
INFO:root:[37,   700] training loss: 0.00056604
INFO:root:[37,   750] training loss: 0.00046116
INFO:root:[37,   800] training loss: 0.00035879
INFO:root:[37,   850] training loss: 0.00028215
INFO:root:[37,   900] training loss: 0.03975521
INFO:root:[37,   950] training loss: 0.02761647
INFO:root:[37,  1000] training loss: 0.00127007
INFO:root:[37,  1050] training loss: 0.00057852
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3333    0.1000    0.1538        10
          G1     0.0000    0.0000    0.0000        74
   Metaphase     0.2632    1.0000    0.4167      1018
   Telophase     1.0000    0.1667    0.2857         6

    accuracy                         0.2634      3872
   macro avg     0.2281    0.1810    0.1223      3872
weighted avg     0.0716    0.2634    0.1104      3872

INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.05420675
INFO:root:[38,   100] training loss: 0.02146830
INFO:root:[38,   150] training loss: 0.02175668
INFO:root:[38,   200] training loss: 0.02515208
INFO:root:[38,   250] training loss: 0.02239503
INFO:root:[38,   300] training loss: 0.02266503
INFO:root:[38,   350] training loss: 0.01852094
INFO:root:[38,   400] training loss: 0.00017811
INFO:root:[38,   450] training loss: 0.00024334
INFO:root:[38,   500] training loss: 0.00519166
INFO:root:[38,   550] training loss: 0.00999452
INFO:root:[38,   600] training loss: 0.02830667
INFO:root:[38,   650] training loss: 0.00107633
INFO:root:[38,   700] training loss: 0.00058151
INFO:root:[38,   750] training loss: 0.00043213
INFO:root:[38,   800] training loss: 0.00033938
INFO:root:[38,   850] training loss: 0.00025034
INFO:root:[38,   900] training loss: 0.04605189
INFO:root:[38,   950] training loss: 0.02522352
INFO:root:[38,  1000] training loss: 0.00160780
INFO:root:[38,  1050] training loss: 0.00061811
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3000    0.3000    0.3000        10
          G1     0.6316    0.1622    0.2581        74
   Metaphase     0.2646    0.9990    0.4184      1018
   Telophase     0.0000    0.0000    0.0000         6

    accuracy                         0.2665      3872
   macro avg     0.1709    0.2087    0.1395      3872
weighted avg     0.0824    0.2665    0.1157      3872

INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.04838931
INFO:root:[39,   100] training loss: 0.02127896
INFO:root:[39,   150] training loss: 0.01896958
INFO:root:[39,   200] training loss: 0.02130207
INFO:root:[39,   250] training loss: 0.01804718
INFO:root:[39,   300] training loss: 0.02056097
INFO:root:[39,   350] training loss: 0.01974816
INFO:root:[39,   400] training loss: 0.00017452
INFO:root:[39,   450] training loss: 0.00013944
INFO:root:[39,   500] training loss: 0.00450331
INFO:root:[39,   550] training loss: 0.01056415
INFO:root:[39,   600] training loss: 0.03277072
INFO:root:[39,   650] training loss: 0.00111433
INFO:root:[39,   700] training loss: 0.00060123
INFO:root:[39,   750] training loss: 0.00043283
INFO:root:[39,   800] training loss: 0.00034527
INFO:root:[39,   850] training loss: 0.00026405
INFO:root:[39,   900] training loss: 0.04252152
INFO:root:[39,   950] training loss: 0.02383798
INFO:root:[39,  1000] training loss: 0.00136958
INFO:root:[39,  1050] training loss: 0.00057026
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5000    0.5000    0.5000        10
          G1     0.6471    0.2973    0.4074        74
   Metaphase     0.2663    1.0000    0.4206      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2712      3872
   macro avg     0.3448    0.3758    0.3196      3872
weighted avg     0.0852    0.2712    0.1211      3872

INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.04538634
INFO:root:[40,   100] training loss: 0.01951420
INFO:root:[40,   150] training loss: 0.01889112
INFO:root:[40,   200] training loss: 0.01912423
INFO:root:[40,   250] training loss: 0.01592325
INFO:root:[40,   300] training loss: 0.01776927
INFO:root:[40,   350] training loss: 0.01902872
INFO:root:[40,   400] training loss: 0.00033706
INFO:root:[40,   450] training loss: 0.00023589
INFO:root:[40,   500] training loss: 0.00442497
INFO:root:[40,   550] training loss: 0.01027041
INFO:root:[40,   600] training loss: 0.02955279
INFO:root:[40,   650] training loss: 0.00099303
INFO:root:[40,   700] training loss: 0.00056028
INFO:root:[40,   750] training loss: 0.00041865
INFO:root:[40,   800] training loss: 0.00030955
INFO:root:[40,   850] training loss: 0.00022875
INFO:root:[40,   900] training loss: 0.03835163
INFO:root:[40,   950] training loss: 0.02337403
INFO:root:[40,  1000] training loss: 0.00135948
INFO:root:[40,  1050] training loss: 0.00059607
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3478    0.8000    0.4848        10
          G1     0.4531    0.3919    0.4203        74
   Metaphase     0.2692    0.9990    0.4241      1018
   Telophase     1.0000    1.0000    1.0000         6

    accuracy                         0.2738      3872
   macro avg     0.2957    0.4558    0.3327      3872
weighted avg     0.0819    0.2738    0.1223      3872

INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.04320871
INFO:root:[41,   100] training loss: 0.01887277
INFO:root:[41,   150] training loss: 0.01757586
INFO:root:[41,   200] training loss: 0.01963830
INFO:root:[41,   250] training loss: 0.01731824
INFO:root:[41,   300] training loss: 0.01759732
INFO:root:[41,   350] training loss: 0.01937360
INFO:root:[41,   400] training loss: 0.00017784
INFO:root:[41,   450] training loss: 0.00014268
INFO:root:[41,   500] training loss: 0.00430580
INFO:root:[41,   550] training loss: 0.01025024
INFO:root:[41,   600] training loss: 0.03053036
INFO:root:[41,   650] training loss: 0.00108728
INFO:root:[41,   700] training loss: 0.00056332
INFO:root:[41,   750] training loss: 0.00044721
INFO:root:[41,   800] training loss: 0.00033553
INFO:root:[41,   850] training loss: 0.00027062
INFO:root:[41,   900] training loss: 0.03465917
INFO:root:[41,   950] training loss: 0.02234689
INFO:root:[41,  1000] training loss: 0.00148336
INFO:root:[41,  1050] training loss: 0.00061971
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3500    0.7000    0.4667        10
          G1     0.4853    0.4459    0.4648        74
   Metaphase     0.2689    0.9971    0.4236      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.2743      3872
   macro avg     0.3363    0.4966    0.3732      3872
weighted avg     0.0824    0.2743    0.1231      3872

INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.04431747
INFO:root:[42,   100] training loss: 0.01917447
INFO:root:[42,   150] training loss: 0.01725085
INFO:root:[42,   200] training loss: 0.01769902
INFO:root:[42,   250] training loss: 0.01967372
INFO:root:[42,   300] training loss: 0.01722296
INFO:root:[42,   350] training loss: 0.01533348
INFO:root:[42,   400] training loss: 0.00036132
INFO:root:[42,   450] training loss: 0.00011976
INFO:root:[42,   500] training loss: 0.00417287
INFO:root:[42,   550] training loss: 0.00917505
INFO:root:[42,   600] training loss: 0.02600168
INFO:root:[42,   650] training loss: 0.00074649
INFO:root:[42,   700] training loss: 0.00028953
INFO:root:[42,   750] training loss: 0.00018684
INFO:root:[42,   800] training loss: 0.00010316
INFO:root:[42,   850] training loss: 0.00005566
INFO:root:[42,   900] training loss: 0.03267995
INFO:root:[42,   950] training loss: 0.02078828
INFO:root:[42,  1000] training loss: 0.00141886
INFO:root:[42,  1050] training loss: 0.00065818
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.4211    0.8000    0.5517        10
          G1     0.4149    0.5270    0.4643        74
   Metaphase     0.2705    0.9961    0.4254      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.2756      3872
   macro avg     0.2533    0.4747    0.3202      3872
weighted avg     0.0812    0.2756    0.1234      3872

INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.03989498
INFO:root:[43,   100] training loss: 0.01786933
INFO:root:[43,   150] training loss: 0.01708454
INFO:root:[43,   200] training loss: 0.01711136
INFO:root:[43,   250] training loss: 0.01431907
INFO:root:[43,   300] training loss: 0.01747091
INFO:root:[43,   350] training loss: 0.01452215
INFO:root:[43,   400] training loss: 0.00009598
INFO:root:[43,   450] training loss: 0.00010860
INFO:root:[43,   500] training loss: 0.00415796
INFO:root:[43,   550] training loss: 0.00800954
INFO:root:[43,   600] training loss: 0.02605521
INFO:root:[43,   650] training loss: 0.00106695
INFO:root:[43,   700] training loss: 0.00056313
INFO:root:[43,   750] training loss: 0.00043595
INFO:root:[43,   800] training loss: 0.00031098
INFO:root:[43,   850] training loss: 0.00023511
INFO:root:[43,   900] training loss: 0.03114284
INFO:root:[43,   950] training loss: 0.02236642
INFO:root:[43,  1000] training loss: 0.00119020
INFO:root:[43,  1050] training loss: 0.00059890
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5385    0.7000    0.6087        10
          G1     0.3884    0.6351    0.4821        74
   Metaphase     0.2712    0.9931    0.4260      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.2769      3872
   macro avg     0.3497    0.5231    0.3963      3872
weighted avg     0.0817    0.2769    0.1244      3872

INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.03901846
INFO:root:[44,   100] training loss: 0.01896808
INFO:root:[44,   150] training loss: 0.01614674
INFO:root:[44,   200] training loss: 0.01913024
INFO:root:[44,   250] training loss: 0.01496931
INFO:root:[44,   300] training loss: 0.01646753
INFO:root:[44,   350] training loss: 0.01718625
INFO:root:[44,   400] training loss: 0.00011949
INFO:root:[44,   450] training loss: 0.00009307
INFO:root:[44,   500] training loss: 0.00402350
INFO:root:[44,   550] training loss: 0.00715632
INFO:root:[44,   600] training loss: 0.02514224
INFO:root:[44,   650] training loss: 0.00103943
INFO:root:[44,   700] training loss: 0.00054422
INFO:root:[44,   750] training loss: 0.00042154
INFO:root:[44,   800] training loss: 0.00033306
INFO:root:[44,   850] training loss: 0.00025088
INFO:root:[44,   900] training loss: 0.03210180
INFO:root:[44,   950] training loss: 0.02294649
INFO:root:[44,  1000] training loss: 0.00121429
INFO:root:[44,  1050] training loss: 0.00054851
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5000    0.6000    0.5455        10
          G1     0.4220    0.6216    0.5027        74
   Metaphase     0.2705    0.9941    0.4253      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.2766      3872
   macro avg     0.3489    0.5070    0.3901      3872
weighted avg     0.0820    0.2766    0.1245      3872

INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.03872569
INFO:root:[45,   100] training loss: 0.01810490
INFO:root:[45,   150] training loss: 0.01643577
INFO:root:[45,   200] training loss: 0.01659518
INFO:root:[45,   250] training loss: 0.01518412
INFO:root:[45,   300] training loss: 0.01599144
INFO:root:[45,   350] training loss: 0.01492005
INFO:root:[45,   400] training loss: 0.00008403
INFO:root:[45,   450] training loss: 0.00007465
INFO:root:[45,   500] training loss: 0.00378613
INFO:root:[45,   550] training loss: 0.00642202
INFO:root:[45,   600] training loss: 0.02409373
INFO:root:[45,   650] training loss: 0.00114865
INFO:root:[45,   700] training loss: 0.00058620
INFO:root:[45,   750] training loss: 0.00043854
INFO:root:[45,   800] training loss: 0.00033163
INFO:root:[45,   850] training loss: 0.00025550
INFO:root:[45,   900] training loss: 0.03093175
INFO:root:[45,   950] training loss: 0.02311861
INFO:root:[45,  1000] training loss: 0.00131034
INFO:root:[45,  1050] training loss: 0.00059245
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5333    0.8000    0.6400        10
          G1     0.4455    0.6081    0.5143        74
   Metaphase     0.2703    0.9941    0.4250      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.2769      3872
   macro avg     0.3356    0.5336    0.3899      3872
weighted avg     0.0823    0.2769    0.1247      3872

INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.03770703
INFO:root:[46,   100] training loss: 0.01674792
INFO:root:[46,   150] training loss: 0.01655475
INFO:root:[46,   200] training loss: 0.01743682
INFO:root:[46,   250] training loss: 0.01449355
INFO:root:[46,   300] training loss: 0.01739184
INFO:root:[46,   350] training loss: 0.01493125
INFO:root:[46,   400] training loss: 0.00025335
INFO:root:[46,   450] training loss: 0.00009430
INFO:root:[46,   500] training loss: 0.00351819
INFO:root:[46,   550] training loss: 0.00559479
INFO:root:[46,   600] training loss: 0.02346328
INFO:root:[46,   650] training loss: 0.00099298
INFO:root:[46,   700] training loss: 0.00053989
INFO:root:[46,   750] training loss: 0.00043155
INFO:root:[46,   800] training loss: 0.00033533
INFO:root:[46,   850] training loss: 0.00024883
INFO:root:[46,   900] training loss: 0.03088443
INFO:root:[46,   950] training loss: 0.02294079
INFO:root:[46,  1000] training loss: 0.00118491
INFO:root:[46,  1050] training loss: 0.00055593
INFO:root:              precision    recall  f1-score   support

          G2     0.6667    0.6667    0.6667         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5385    0.7000    0.6087        10
          G1     0.3087    0.6216    0.4126        74
   Metaphase     0.2730    0.9921    0.4282      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.2766      3872
   macro avg     0.3624    0.5686    0.4248      3872
weighted avg     0.0808    0.2766    0.1239      3872

INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.03428042
INFO:root:[47,   100] training loss: 0.01661256
INFO:root:[47,   150] training loss: 0.01515218
INFO:root:[47,   200] training loss: 0.01672895
INFO:root:[47,   250] training loss: 0.01532853
INFO:root:[47,   300] training loss: 0.01900477
INFO:root:[47,   350] training loss: 0.01627672
INFO:root:[47,   400] training loss: 0.00014532
INFO:root:[47,   450] training loss: 0.00009615
INFO:root:[47,   500] training loss: 0.00371182
INFO:root:[47,   550] training loss: 0.00469350
INFO:root:[47,   600] training loss: 0.02089113
INFO:root:[47,   650] training loss: 0.00116061
INFO:root:[47,   700] training loss: 0.00057325
INFO:root:[47,   750] training loss: 0.00043487
INFO:root:[47,   800] training loss: 0.00034630
INFO:root:[47,   850] training loss: 0.00026601
INFO:root:[47,   900] training loss: 0.02887352
INFO:root:[47,   950] training loss: 0.01950680
INFO:root:[47,  1000] training loss: 0.00099809
INFO:root:[47,  1050] training loss: 0.00044985
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.6667    0.5714         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.5833    0.7000    0.6364        10
          G1     0.2566    0.7838    0.3867        74
   Metaphase     0.2767    0.9853    0.4320      1018
   Telophase     1.0000    0.8333    0.9091         6

    accuracy                         0.2776      3872
   macro avg     0.3738    0.5670    0.4194      3872
weighted avg     0.0811    0.2776    0.1245      3872

INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.03150170
INFO:root:[48,   100] training loss: 0.01661300
INFO:root:[48,   150] training loss: 0.01607928
INFO:root:[48,   200] training loss: 0.01674954
INFO:root:[48,   250] training loss: 0.01403224
INFO:root:[48,   300] training loss: 0.01650215
INFO:root:[48,   350] training loss: 0.01382279
INFO:root:[48,   400] training loss: 0.00006993
INFO:root:[48,   450] training loss: 0.00006879
INFO:root:[48,   500] training loss: 0.00318235
INFO:root:[48,   550] training loss: 0.00401440
INFO:root:[48,   600] training loss: 0.01932863
INFO:root:[48,   650] training loss: 0.00096049
INFO:root:[48,   700] training loss: 0.00049460
INFO:root:[48,   750] training loss: 0.00043504
INFO:root:[48,   800] training loss: 0.00031066
INFO:root:[48,   850] training loss: 0.00025482
INFO:root:[48,   900] training loss: 0.02968812
INFO:root:[48,   950] training loss: 0.01776754
INFO:root:[48,  1000] training loss: 0.00087414
INFO:root:[48,  1050] training loss: 0.00044210
INFO:root:              precision    recall  f1-score   support

          G2     0.6667    0.6667    0.6667         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3750    0.6000    0.4615        10
          G1     0.2639    0.7703    0.3931        74
   Metaphase     0.2757    0.9833    0.4306      1018
   Telophase     0.8333    0.8333    0.8333         6

    accuracy                         0.2766      3872
   macro avg     0.3449    0.5505    0.3979      3872
weighted avg     0.0803    0.2766    0.1237      3872

INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.02941403
INFO:root:[49,   100] training loss: 0.01782408
INFO:root:[49,   150] training loss: 0.01673843
INFO:root:[49,   200] training loss: 0.01758853
INFO:root:[49,   250] training loss: 0.01643016
INFO:root:[49,   300] training loss: 0.01522873
INFO:root:[49,   350] training loss: 0.01529469
INFO:root:[49,   400] training loss: 0.00010925
INFO:root:[49,   450] training loss: 0.00007943
INFO:root:[49,   500] training loss: 0.00309718
INFO:root:[49,   550] training loss: 0.00351226
INFO:root:[49,   600] training loss: 0.01851231
INFO:root:[49,   650] training loss: 0.00099836
INFO:root:[49,   700] training loss: 0.00048930
INFO:root:[49,   750] training loss: 0.00039694
INFO:root:[49,   800] training loss: 0.00030222
INFO:root:[49,   850] training loss: 0.00024008
INFO:root:[49,   900] training loss: 0.02673292
INFO:root:[49,   950] training loss: 0.01740746
INFO:root:[49,  1000] training loss: 0.00069976
INFO:root:[49,  1050] training loss: 0.00038851
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.2333    0.7000    0.3500        10
          G1     0.1812    0.7027    0.2881        74
   Metaphase     0.2803    0.9764    0.4356      1018
   Telophase     0.7143    0.8333    0.7692         6

    accuracy                         0.2735      3872
   macro avg     0.2727    0.5065    0.3204      3872
weighted avg     0.0793    0.2735    0.1224      3872

INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.02987701
INFO:root:[50,   100] training loss: 0.01743910
INFO:root:[50,   150] training loss: 0.01661533
INFO:root:[50,   200] training loss: 0.01666642
INFO:root:[50,   250] training loss: 0.01334953
INFO:root:[50,   300] training loss: 0.01520996
INFO:root:[50,   350] training loss: 0.01394945
INFO:root:[50,   400] training loss: 0.00044989
INFO:root:[50,   450] training loss: 0.00016261
INFO:root:[50,   500] training loss: 0.00333968
INFO:root:[50,   550] training loss: 0.00295389
INFO:root:[50,   600] training loss: 0.01873824
INFO:root:[50,   650] training loss: 0.00073457
INFO:root:[50,   700] training loss: 0.00038189
INFO:root:[50,   750] training loss: 0.00031213
INFO:root:[50,   800] training loss: 0.00023142
INFO:root:[50,   850] training loss: 0.00017769
INFO:root:[50,   900] training loss: 0.02832864
INFO:root:[50,   950] training loss: 0.01459812
INFO:root:[50,  1000] training loss: 0.00067026
INFO:root:[50,  1050] training loss: 0.00034349
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3333    0.6000    0.4286        10
          G1     0.2118    0.7297    0.3283        74
   Metaphase     0.2769    0.9764    0.4314      1018
   Telophase     0.7143    0.8333    0.7692         6

    accuracy                         0.2738      3872
   macro avg     0.2909    0.4961    0.3368      3872
weighted avg     0.0792    0.2738    0.1223      3872

INFO:root:epoch50
INFO:root:[51,    50] training loss: 0.03062081
INFO:root:[51,   100] training loss: 0.01585045
INFO:root:[51,   150] training loss: 0.01509488
INFO:root:[51,   200] training loss: 0.01747343
INFO:root:[51,   250] training loss: 0.01409011
INFO:root:[51,   300] training loss: 0.01682657
INFO:root:[51,   350] training loss: 0.01576351
INFO:root:[51,   400] training loss: 0.00007334
INFO:root:[51,   450] training loss: 0.00007730
INFO:root:[51,   500] training loss: 0.00289939
INFO:root:[51,   550] training loss: 0.00282319
INFO:root:[51,   600] training loss: 0.01712634
INFO:root:[51,   650] training loss: 0.00079774
INFO:root:[51,   700] training loss: 0.00042462
INFO:root:[51,   750] training loss: 0.00034984
INFO:root:[51,   800] training loss: 0.00024279
INFO:root:[51,   850] training loss: 0.00020674
INFO:root:[51,   900] training loss: 0.02572005
INFO:root:[51,   950] training loss: 0.01429860
INFO:root:[51,  1000] training loss: 0.00056510
INFO:root:[51,  1050] training loss: 0.00024014
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.3333    0.4000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3077    0.8000    0.4444        10
          G1     0.1714    0.7297    0.2776        74
   Metaphase     0.2788    0.9646    0.4326      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.2714      3872
   macro avg     0.3022    0.5468    0.3540      3872
weighted avg     0.0791    0.2714    0.1219      3872

INFO:root:epoch51
INFO:root:[52,    50] training loss: 0.03089214
INFO:root:[52,   100] training loss: 0.01639571
INFO:root:[52,   150] training loss: 0.01605234
INFO:root:[52,   200] training loss: 0.01550665
INFO:root:[52,   250] training loss: 0.01360425
INFO:root:[52,   300] training loss: 0.01726960
INFO:root:[52,   350] training loss: 0.01654558
INFO:root:[52,   400] training loss: 0.00009352
INFO:root:[52,   450] training loss: 0.00007772
INFO:root:[52,   500] training loss: 0.00260591
INFO:root:[52,   550] training loss: 0.00235555
INFO:root:[52,   600] training loss: 0.01729687
INFO:root:[52,   650] training loss: 0.00085495
INFO:root:[52,   700] training loss: 0.00044296
INFO:root:[52,   750] training loss: 0.00037673
INFO:root:[52,   800] training loss: 0.00028350
INFO:root:[52,   850] training loss: 0.00019842
INFO:root:[52,   900] training loss: 0.02889518
INFO:root:[52,   950] training loss: 0.01527485
INFO:root:[52,  1000] training loss: 0.00048642
INFO:root:[52,  1050] training loss: 0.00030120
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.2581    0.8000    0.3902        10
          G1     0.2848    0.6351    0.3933        74
   Metaphase     0.2728    0.9833    0.4270      1018
   Telophase     0.8000    0.6667    0.7273         6

    accuracy                         0.2738      3872
   macro avg     0.2308    0.4407    0.2768      3872
weighted avg     0.0791    0.2738    0.1219      3872

INFO:root:epoch52
INFO:root:[53,    50] training loss: 0.03112036
INFO:root:[53,   100] training loss: 0.01776013
INFO:root:[53,   150] training loss: 0.01826247
INFO:root:[53,   200] training loss: 0.01673288
INFO:root:[53,   250] training loss: 0.01333914
INFO:root:[53,   300] training loss: 0.01782866
INFO:root:[53,   350] training loss: 0.01608170
INFO:root:[53,   400] training loss: 0.00019474
INFO:root:[53,   450] training loss: 0.00013895
INFO:root:[53,   500] training loss: 0.00277150
INFO:root:[53,   550] training loss: 0.00214895
INFO:root:[53,   600] training loss: 0.01631895
INFO:root:[53,   650] training loss: 0.00072177
INFO:root:[53,   700] training loss: 0.00039910
INFO:root:[53,   750] training loss: 0.00034709
INFO:root:[53,   800] training loss: 0.00026763
INFO:root:[53,   850] training loss: 0.00018427
INFO:root:[53,   900] training loss: 0.02672361
INFO:root:[53,   950] training loss: 0.01369650
INFO:root:[53,  1000] training loss: 0.00050869
INFO:root:[53,  1050] training loss: 0.00030128
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.0000    0.0000    0.0000      1039
    Anaphase     0.3333    0.8000    0.4706        10
          G1     0.2070    0.7162    0.3212        74
   Metaphase     0.2760    0.9725    0.4300      1018
   Telophase     1.0000    0.6667    0.8000         6

    accuracy                         0.2725      3872
   macro avg     0.2595    0.4508    0.2888      3872
weighted avg     0.0789    0.2725    0.1216      3872

INFO:root:epoch53
INFO:root:[54,    50] training loss: 0.03048946
INFO:root:[54,   100] training loss: 0.01773762
INFO:root:[54,   150] training loss: 0.01661144
INFO:root:[54,   200] training loss: 0.01445293
INFO:root:[54,   250] training loss: 0.01271696
INFO:root:[54,   300] training loss: 0.01686905
INFO:root:[54,   350] training loss: 0.01414245
INFO:root:[54,   400] training loss: 0.00010987
INFO:root:[54,   450] training loss: 0.00008662
INFO:root:[54,   500] training loss: 0.00256578
INFO:root:[54,   550] training loss: 0.00200538
INFO:root:[54,   600] training loss: 0.01439136
INFO:root:[54,   650] training loss: 0.00064064
INFO:root:[54,   700] training loss: 0.00033487
INFO:root:[54,   750] training loss: 0.00030115
INFO:root:[54,   800] training loss: 0.00022485
INFO:root:[54,   850] training loss: 0.00014903
INFO:root:[54,   900] training loss: 0.02258261
INFO:root:[54,   950] training loss: 0.01097256
INFO:root:[54,  1000] training loss: 0.00046334
INFO:root:[54,  1050] training loss: 0.00021857
INFO:root:              precision    recall  f1-score   support

          G2     0.6667    0.6667    0.6667         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     1.0000    0.0058    0.0115      1039
    Anaphase     0.4615    0.6000    0.5217        10
          G1     0.1518    0.8514    0.2577        74
   Metaphase     0.2842    0.9568    0.4382      1018
   Telophase     0.6250    0.8333    0.7143         6

    accuracy                         0.2727      3872
   macro avg     0.4556    0.5591    0.3729      3872
weighted avg     0.3486    0.2727    0.1262      3872

INFO:root:epoch54
INFO:root:[55,    50] training loss: 0.02950281
INFO:root:[55,   100] training loss: 0.01652463
INFO:root:[55,   150] training loss: 0.01650693
INFO:root:[55,   200] training loss: 0.01649371
INFO:root:[55,   250] training loss: 0.01294169
INFO:root:[55,   300] training loss: 0.01527827
INFO:root:[55,   350] training loss: 0.01383085
INFO:root:[55,   400] training loss: 0.00007118
INFO:root:[55,   450] training loss: 0.00007312
INFO:root:[55,   500] training loss: 0.00240579
INFO:root:[55,   550] training loss: 0.00153386
INFO:root:[55,   600] training loss: 0.01340919
INFO:root:[55,   650] training loss: 0.00051595
INFO:root:[55,   700] training loss: 0.00028868
INFO:root:[55,   750] training loss: 0.00033518
INFO:root:[55,   800] training loss: 0.00023033
INFO:root:[55,   850] training loss: 0.00016610
INFO:root:[55,   900] training loss: 0.02100380
INFO:root:[55,   950] training loss: 0.01422275
INFO:root:[55,  1000] training loss: 0.00037518
INFO:root:[55,  1050] training loss: 0.00018793
INFO:root:              precision    recall  f1-score   support

          G2     0.3333    0.3333    0.3333         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.9167    0.0212    0.0414      1039
    Anaphase     0.2759    0.8000    0.4103        10
          G1     0.1821    0.6892    0.2881        74
   Metaphase     0.2795    0.9686    0.4338      1018
   Telophase     0.6250    0.8333    0.7143         6

    accuracy                         0.2771      3872
   macro avg     0.3732    0.5208    0.3173      3872
weighted avg     0.3249    0.2771    0.1331      3872

INFO:root:epoch55
INFO:root:[56,    50] training loss: 0.02745066
INFO:root:[56,   100] training loss: 0.01737974
INFO:root:[56,   150] training loss: 0.01776205
INFO:root:[56,   200] training loss: 0.01628817
INFO:root:[56,   250] training loss: 0.01337633
INFO:root:[56,   300] training loss: 0.01411753
INFO:root:[56,   350] training loss: 0.01518923
INFO:root:[56,   400] training loss: 0.00011552
INFO:root:[56,   450] training loss: 0.00008026
INFO:root:[56,   500] training loss: 0.00319097
INFO:root:[56,   550] training loss: 0.00223921
INFO:root:[56,   600] training loss: 0.01369566
INFO:root:[56,   650] training loss: 0.00067847
INFO:root:[56,   700] training loss: 0.00037451
INFO:root:[56,   750] training loss: 0.00045068
INFO:root:[56,   800] training loss: 0.00029462
INFO:root:[56,   850] training loss: 0.00022809
INFO:root:[56,   900] training loss: 0.02353771
INFO:root:[56,   950] training loss: 0.01399315
INFO:root:[56,  1000] training loss: 0.00029664
INFO:root:[56,  1050] training loss: 0.00014898
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.9549    0.1222    0.2167      1039
    Anaphase     0.5455    0.6000    0.5714        10
          G1     0.1315    0.8514    0.2278        74
   Metaphase     0.2925    0.9293    0.4450      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.2973      3872
   macro avg     0.4463    0.6433    0.4230      3872
weighted avg     0.3385    0.2973    0.1827      3872

INFO:root:epoch56
INFO:root:[57,    50] training loss: 0.02546209
INFO:root:[57,   100] training loss: 0.01538918
INFO:root:[57,   150] training loss: 0.01440140
INFO:root:[57,   200] training loss: 0.01528848
INFO:root:[57,   250] training loss: 0.01281839
INFO:root:[57,   300] training loss: 0.01565977
INFO:root:[57,   350] training loss: 0.01396955
INFO:root:[57,   400] training loss: 0.00006107
INFO:root:[57,   450] training loss: 0.00005624
INFO:root:[57,   500] training loss: 0.00293420
INFO:root:[57,   550] training loss: 0.00170959
INFO:root:[57,   600] training loss: 0.01292503
INFO:root:[57,   650] training loss: 0.00065365
INFO:root:[57,   700] training loss: 0.00037924
INFO:root:[57,   750] training loss: 0.00040598
INFO:root:[57,   800] training loss: 0.00034357
INFO:root:[57,   850] training loss: 0.00023290
INFO:root:[57,   900] training loss: 0.02260602
INFO:root:[57,   950] training loss: 0.01270763
INFO:root:[57,  1000] training loss: 0.00029146
INFO:root:[57,  1050] training loss: 0.00015993
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.9421    0.1097    0.1966      1039
    Anaphase     0.4000    0.6000    0.4800        10
          G1     0.1128    0.7973    0.1977        74
   Metaphase     0.2945    0.9253    0.4468      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.2918      3872
   macro avg     0.4309    0.6332    0.4101      3872
weighted avg     0.3349    0.2918    0.1770      3872

INFO:root:epoch57
INFO:root:[58,    50] training loss: 0.02559984
INFO:root:[58,   100] training loss: 0.01488139
INFO:root:[58,   150] training loss: 0.01613421
INFO:root:[58,   200] training loss: 0.01516029
INFO:root:[58,   250] training loss: 0.01279885
INFO:root:[58,   300] training loss: 0.01425809
INFO:root:[58,   350] training loss: 0.01259302
INFO:root:[58,   400] training loss: 0.00008363
INFO:root:[58,   450] training loss: 0.00006193
INFO:root:[58,   500] training loss: 0.00246625
INFO:root:[58,   550] training loss: 0.00141796
INFO:root:[58,   600] training loss: 0.01057198
INFO:root:[58,   650] training loss: 0.00058469
INFO:root:[58,   700] training loss: 0.00034354
INFO:root:[58,   750] training loss: 0.00043031
INFO:root:[58,   800] training loss: 0.00031001
INFO:root:[58,   850] training loss: 0.00022037
INFO:root:[58,   900] training loss: 0.02114754
INFO:root:[58,   950] training loss: 0.01068583
INFO:root:[58,  1000] training loss: 0.00035416
INFO:root:[58,  1050] training loss: 0.00015243
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.8783    0.0972    0.1750      1039
    Anaphase     0.2571    0.9000    0.4000        10
          G1     0.1002    0.7973    0.1780        74
   Metaphase     0.2984    0.9145    0.4500      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.2864      3872
   macro avg     0.4215    0.6727    0.4086      3872
weighted avg     0.3183    0.2864    0.1716      3872

INFO:root:epoch58
INFO:root:[59,    50] training loss: 0.02539489
INFO:root:[59,   100] training loss: 0.01883584
INFO:root:[59,   150] training loss: 0.01457146
INFO:root:[59,   200] training loss: 0.01512641
INFO:root:[59,   250] training loss: 0.01389682
INFO:root:[59,   300] training loss: 0.01469812
INFO:root:[59,   350] training loss: 0.01275145
INFO:root:[59,   400] training loss: 0.00010031
INFO:root:[59,   450] training loss: 0.00007353
INFO:root:[59,   500] training loss: 0.00247348
INFO:root:[59,   550] training loss: 0.00147297
INFO:root:[59,   600] training loss: 0.00918438
INFO:root:[59,   650] training loss: 0.00050819
INFO:root:[59,   700] training loss: 0.00026663
INFO:root:[59,   750] training loss: 0.00041291
INFO:root:[59,   800] training loss: 0.00032524
INFO:root:[59,   850] training loss: 0.00017028
INFO:root:[59,   900] training loss: 0.01952348
INFO:root:[59,   950] training loss: 0.00907629
INFO:root:[59,  1000] training loss: 0.00024864
INFO:root:[59,  1050] training loss: 0.00011341
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.8864    0.1126    0.1998      1039
    Anaphase     0.7778    0.7000    0.7368        10
          G1     0.1192    0.9054    0.2107        74
   Metaphase     0.2957    0.9165    0.4472      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.2926      3872
   macro avg     0.4780    0.6621    0.4492      3872
weighted avg     0.3214    0.2926    0.1789      3872

INFO:root:epoch59
INFO:root:[60,    50] training loss: 0.02264673
INFO:root:[60,   100] training loss: 0.01489138
INFO:root:[60,   150] training loss: 0.01648330
INFO:root:[60,   200] training loss: 0.01526475
INFO:root:[60,   250] training loss: 0.01257516
INFO:root:[60,   300] training loss: 0.01373545
INFO:root:[60,   350] training loss: 0.01285548
INFO:root:[60,   400] training loss: 0.00006133
INFO:root:[60,   450] training loss: 0.00006008
INFO:root:[60,   500] training loss: 0.00227482
INFO:root:[60,   550] training loss: 0.00126690
INFO:root:[60,   600] training loss: 0.00761881
INFO:root:[60,   650] training loss: 0.00042042
INFO:root:[60,   700] training loss: 0.00023946
INFO:root:[60,   750] training loss: 0.00039022
INFO:root:[60,   800] training loss: 0.00023734
INFO:root:[60,   850] training loss: 0.00017896
INFO:root:[60,   900] training loss: 0.02164376
INFO:root:[60,   950] training loss: 0.00746470
INFO:root:[60,  1000] training loss: 0.00027750
INFO:root:[60,  1050] training loss: 0.00012798
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.0000    0.0000    0.0000      1722
    Prophase     0.7709    0.1328    0.2266      1039
    Anaphase     0.4444    0.8000    0.5714        10
          G1     0.0959    0.8514    0.1724        74
   Metaphase     0.2965    0.8752    0.4430      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.2864      3872
   macro avg     0.4321    0.6656    0.4386      3872
weighted avg     0.2894    0.2864    0.1839      3872

INFO:root:epoch60
INFO:root:[61,    50] training loss: 0.02157721
INFO:root:[61,   100] training loss: 0.01565934
INFO:root:[61,   150] training loss: 0.01448629
INFO:root:[61,   200] training loss: 0.01461911
INFO:root:[61,   250] training loss: 0.01381648
INFO:root:[61,   300] training loss: 0.01607515
INFO:root:[61,   350] training loss: 0.01389066
INFO:root:[61,   400] training loss: 0.00008233
INFO:root:[61,   450] training loss: 0.00005005
INFO:root:[61,   500] training loss: 0.00198343
INFO:root:[61,   550] training loss: 0.00121159
INFO:root:[61,   600] training loss: 0.00707426
INFO:root:[61,   650] training loss: 0.00035230
INFO:root:[61,   700] training loss: 0.00018232
INFO:root:[61,   750] training loss: 0.00034680
INFO:root:[61,   800] training loss: 0.00022905
INFO:root:[61,   850] training loss: 0.00014294
INFO:root:[61,   900] training loss: 0.01880988
INFO:root:[61,   950] training loss: 0.00539575
INFO:root:[61,  1000] training loss: 0.00032391
INFO:root:[61,  1050] training loss: 0.00013747
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.8627    0.0256    0.0496      1722
    Prophase     0.7763    0.2204    0.3433      1039
    Anaphase     0.4286    0.9000    0.5806        10
          G1     0.1165    0.8378    0.2046        74
   Metaphase     0.2987    0.8684    0.4446      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.3195      3872
   macro avg     0.5356    0.6932    0.4533      3872
weighted avg     0.6754    0.3195    0.2383      3872

INFO:root:epoch61
INFO:root:[62,    50] training loss: 0.01958192
INFO:root:[62,   100] training loss: 0.01450518
INFO:root:[62,   150] training loss: 0.01417730
INFO:root:[62,   200] training loss: 0.01308916
INFO:root:[62,   250] training loss: 0.01252852
INFO:root:[62,   300] training loss: 0.01427933
INFO:root:[62,   350] training loss: 0.01256152
INFO:root:[62,   400] training loss: 0.00004361
INFO:root:[62,   450] training loss: 0.00004009
INFO:root:[62,   500] training loss: 0.00207695
INFO:root:[62,   550] training loss: 0.00104075
INFO:root:[62,   600] training loss: 0.00576096
INFO:root:[62,   650] training loss: 0.00037573
INFO:root:[62,   700] training loss: 0.00023824
INFO:root:[62,   750] training loss: 0.00040421
INFO:root:[62,   800] training loss: 0.00027758
INFO:root:[62,   850] training loss: 0.00017948
INFO:root:[62,   900] training loss: 0.01791254
INFO:root:[62,   950] training loss: 0.00554130
INFO:root:[62,  1000] training loss: 0.00032812
INFO:root:[62,  1050] training loss: 0.00012578
INFO:root:              precision    recall  f1-score   support

          G2     0.6667    0.6667    0.6667         3
           S     0.9167    0.0256    0.0497      1722
    Prophase     0.7452    0.3349    0.4622      1039
    Anaphase     0.4286    0.9000    0.5806        10
          G1     0.1198    0.8514    0.2100        74
   Metaphase     0.3002    0.8251    0.4403      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.3388      3872
   macro avg     0.5491    0.6577    0.4585      3872
weighted avg     0.6915    0.3388    0.2691      3872

INFO:root:epoch62
INFO:root:[63,    50] training loss: 0.01820129
INFO:root:[63,   100] training loss: 0.01346345
INFO:root:[63,   150] training loss: 0.01433897
INFO:root:[63,   200] training loss: 0.01385636
INFO:root:[63,   250] training loss: 0.01180430
INFO:root:[63,   300] training loss: 0.01494112
INFO:root:[63,   350] training loss: 0.01229450
INFO:root:[63,   400] training loss: 0.00005208
INFO:root:[63,   450] training loss: 0.00006156
INFO:root:[63,   500] training loss: 0.00177585
INFO:root:[63,   550] training loss: 0.00122033
INFO:root:[63,   600] training loss: 0.00515478
INFO:root:[63,   650] training loss: 0.00033386
INFO:root:[63,   700] training loss: 0.00019244
INFO:root:[63,   750] training loss: 0.00037317
INFO:root:[63,   800] training loss: 0.00023046
INFO:root:[63,   850] training loss: 0.00016523
INFO:root:[63,   900] training loss: 0.01663826
INFO:root:[63,   950] training loss: 0.00648281
INFO:root:[63,  1000] training loss: 0.00034665
INFO:root:[63,  1050] training loss: 0.00013633
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9504    0.0668    0.1248      1722
    Prophase     0.7523    0.2397    0.3635      1039
    Anaphase     0.4706    0.8000    0.5926        10
          G1     0.1184    0.8514    0.2079        74
   Metaphase     0.3122    0.8762    0.4604      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.3450      3872
   macro avg     0.5529    0.6906    0.4713      3872
weighted avg     0.7116    0.3450    0.2814      3872

INFO:root:epoch63
INFO:root:[64,    50] training loss: 0.01784439
INFO:root:[64,   100] training loss: 0.01429048
INFO:root:[64,   150] training loss: 0.01485142
INFO:root:[64,   200] training loss: 0.01388363
INFO:root:[64,   250] training loss: 0.01108770
INFO:root:[64,   300] training loss: 0.01327283
INFO:root:[64,   350] training loss: 0.01232043
INFO:root:[64,   400] training loss: 0.00004997
INFO:root:[64,   450] training loss: 0.00004190
INFO:root:[64,   500] training loss: 0.00145966
INFO:root:[64,   550] training loss: 0.00095081
INFO:root:[64,   600] training loss: 0.00405079
INFO:root:[64,   650] training loss: 0.00028081
INFO:root:[64,   700] training loss: 0.00016427
INFO:root:[64,   750] training loss: 0.00040028
INFO:root:[64,   800] training loss: 0.00025493
INFO:root:[64,   850] training loss: 0.00013251
INFO:root:[64,   900] training loss: 0.01718352
INFO:root:[64,   950] training loss: 0.00476554
INFO:root:[64,  1000] training loss: 0.00045051
INFO:root:[64,  1050] training loss: 0.00013102
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.6667    0.5714         3
           S     0.9353    0.3612    0.5212      1722
    Prophase     0.7734    0.3744    0.5045      1039
    Anaphase     0.5333    0.8000    0.6400        10
          G1     0.1260    0.8514    0.2195        74
   Metaphase     0.3789    0.8094    0.5161      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.4943      3872
   macro avg     0.5496    0.6947    0.5318      3872
weighted avg     0.7282    0.4943    0.5103      3872

INFO:root:epoch64
INFO:root:[65,    50] training loss: 0.01806522
INFO:root:[65,   100] training loss: 0.01402270
INFO:root:[65,   150] training loss: 0.01345592
INFO:root:[65,   200] training loss: 0.01180646
INFO:root:[65,   250] training loss: 0.01509018
INFO:root:[65,   300] training loss: 0.01949100
INFO:root:[65,   350] training loss: 0.01452652
INFO:root:[65,   400] training loss: 0.00016512
INFO:root:[65,   450] training loss: 0.00005053
INFO:root:[65,   500] training loss: 0.00128046
INFO:root:[65,   550] training loss: 0.00110419
INFO:root:[65,   600] training loss: 0.00407785
INFO:root:[65,   650] training loss: 0.00030534
INFO:root:[65,   700] training loss: 0.00014941
INFO:root:[65,   750] training loss: 0.00043932
INFO:root:[65,   800] training loss: 0.00020837
INFO:root:[65,   850] training loss: 0.00016516
INFO:root:[65,   900] training loss: 0.01829583
INFO:root:[65,   950] training loss: 0.00521925
INFO:root:[65,  1000] training loss: 0.00035215
INFO:root:[65,  1050] training loss: 0.00010036
INFO:root:              precision    recall  f1-score   support

          G2     0.0000    0.0000    0.0000         3
           S     0.9502    0.4100    0.5728      1722
    Prophase     0.7286    0.4678    0.5698      1039
    Anaphase     0.3684    0.7000    0.4828        10
          G1     0.1416    0.8378    0.2422        74
   Metaphase     0.3994    0.7819    0.5287      1018
   Telophase     0.5455    1.0000    0.7059         6

    accuracy                         0.5328      3872
   macro avg     0.4477    0.5996    0.4432      3872
weighted avg     0.7276    0.5328    0.5536      3872

INFO:root:epoch65
INFO:root:[66,    50] training loss: 0.01953501
INFO:root:[66,   100] training loss: 0.01415239
INFO:root:[66,   150] training loss: 0.01515881
INFO:root:[66,   200] training loss: 0.01297025
INFO:root:[66,   250] training loss: 0.01374793
INFO:root:[66,   300] training loss: 0.01325420
INFO:root:[66,   350] training loss: 0.01198426
INFO:root:[66,   400] training loss: 0.00004719
INFO:root:[66,   450] training loss: 0.00004324
INFO:root:[66,   500] training loss: 0.00092643
INFO:root:[66,   550] training loss: 0.00081699
INFO:root:[66,   600] training loss: 0.00395298
INFO:root:[66,   650] training loss: 0.00023786
INFO:root:[66,   700] training loss: 0.00013971
INFO:root:[66,   750] training loss: 0.00037381
INFO:root:[66,   800] training loss: 0.00023367
INFO:root:[66,   850] training loss: 0.00013201
INFO:root:[66,   900] training loss: 0.01561767
INFO:root:[66,   950] training loss: 0.00397788
INFO:root:[66,  1000] training loss: 0.00027092
INFO:root:[66,  1050] training loss: 0.00008830
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9517    0.4350    0.5971      1722
    Prophase     0.8003    0.4860    0.6048      1039
    Anaphase     0.5294    0.9000    0.6667        10
          G1     0.1537    0.8514    0.2603        74
   Metaphase     0.4133    0.8173    0.5490      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.5597      3872
   macro avg     0.5879    0.7842    0.6040      3872
weighted avg     0.7525    0.5597    0.5807      3872

INFO:root:epoch66
INFO:root:[67,    50] training loss: 0.01610500
INFO:root:[67,   100] training loss: 0.01473709
INFO:root:[67,   150] training loss: 0.01316517
INFO:root:[67,   200] training loss: 0.01471090
INFO:root:[67,   250] training loss: 0.01169161
INFO:root:[67,   300] training loss: 0.01314865
INFO:root:[67,   350] training loss: 0.01193429
INFO:root:[67,   400] training loss: 0.00003218
INFO:root:[67,   450] training loss: 0.00003251
INFO:root:[67,   500] training loss: 0.00085197
INFO:root:[67,   550] training loss: 0.00075320
INFO:root:[67,   600] training loss: 0.00290050
INFO:root:[67,   650] training loss: 0.00027000
INFO:root:[67,   700] training loss: 0.00015975
INFO:root:[67,   750] training loss: 0.00041614
INFO:root:[67,   800] training loss: 0.00020265
INFO:root:[67,   850] training loss: 0.00012073
INFO:root:[67,   900] training loss: 0.01526895
INFO:root:[67,   950] training loss: 0.00412886
INFO:root:[67,  1000] training loss: 0.00030149
INFO:root:[67,  1050] training loss: 0.00009913
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.6667    0.5714         3
           S     0.9346    0.5145    0.6637      1722
    Prophase     0.8080    0.5467    0.6521      1039
    Anaphase     0.4500    0.9000    0.6000        10
          G1     0.1703    0.7432    0.2771        74
   Metaphase     0.4345    0.7957    0.5621      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.6033      3872
   macro avg     0.5568    0.7381    0.5823      3872
weighted avg     0.7524    0.6033    0.6264      3872

INFO:root:epoch67
INFO:root:[68,    50] training loss: 0.01475456
INFO:root:[68,   100] training loss: 0.01355019
INFO:root:[68,   150] training loss: 0.01292990
INFO:root:[68,   200] training loss: 0.01289472
INFO:root:[68,   250] training loss: 0.01192807
INFO:root:[68,   300] training loss: 0.01359890
INFO:root:[68,   350] training loss: 0.01267809
INFO:root:[68,   400] training loss: 0.00002894
INFO:root:[68,   450] training loss: 0.00002895
INFO:root:[68,   500] training loss: 0.00109133
INFO:root:[68,   550] training loss: 0.00066312
INFO:root:[68,   600] training loss: 0.00285769
INFO:root:[68,   650] training loss: 0.00023802
INFO:root:[68,   700] training loss: 0.00013674
INFO:root:[68,   750] training loss: 0.00036721
INFO:root:[68,   800] training loss: 0.00023415
INFO:root:[68,   850] training loss: 0.00010391
INFO:root:[68,   900] training loss: 0.01511086
INFO:root:[68,   950] training loss: 0.00319130
INFO:root:[68,  1000] training loss: 0.00031051
INFO:root:[68,  1050] training loss: 0.00010205
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9390    0.5627    0.7037      1722
    Prophase     0.7109    0.5207    0.6011      1039
    Anaphase     0.5000    0.5000    0.5000        10
          G1     0.1438    0.8649    0.2466        74
   Metaphase     0.4522    0.7151    0.5540      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.5981      3872
   macro avg     0.5851    0.7376    0.6018      3872
weighted avg     0.7328    0.5981    0.6278      3872

INFO:root:epoch68
INFO:root:[69,    50] training loss: 0.01478853
INFO:root:[69,   100] training loss: 0.01353681
INFO:root:[69,   150] training loss: 0.01481298
INFO:root:[69,   200] training loss: 0.01264848
INFO:root:[69,   250] training loss: 0.01279402
INFO:root:[69,   300] training loss: 0.01326575
INFO:root:[69,   350] training loss: 0.01211764
INFO:root:[69,   400] training loss: 0.00004620
INFO:root:[69,   450] training loss: 0.00003899
INFO:root:[69,   500] training loss: 0.00079830
INFO:root:[69,   550] training loss: 0.00067706
INFO:root:[69,   600] training loss: 0.00238786
INFO:root:[69,   650] training loss: 0.00022825
INFO:root:[69,   700] training loss: 0.00013844
INFO:root:[69,   750] training loss: 0.00043421
INFO:root:[69,   800] training loss: 0.00020980
INFO:root:[69,   850] training loss: 0.00013412
INFO:root:[69,   900] training loss: 0.01260746
INFO:root:[69,   950] training loss: 0.00317538
INFO:root:[69,  1000] training loss: 0.00028529
INFO:root:[69,  1050] training loss: 0.00008597
INFO:root:              precision    recall  f1-score   support

          G2     0.2500    1.0000    0.4000         3
           S     0.9254    0.6272    0.7477      1722
    Prophase     0.6941    0.6420    0.6670      1039
    Anaphase     0.4706    0.8000    0.5926        10
          G1     0.1745    0.8514    0.2897        74
   Metaphase     0.4747    0.6267    0.5402      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.6366      3872
   macro avg     0.5128    0.7925    0.5696      3872
weighted avg     0.7283    0.6366    0.6621      3872

INFO:root:epoch69
INFO:root:[70,    50] training loss: 0.01455762
INFO:root:[70,   100] training loss: 0.01252482
INFO:root:[70,   150] training loss: 0.01485669
INFO:root:[70,   200] training loss: 0.01376702
INFO:root:[70,   250] training loss: 0.01281681
INFO:root:[70,   300] training loss: 0.01316569
INFO:root:[70,   350] training loss: 0.01149714
INFO:root:[70,   400] training loss: 0.00005474
INFO:root:[70,   450] training loss: 0.00003325
INFO:root:[70,   500] training loss: 0.00070023
INFO:root:[70,   550] training loss: 0.00066685
INFO:root:[70,   600] training loss: 0.00211986
INFO:root:[70,   650] training loss: 0.00020352
INFO:root:[70,   700] training loss: 0.00012399
INFO:root:[70,   750] training loss: 0.00040767
INFO:root:[70,   800] training loss: 0.00018538
INFO:root:[70,   850] training loss: 0.00010605
INFO:root:[70,   900] training loss: 0.01185873
INFO:root:[70,   950] training loss: 0.00322155
INFO:root:[70,  1000] training loss: 0.00030499
INFO:root:[70,  1050] training loss: 0.00009639
INFO:root:              precision    recall  f1-score   support

          G2     0.3750    1.0000    0.5455         3
           S     0.9154    0.6847    0.7834      1722
    Prophase     0.7450    0.6497    0.6941      1039
    Anaphase     0.3448    1.0000    0.5128        10
          G1     0.1806    0.7297    0.2895        74
   Metaphase     0.5180    0.6778    0.5872      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.6759      3872
   macro avg     0.5256    0.8203    0.5946      3872
weighted avg     0.7488    0.6759    0.6975      3872

INFO:root:epoch70
INFO:root:[71,    50] training loss: 0.01519826
INFO:root:[71,   100] training loss: 0.01284749
INFO:root:[71,   150] training loss: 0.01303810
INFO:root:[71,   200] training loss: 0.01162584
INFO:root:[71,   250] training loss: 0.01085531
INFO:root:[71,   300] training loss: 0.01290745
INFO:root:[71,   350] training loss: 0.01265794
INFO:root:[71,   400] training loss: 0.00004027
INFO:root:[71,   450] training loss: 0.00009880
INFO:root:[71,   500] training loss: 0.00082598
INFO:root:[71,   550] training loss: 0.00060139
INFO:root:[71,   600] training loss: 0.00210498
INFO:root:[71,   650] training loss: 0.00016999
INFO:root:[71,   700] training loss: 0.00011153
INFO:root:[71,   750] training loss: 0.00033562
INFO:root:[71,   800] training loss: 0.00016696
INFO:root:[71,   850] training loss: 0.00012213
INFO:root:[71,   900] training loss: 0.01260161
INFO:root:[71,   950] training loss: 0.00268801
INFO:root:[71,  1000] training loss: 0.00028839
INFO:root:[71,  1050] training loss: 0.00008728
INFO:root:              precision    recall  f1-score   support

          G2     0.4000    0.6667    0.5000         3
           S     0.9209    0.6829    0.7843      1722
    Prophase     0.7238    0.6458    0.6826      1039
    Anaphase     0.3750    0.9000    0.5294        10
          G1     0.1707    0.7703    0.2794        74
   Metaphase     0.5201    0.6621    0.5825      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.6702      3872
   macro avg     0.5396    0.7611    0.5940      3872
weighted avg     0.7461    0.6702    0.6934      3872

INFO:root:epoch71
INFO:root:[72,    50] training loss: 0.01364774
INFO:root:[72,   100] training loss: 0.01216123
INFO:root:[72,   150] training loss: 0.01298440
INFO:root:[72,   200] training loss: 0.01341961
INFO:root:[72,   250] training loss: 0.01015723
INFO:root:[72,   300] training loss: 0.01219509
INFO:root:[72,   350] training loss: 0.01071383
INFO:root:[72,   400] training loss: 0.00003315
INFO:root:[72,   450] training loss: 0.00002775
INFO:root:[72,   500] training loss: 0.00048249
INFO:root:[72,   550] training loss: 0.00063599
INFO:root:[72,   600] training loss: 0.00152660
INFO:root:[72,   650] training loss: 0.00016074
INFO:root:[72,   700] training loss: 0.00008457
INFO:root:[72,   750] training loss: 0.00031976
INFO:root:[72,   800] training loss: 0.00015512
INFO:root:[72,   850] training loss: 0.00009396
INFO:root:[72,   900] training loss: 0.01138842
INFO:root:[72,   950] training loss: 0.00309643
INFO:root:[72,  1000] training loss: 0.00025449
INFO:root:[72,  1050] training loss: 0.00007418
INFO:root:              precision    recall  f1-score   support

          G2     0.3333    1.0000    0.5000         3
           S     0.9213    0.6934    0.7913      1722
    Prophase     0.7166    0.6352    0.6735      1039
    Anaphase     0.3226    1.0000    0.4878        10
          G1     0.1457    0.6892    0.2406        74
   Metaphase     0.5203    0.6415    0.5746      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.6655      3872
   macro avg     0.5086    0.8085    0.5740      3872
weighted avg     0.7436    0.6655    0.6911      3872

INFO:root:epoch72
INFO:root:[73,    50] training loss: 0.01376747
INFO:root:[73,   100] training loss: 0.01326256
INFO:root:[73,   150] training loss: 0.01427355
INFO:root:[73,   200] training loss: 0.01376995
INFO:root:[73,   250] training loss: 0.01088563
INFO:root:[73,   300] training loss: 0.01367958
INFO:root:[73,   350] training loss: 0.01105143
INFO:root:[73,   400] training loss: 0.00002685
INFO:root:[73,   450] training loss: 0.00002975
INFO:root:[73,   500] training loss: 0.00066537
INFO:root:[73,   550] training loss: 0.00047070
INFO:root:[73,   600] training loss: 0.00153581
INFO:root:[73,   650] training loss: 0.00016028
INFO:root:[73,   700] training loss: 0.00009731
INFO:root:[73,   750] training loss: 0.00036664
INFO:root:[73,   800] training loss: 0.00014938
INFO:root:[73,   850] training loss: 0.00011096
INFO:root:[73,   900] training loss: 0.01453328
INFO:root:[73,   950] training loss: 0.00301501
INFO:root:[73,  1000] training loss: 0.00023372
INFO:root:[73,  1050] training loss: 0.00009343
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9326    0.6272    0.7500      1722
    Prophase     0.6809    0.7084    0.6943      1039
    Anaphase     0.5385    0.7000    0.6087        10
          G1     0.1889    0.7838    0.3045        74
   Metaphase     0.4977    0.6346    0.5579      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.6550      3872
   macro avg     0.5769    0.7791    0.6308      3872
weighted avg     0.7347    0.6550    0.6757      3872

INFO:root:epoch73
INFO:root:[74,    50] training loss: 0.01247890
INFO:root:[74,   100] training loss: 0.01255776
INFO:root:[74,   150] training loss: 0.01195294
INFO:root:[74,   200] training loss: 0.01126084
INFO:root:[74,   250] training loss: 0.01182970
INFO:root:[74,   300] training loss: 0.01253590
INFO:root:[74,   350] training loss: 0.01135870
INFO:root:[74,   400] training loss: 0.00003094
INFO:root:[74,   450] training loss: 0.00002521
INFO:root:[74,   500] training loss: 0.00048630
INFO:root:[74,   550] training loss: 0.00051418
INFO:root:[74,   600] training loss: 0.00143238
INFO:root:[74,   650] training loss: 0.00015527
INFO:root:[74,   700] training loss: 0.00010459
INFO:root:[74,   750] training loss: 0.00035476
INFO:root:[74,   800] training loss: 0.00023030
INFO:root:[74,   850] training loss: 0.00008351
INFO:root:[74,   900] training loss: 0.01118190
INFO:root:[74,   950] training loss: 0.00380846
INFO:root:[74,  1000] training loss: 0.00026093
INFO:root:[74,  1050] training loss: 0.00006665
INFO:root:              precision    recall  f1-score   support

          G2     0.3750    1.0000    0.5455         3
           S     0.9228    0.7015    0.7971      1722
    Prophase     0.7208    0.6959    0.7081      1039
    Anaphase     0.4444    0.8000    0.5714        10
          G1     0.2066    0.7568    0.3246        74
   Metaphase     0.5311    0.6542    0.5863      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.6896      3872
   macro avg     0.5525    0.8012    0.6190      3872
weighted avg     0.7499    0.6896    0.7080      3872

INFO:root:epoch74
INFO:root:[75,    50] training loss: 0.01314975
INFO:root:[75,   100] training loss: 0.01248465
INFO:root:[75,   150] training loss: 0.01261115
INFO:root:[75,   200] training loss: 0.01160482
INFO:root:[75,   250] training loss: 0.01032202
INFO:root:[75,   300] training loss: 0.01221566
INFO:root:[75,   350] training loss: 0.00983291
INFO:root:[75,   400] training loss: 0.00002093
INFO:root:[75,   450] training loss: 0.00003104
INFO:root:[75,   500] training loss: 0.00048125
INFO:root:[75,   550] training loss: 0.00038485
INFO:root:[75,   600] training loss: 0.00108201
INFO:root:[75,   650] training loss: 0.00014057
INFO:root:[75,   700] training loss: 0.00009821
INFO:root:[75,   750] training loss: 0.00044054
INFO:root:[75,   800] training loss: 0.00011980
INFO:root:[75,   850] training loss: 0.00007532
INFO:root:[75,   900] training loss: 0.01055656
INFO:root:[75,   950] training loss: 0.00231828
INFO:root:[75,  1000] training loss: 0.00017585
INFO:root:[75,  1050] training loss: 0.00007301
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9073    0.7276    0.8076      1722
    Prophase     0.6446    0.7228    0.6815      1039
    Anaphase     0.5833    0.7000    0.6364        10
          G1     0.1929    0.8108    0.3117        74
   Metaphase     0.5126    0.4980    0.5052      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.6681      3872
   macro avg     0.5868    0.7799    0.6418      3872
weighted avg     0.7180    0.6681    0.6843      3872

INFO:root:epoch75
INFO:root:[76,    50] training loss: 0.01276923
INFO:root:[76,   100] training loss: 0.01130531
INFO:root:[76,   150] training loss: 0.01316449
INFO:root:[76,   200] training loss: 0.01272032
INFO:root:[76,   250] training loss: 0.01230626
INFO:root:[76,   300] training loss: 0.01226820
INFO:root:[76,   350] training loss: 0.01056243
INFO:root:[76,   400] training loss: 0.00002203
INFO:root:[76,   450] training loss: 0.00002521
INFO:root:[76,   500] training loss: 0.00052051
INFO:root:[76,   550] training loss: 0.00048882
INFO:root:[76,   600] training loss: 0.00100015
INFO:root:[76,   650] training loss: 0.00017068
INFO:root:[76,   700] training loss: 0.00010019
INFO:root:[76,   750] training loss: 0.00034846
INFO:root:[76,   800] training loss: 0.00014519
INFO:root:[76,   850] training loss: 0.00007804
INFO:root:[76,   900] training loss: 0.01041017
INFO:root:[76,   950] training loss: 0.00222559
INFO:root:[76,  1000] training loss: 0.00019576
INFO:root:[76,  1050] training loss: 0.00007436
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.8952    0.8037    0.8470      1722
    Prophase     0.6969    0.7392    0.7174      1039
    Anaphase     0.8571    0.6000    0.7059        10
          G1     0.2268    0.8243    0.3557        74
   Metaphase     0.6006    0.5511    0.5748      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.7203      3872
   macro avg     0.6490    0.7883    0.6787      3872
weighted avg     0.7511    0.7203    0.7308      3872

INFO:root:epoch76
INFO:root:[77,    50] training loss: 0.01205624
INFO:root:[77,   100] training loss: 0.01157591
INFO:root:[77,   150] training loss: 0.01234727
INFO:root:[77,   200] training loss: 0.01155765
INFO:root:[77,   250] training loss: 0.01073805
INFO:root:[77,   300] training loss: 0.01160126
INFO:root:[77,   350] training loss: 0.01063756
INFO:root:[77,   400] training loss: 0.00003241
INFO:root:[77,   450] training loss: 0.00001826
INFO:root:[77,   500] training loss: 0.00032163
INFO:root:[77,   550] training loss: 0.00052143
INFO:root:[77,   600] training loss: 0.00085924
INFO:root:[77,   650] training loss: 0.00011749
INFO:root:[77,   700] training loss: 0.00007364
INFO:root:[77,   750] training loss: 0.00032872
INFO:root:[77,   800] training loss: 0.00013050
INFO:root:[77,   850] training loss: 0.00008640
INFO:root:[77,   900] training loss: 0.00968932
INFO:root:[77,   950] training loss: 0.00228098
INFO:root:[77,  1000] training loss: 0.00016401
INFO:root:[77,  1050] training loss: 0.00005590
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9200    0.7015    0.7960      1722
    Prophase     0.6327    0.6997    0.6645      1039
    Anaphase     0.6000    0.6000    0.6000        10
          G1     0.1798    0.8649    0.2977        74
   Metaphase     0.5015    0.5069    0.5042      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.6534      3872
   macro avg     0.5763    0.7676    0.6232      3872
weighted avg     0.7172    0.6534    0.6739      3872

INFO:root:epoch77
INFO:root:[78,    50] training loss: 0.01234325
INFO:root:[78,   100] training loss: 0.01140698
INFO:root:[78,   150] training loss: 0.01155846
INFO:root:[78,   200] training loss: 0.01490971
INFO:root:[78,   250] training loss: 0.01052935
INFO:root:[78,   300] training loss: 0.01295300
INFO:root:[78,   350] training loss: 0.01073293
INFO:root:[78,   400] training loss: 0.00002356
INFO:root:[78,   450] training loss: 0.00002911
INFO:root:[78,   500] training loss: 0.00042428
INFO:root:[78,   550] training loss: 0.00044145
INFO:root:[78,   600] training loss: 0.00085595
INFO:root:[78,   650] training loss: 0.00011896
INFO:root:[78,   700] training loss: 0.00007448
INFO:root:[78,   750] training loss: 0.00046863
INFO:root:[78,   800] training loss: 0.00014068
INFO:root:[78,   850] training loss: 0.00007308
INFO:root:[78,   900] training loss: 0.01247368
INFO:root:[78,   950] training loss: 0.00273189
INFO:root:[78,  1000] training loss: 0.00017888
INFO:root:[78,  1050] training loss: 0.00005009
INFO:root:              precision    recall  f1-score   support

          G2     0.4286    1.0000    0.6000         3
           S     0.9216    0.7300    0.8146      1722
    Prophase     0.6867    0.6035    0.6424      1039
    Anaphase     0.5714    0.8000    0.6667        10
          G1     0.1502    0.8649    0.2560        74
   Metaphase     0.5431    0.6071    0.5733      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.6671      3872
   macro avg     0.5574    0.8008    0.6147      3872
weighted avg     0.7425    0.6671    0.6936      3872

INFO:root:epoch78
INFO:root:[79,    50] training loss: 0.01262754
INFO:root:[79,   100] training loss: 0.01186007
INFO:root:[79,   150] training loss: 0.01486635
INFO:root:[79,   200] training loss: 0.01152058
INFO:root:[79,   250] training loss: 0.01069872
INFO:root:[79,   300] training loss: 0.01244394
INFO:root:[79,   350] training loss: 0.01360132
INFO:root:[79,   400] training loss: 0.00006414
INFO:root:[79,   450] training loss: 0.00003895
INFO:root:[79,   500] training loss: 0.00033034
INFO:root:[79,   550] training loss: 0.00049443
INFO:root:[79,   600] training loss: 0.00096544
INFO:root:[79,   650] training loss: 0.00010516
INFO:root:[79,   700] training loss: 0.00006743
INFO:root:[79,   750] training loss: 0.00041178
INFO:root:[79,   800] training loss: 0.00010253
INFO:root:[79,   850] training loss: 0.00006647
INFO:root:[79,   900] training loss: 0.01074067
INFO:root:[79,   950] training loss: 0.00246756
INFO:root:[79,  1000] training loss: 0.00013238
INFO:root:[79,  1050] training loss: 0.00006057
INFO:root:              precision    recall  f1-score   support

          G2     1.0000    0.6667    0.8000         3
           S     0.9079    0.7956    0.8480      1722
    Prophase     0.6920    0.7007    0.6963      1039
    Anaphase     0.5000    0.7000    0.5833        10
          G1     0.1826    0.8243    0.2990        74
   Metaphase     0.5996    0.5589    0.5785      1018
   Telophase     0.5000    1.0000    0.6667         6

    accuracy                         0.7084      3872
   macro avg     0.6260    0.7495    0.6388      3872
weighted avg     0.7534    0.7084    0.7250      3872

INFO:root:epoch79
INFO:root:[80,    50] training loss: 0.01179753
INFO:root:[80,   100] training loss: 0.01120819
INFO:root:[80,   150] training loss: 0.01400341
INFO:root:[80,   200] training loss: 0.01178991
INFO:root:[80,   250] training loss: 0.01076987
INFO:root:[80,   300] training loss: 0.01202864
INFO:root:[80,   350] training loss: 0.00987977
INFO:root:[80,   400] training loss: 0.00001332
INFO:root:[80,   450] training loss: 0.00001451
INFO:root:[80,   500] training loss: 0.00031791
INFO:root:[80,   550] training loss: 0.00042267
INFO:root:[80,   600] training loss: 0.00069482
INFO:root:[80,   650] training loss: 0.00013807
INFO:root:[80,   700] training loss: 0.00006096
INFO:root:[80,   750] training loss: 0.00030219
INFO:root:[80,   800] training loss: 0.00014528
INFO:root:[80,   850] training loss: 0.00005367
INFO:root:[80,   900] training loss: 0.01052477
INFO:root:[80,   950] training loss: 0.00335878
INFO:root:[80,  1000] training loss: 0.00019035
INFO:root:[80,  1050] training loss: 0.00006720
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    0.6667    0.5714         3
           S     0.9307    0.6864    0.7901      1722
    Prophase     0.7247    0.7449    0.7347      1039
    Anaphase     0.2143    0.6000    0.3158        10
          G1     0.2379    0.7297    0.3588        74
   Metaphase     0.5178    0.6444    0.5742      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.6921      3872
   macro avg     0.5536    0.7246    0.6003      3872
weighted avg     0.7512    0.6921    0.7089      3872

INFO:root:epoch80
INFO:root:[81,    50] training loss: 0.01216990
INFO:root:[81,   100] training loss: 0.01159046
INFO:root:[81,   150] training loss: 0.01223926
INFO:root:[81,   200] training loss: 0.01136526
INFO:root:[81,   250] training loss: 0.01103105
INFO:root:[81,   300] training loss: 0.01104491
INFO:root:[81,   350] training loss: 0.01070543
INFO:root:[81,   400] training loss: 0.00001348
INFO:root:[81,   450] training loss: 0.00001367
INFO:root:[81,   500] training loss: 0.00021897
INFO:root:[81,   550] training loss: 0.00046486
INFO:root:[81,   600] training loss: 0.00094774
INFO:root:[81,   650] training loss: 0.00009427
INFO:root:[81,   700] training loss: 0.00007110
INFO:root:[81,   750] training loss: 0.00035642
INFO:root:[81,   800] training loss: 0.00012634
INFO:root:[81,   850] training loss: 0.00006791
INFO:root:[81,   900] training loss: 0.00873171
INFO:root:[81,   950] training loss: 0.00159430
INFO:root:[81,  1000] training loss: 0.00012511
INFO:root:[81,  1050] training loss: 0.00005358
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    1.0000    0.6667         3
           S     0.9083    0.7938    0.8472      1722
    Prophase     0.6974    0.7652    0.7297      1039
    Anaphase     0.5000    0.7000    0.5833        10
          G1     0.2191    0.7432    0.3385        74
   Metaphase     0.5960    0.5550    0.5748      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7226      3872
   macro avg     0.5958    0.7939    0.6568      3872
weighted avg     0.7548    0.7226    0.7335      3872

INFO:root:epoch81
INFO:root:[82,    50] training loss: 0.01080505
INFO:root:[82,   100] training loss: 0.01303946
INFO:root:[82,   150] training loss: 0.01141186
INFO:root:[82,   200] training loss: 0.01227221
INFO:root:[82,   250] training loss: 0.00960975
INFO:root:[82,   300] training loss: 0.01105180
INFO:root:[82,   350] training loss: 0.01038188
INFO:root:[82,   400] training loss: 0.00002154
INFO:root:[82,   450] training loss: 0.00010027
INFO:root:[82,   500] training loss: 0.00033107
INFO:root:[82,   550] training loss: 0.00042753
INFO:root:[82,   600] training loss: 0.00073743
INFO:root:[82,   650] training loss: 0.00008834
INFO:root:[82,   700] training loss: 0.00006220
INFO:root:[82,   750] training loss: 0.00032084
INFO:root:[82,   800] training loss: 0.00012554
INFO:root:[82,   850] training loss: 0.00006083
INFO:root:[82,   900] training loss: 0.00939192
INFO:root:[82,   950] training loss: 0.00248871
INFO:root:[82,  1000] training loss: 0.00012437
INFO:root:[82,  1050] training loss: 0.00005777
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9068    0.7741    0.8352      1722
    Prophase     0.6416    0.7632    0.6971      1039
    Anaphase     0.6667    0.6000    0.6316        10
          G1     0.2273    0.8108    0.3550        74
   Metaphase     0.5784    0.5000    0.5364      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.6999      3872
   macro avg     0.6244    0.7783    0.6661      3872
weighted avg     0.7352    0.6999    0.7099      3872

INFO:root:epoch82
INFO:root:[83,    50] training loss: 0.01128762
INFO:root:[83,   100] training loss: 0.01102708
INFO:root:[83,   150] training loss: 0.01270056
INFO:root:[83,   200] training loss: 0.01148564
INFO:root:[83,   250] training loss: 0.00998776
INFO:root:[83,   300] training loss: 0.01270006
INFO:root:[83,   350] training loss: 0.00990320
INFO:root:[83,   400] training loss: 0.00002806
INFO:root:[83,   450] training loss: 0.00004516
INFO:root:[83,   500] training loss: 0.00032067
INFO:root:[83,   550] training loss: 0.00037246
INFO:root:[83,   600] training loss: 0.00070199
INFO:root:[83,   650] training loss: 0.00009158
INFO:root:[83,   700] training loss: 0.00005698
INFO:root:[83,   750] training loss: 0.00032312
INFO:root:[83,   800] training loss: 0.00010267
INFO:root:[83,   850] training loss: 0.00005596
INFO:root:[83,   900] training loss: 0.00892718
INFO:root:[83,   950] training loss: 0.00310591
INFO:root:[83,  1000] training loss: 0.00013291
INFO:root:[83,  1050] training loss: 0.00005813
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    1.0000    0.6667         3
           S     0.9124    0.8101    0.8582      1722
    Prophase     0.7096    0.6939    0.7017      1039
    Anaphase     0.3684    0.7000    0.4828        10
          G1     0.1667    0.7838    0.2749        74
   Metaphase     0.6159    0.5717    0.5930      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.7159      3872
   macro avg     0.5628    0.7942    0.6253      3872
weighted avg     0.7637    0.7159    0.7341      3872

INFO:root:epoch83
INFO:root:[84,    50] training loss: 0.01134242
INFO:root:[84,   100] training loss: 0.01042751
INFO:root:[84,   150] training loss: 0.01235008
INFO:root:[84,   200] training loss: 0.00974844
INFO:root:[84,   250] training loss: 0.00987540
INFO:root:[84,   300] training loss: 0.01127445
INFO:root:[84,   350] training loss: 0.01040457
INFO:root:[84,   400] training loss: 0.00002763
INFO:root:[84,   450] training loss: 0.00002493
INFO:root:[84,   500] training loss: 0.00016118
INFO:root:[84,   550] training loss: 0.00041842
INFO:root:[84,   600] training loss: 0.00047715
INFO:root:[84,   650] training loss: 0.00009071
INFO:root:[84,   700] training loss: 0.00005692
INFO:root:[84,   750] training loss: 0.00029886
INFO:root:[84,   800] training loss: 0.00012553
INFO:root:[84,   850] training loss: 0.00004922
INFO:root:[84,   900] training loss: 0.00916393
INFO:root:[84,   950] training loss: 0.00430847
INFO:root:[84,  1000] training loss: 0.00013483
INFO:root:[84,  1050] training loss: 0.00005065
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9136    0.7427    0.8193      1722
    Prophase     0.6084    0.6670    0.6364      1039
    Anaphase     0.5000    0.8000    0.6154        10
          G1     0.1546    0.8108    0.2597        74
   Metaphase     0.5305    0.4784    0.5031      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.6550      3872
   macro avg     0.6163    0.7856    0.6592      3872
weighted avg     0.7152    0.6550    0.6761      3872

INFO:root:epoch84
INFO:root:[85,    50] training loss: 0.01124902
INFO:root:[85,   100] training loss: 0.01072953
INFO:root:[85,   150] training loss: 0.01063725
INFO:root:[85,   200] training loss: 0.01051963
INFO:root:[85,   250] training loss: 0.00928307
INFO:root:[85,   300] training loss: 0.01054047
INFO:root:[85,   350] training loss: 0.01099502
INFO:root:[85,   400] training loss: 0.00001290
INFO:root:[85,   450] training loss: 0.00001569
INFO:root:[85,   500] training loss: 0.00015945
INFO:root:[85,   550] training loss: 0.00038441
INFO:root:[85,   600] training loss: 0.00043558
INFO:root:[85,   650] training loss: 0.00008196
INFO:root:[85,   700] training loss: 0.00006701
INFO:root:[85,   750] training loss: 0.00040352
INFO:root:[85,   800] training loss: 0.00007861
INFO:root:[85,   850] training loss: 0.00005903
INFO:root:[85,   900] training loss: 0.00841865
INFO:root:[85,   950] training loss: 0.00214533
INFO:root:[85,  1000] training loss: 0.00009886
INFO:root:[85,  1050] training loss: 0.00005082
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9148    0.7735    0.8383      1722
    Prophase     0.6672    0.7892    0.7231      1039
    Anaphase     0.5714    0.8000    0.6667        10
          G1     0.2437    0.7838    0.3718        74
   Metaphase     0.5764    0.5226    0.5482      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7126      3872
   macro avg     0.6330    0.8099    0.6887      3872
weighted avg     0.7454    0.7126    0.7218      3872

INFO:root:epoch85
INFO:root:[86,    50] training loss: 0.01174460
INFO:root:[86,   100] training loss: 0.01123230
INFO:root:[86,   150] training loss: 0.01090017
INFO:root:[86,   200] training loss: 0.00959476
INFO:root:[86,   250] training loss: 0.01036090
INFO:root:[86,   300] training loss: 0.01606252
INFO:root:[86,   350] training loss: 0.01073038
INFO:root:[86,   400] training loss: 0.00003427
INFO:root:[86,   450] training loss: 0.00002860
INFO:root:[86,   500] training loss: 0.00017717
INFO:root:[86,   550] training loss: 0.00040387
INFO:root:[86,   600] training loss: 0.00048435
INFO:root:[86,   650] training loss: 0.00007542
INFO:root:[86,   700] training loss: 0.00005626
INFO:root:[86,   750] training loss: 0.00030786
INFO:root:[86,   800] training loss: 0.00011625
INFO:root:[86,   850] training loss: 0.00008385
INFO:root:[86,   900] training loss: 0.00790310
INFO:root:[86,   950] training loss: 0.00202390
INFO:root:[86,  1000] training loss: 0.00009516
INFO:root:[86,  1050] training loss: 0.00004986
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    1.0000    0.6667         3
           S     0.9051    0.8252    0.8633      1722
    Prophase     0.7202    0.7902    0.7536      1039
    Anaphase     0.3600    0.9000    0.5143        10
          G1     0.2451    0.6757    0.3597        74
   Metaphase     0.6224    0.5619    0.5906      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7443      3872
   macro avg     0.5861    0.8218    0.6579      3872
weighted avg     0.7666    0.7443    0.7515      3872

INFO:root:epoch86
INFO:root:[87,    50] training loss: 0.01156600
INFO:root:[87,   100] training loss: 0.01043890
INFO:root:[87,   150] training loss: 0.01199552
INFO:root:[87,   200] training loss: 0.01054171
INFO:root:[87,   250] training loss: 0.01086918
INFO:root:[87,   300] training loss: 0.01114600
INFO:root:[87,   350] training loss: 0.01012063
INFO:root:[87,   400] training loss: 0.00001949
INFO:root:[87,   450] training loss: 0.00002810
INFO:root:[87,   500] training loss: 0.00029298
INFO:root:[87,   550] training loss: 0.00034120
INFO:root:[87,   600] training loss: 0.00081855
INFO:root:[87,   650] training loss: 0.00008162
INFO:root:[87,   700] training loss: 0.00005394
INFO:root:[87,   750] training loss: 0.00029627
INFO:root:[87,   800] training loss: 0.00008086
INFO:root:[87,   850] training loss: 0.00004857
INFO:root:[87,   900] training loss: 0.00830134
INFO:root:[87,   950] training loss: 0.00209993
INFO:root:[87,  1000] training loss: 0.00009176
INFO:root:[87,  1050] training loss: 0.00004338
INFO:root:              precision    recall  f1-score   support

          G2     0.3750    1.0000    0.5455         3
           S     0.9062    0.8084    0.8545      1722
    Prophase     0.6545    0.7421    0.6955      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.1973    0.7973    0.3164        74
   Metaphase     0.6050    0.4951    0.5446      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7084      3872
   macro avg     0.6126    0.8061    0.6591      3872
weighted avg     0.7450    0.7084    0.7197      3872

INFO:root:epoch87
INFO:root:[88,    50] training loss: 0.01064278
INFO:root:[88,   100] training loss: 0.01058998
INFO:root:[88,   150] training loss: 0.01130171
INFO:root:[88,   200] training loss: 0.01114960
INFO:root:[88,   250] training loss: 0.00998575
INFO:root:[88,   300] training loss: 0.01155328
INFO:root:[88,   350] training loss: 0.01091623
INFO:root:[88,   400] training loss: 0.00002501
INFO:root:[88,   450] training loss: 0.00002201
INFO:root:[88,   500] training loss: 0.00016919
INFO:root:[88,   550] training loss: 0.00041693
INFO:root:[88,   600] training loss: 0.00039278
INFO:root:[88,   650] training loss: 0.00006530
INFO:root:[88,   700] training loss: 0.00004833
INFO:root:[88,   750] training loss: 0.00040673
INFO:root:[88,   800] training loss: 0.00008543
INFO:root:[88,   850] training loss: 0.00004041
INFO:root:[88,   900] training loss: 0.00786824
INFO:root:[88,   950] training loss: 0.00203631
INFO:root:[88,  1000] training loss: 0.00006675
INFO:root:[88,  1050] training loss: 0.00003950
INFO:root:              precision    recall  f1-score   support

          G2     0.3750    1.0000    0.5455         3
           S     0.9031    0.8333    0.8668      1722
    Prophase     0.7092    0.7276    0.7183      1039
    Anaphase     0.7778    0.7000    0.7368        10
          G1     0.2120    0.8108    0.3361        74
   Metaphase     0.6284    0.5599    0.5922      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.7327      3872
   macro avg     0.6008    0.8045    0.6494      3872
weighted avg     0.7644    0.7327    0.7439      3872

INFO:root:epoch88
INFO:root:[89,    50] training loss: 0.01092026
INFO:root:[89,   100] training loss: 0.01030514
INFO:root:[89,   150] training loss: 0.01028293
INFO:root:[89,   200] training loss: 0.00926204
INFO:root:[89,   250] training loss: 0.01023805
INFO:root:[89,   300] training loss: 0.01082197
INFO:root:[89,   350] training loss: 0.00938796
INFO:root:[89,   400] training loss: 0.00001673
INFO:root:[89,   450] training loss: 0.00001889
INFO:root:[89,   500] training loss: 0.00012939
INFO:root:[89,   550] training loss: 0.00038159
INFO:root:[89,   600] training loss: 0.00034866
INFO:root:[89,   650] training loss: 0.00005400
INFO:root:[89,   700] training loss: 0.00003365
INFO:root:[89,   750] training loss: 0.00034047
INFO:root:[89,   800] training loss: 0.00008381
INFO:root:[89,   850] training loss: 0.00006783
INFO:root:[89,   900] training loss: 0.00768459
INFO:root:[89,   950] training loss: 0.00192376
INFO:root:[89,  1000] training loss: 0.00007571
INFO:root:[89,  1050] training loss: 0.00006230
INFO:root:              precision    recall  f1-score   support

          G2     0.3333    1.0000    0.5000         3
           S     0.9164    0.8026    0.8557      1722
    Prophase     0.7043    0.7498    0.7263      1039
    Anaphase     0.4762    1.0000    0.6452        10
          G1     0.2072    0.7027    0.3200        74
   Metaphase     0.5915    0.5619    0.5763      1018
   Telophase     0.6000    1.0000    0.7500         6

    accuracy                         0.7242      3872
   macro avg     0.5470    0.8310    0.6248      3872
weighted avg     0.7585    0.7242    0.7363      3872

INFO:root:epoch89
INFO:root:[90,    50] training loss: 0.01023417
INFO:root:[90,   100] training loss: 0.01304099
INFO:root:[90,   150] training loss: 0.01260149
INFO:root:[90,   200] training loss: 0.01109887
INFO:root:[90,   250] training loss: 0.00994419
INFO:root:[90,   300] training loss: 0.01156925
INFO:root:[90,   350] training loss: 0.00878003
INFO:root:[90,   400] training loss: 0.00002554
INFO:root:[90,   450] training loss: 0.00001613
INFO:root:[90,   500] training loss: 0.00016782
INFO:root:[90,   550] training loss: 0.00032715
INFO:root:[90,   600] training loss: 0.00029327
INFO:root:[90,   650] training loss: 0.00007327
INFO:root:[90,   700] training loss: 0.00003751
INFO:root:[90,   750] training loss: 0.00035066
INFO:root:[90,   800] training loss: 0.00006952
INFO:root:[90,   850] training loss: 0.00006089
INFO:root:[90,   900] training loss: 0.00662930
INFO:root:[90,   950] training loss: 0.00176386
INFO:root:[90,  1000] training loss: 0.00006705
INFO:root:[90,  1050] training loss: 0.00003348
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9026    0.8072    0.8522      1722
    Prophase     0.6722    0.7459    0.7071      1039
    Anaphase     0.5833    0.7000    0.6364        10
          G1     0.2049    0.7973    0.3260        74
   Metaphase     0.5889    0.5010    0.5414      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7102      3872
   macro avg     0.6146    0.7931    0.6672      3872
weighted avg     0.7437    0.7102    0.7209      3872

INFO:root:epoch90
INFO:root:[91,    50] training loss: 0.01044325
INFO:root:[91,   100] training loss: 0.01113394
INFO:root:[91,   150] training loss: 0.01282540
INFO:root:[91,   200] training loss: 0.01082114
INFO:root:[91,   250] training loss: 0.00931508
INFO:root:[91,   300] training loss: 0.01070877
INFO:root:[91,   350] training loss: 0.00915495
INFO:root:[91,   400] training loss: 0.00003115
INFO:root:[91,   450] training loss: 0.00002144
INFO:root:[91,   500] training loss: 0.00012862
INFO:root:[91,   550] training loss: 0.00037326
INFO:root:[91,   600] training loss: 0.00029863
INFO:root:[91,   650] training loss: 0.00005730
INFO:root:[91,   700] training loss: 0.00004541
INFO:root:[91,   750] training loss: 0.00034936
INFO:root:[91,   800] training loss: 0.00008224
INFO:root:[91,   850] training loss: 0.00004712
INFO:root:[91,   900] training loss: 0.00794235
INFO:root:[91,   950] training loss: 0.00172704
INFO:root:[91,  1000] training loss: 0.00006356
INFO:root:[91,  1050] training loss: 0.00003266
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9073    0.7962    0.8481      1722
    Prophase     0.7044    0.7315    0.7177      1039
    Anaphase     0.6000    0.9000    0.7200        10
          G1     0.2074    0.7568    0.3256        74
   Metaphase     0.5848    0.5658    0.5751      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7182      3872
   macro avg     0.6373    0.8215    0.6942      3872
weighted avg     0.7536    0.7182    0.7311      3872

INFO:root:epoch91
INFO:root:[92,    50] training loss: 0.01019462
INFO:root:[92,   100] training loss: 0.00952155
INFO:root:[92,   150] training loss: 0.01039022
INFO:root:[92,   200] training loss: 0.00945610
INFO:root:[92,   250] training loss: 0.00904897
INFO:root:[92,   300] training loss: 0.00979467
INFO:root:[92,   350] training loss: 0.00858302
INFO:root:[92,   400] training loss: 0.00001421
INFO:root:[92,   450] training loss: 0.00001416
INFO:root:[92,   500] training loss: 0.00016672
INFO:root:[92,   550] training loss: 0.00026717
INFO:root:[92,   600] training loss: 0.00042109
INFO:root:[92,   650] training loss: 0.00005142
INFO:root:[92,   700] training loss: 0.00002820
INFO:root:[92,   750] training loss: 0.00024704
INFO:root:[92,   800] training loss: 0.00009002
INFO:root:[92,   850] training loss: 0.00004107
INFO:root:[92,   900] training loss: 0.00603271
INFO:root:[92,   950] training loss: 0.00156308
INFO:root:[92,  1000] training loss: 0.00007415
INFO:root:[92,  1050] training loss: 0.00003440
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    1.0000    0.6667         3
           S     0.9116    0.7904    0.8467      1722
    Prophase     0.7108    0.7238    0.7172      1039
    Anaphase     0.5625    0.9000    0.6923        10
          G1     0.2064    0.7838    0.3268        74
   Metaphase     0.5822    0.5776    0.5799      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7172      3872
   macro avg     0.6033    0.8251    0.6695      3872
weighted avg     0.7561    0.7172    0.7313      3872

INFO:root:epoch92
INFO:root:[93,    50] training loss: 0.01009513
INFO:root:[93,   100] training loss: 0.00980035
INFO:root:[93,   150] training loss: 0.01003349
INFO:root:[93,   200] training loss: 0.00904852
INFO:root:[93,   250] training loss: 0.00875445
INFO:root:[93,   300] training loss: 0.01031801
INFO:root:[93,   350] training loss: 0.00875363
INFO:root:[93,   400] training loss: 0.00000743
INFO:root:[93,   450] training loss: 0.00000919
INFO:root:[93,   500] training loss: 0.00007123
INFO:root:[93,   550] training loss: 0.00035335
INFO:root:[93,   600] training loss: 0.00024142
INFO:root:[93,   650] training loss: 0.00003242
INFO:root:[93,   700] training loss: 0.00003122
INFO:root:[93,   750] training loss: 0.00027992
INFO:root:[93,   800] training loss: 0.00006584
INFO:root:[93,   850] training loss: 0.00003139
INFO:root:[93,   900] training loss: 0.00719997
INFO:root:[93,   950] training loss: 0.00170797
INFO:root:[93,  1000] training loss: 0.00005720
INFO:root:[93,  1050] training loss: 0.00003177
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9194    0.8020    0.8567      1722
    Prophase     0.7199    0.7988    0.7573      1039
    Anaphase     0.5714    0.8000    0.6667        10
          G1     0.2578    0.7838    0.3880        74
   Metaphase     0.6070    0.5766    0.5914      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7420      3872
   macro avg     0.6690    0.8230    0.7200      3872
weighted avg     0.7700    0.7420    0.7509      3872

INFO:root:epoch93
INFO:root:[94,    50] training loss: 0.01000914
INFO:root:[94,   100] training loss: 0.00954715
INFO:root:[94,   150] training loss: 0.01127944
INFO:root:[94,   200] training loss: 0.01126432
INFO:root:[94,   250] training loss: 0.00775445
INFO:root:[94,   300] training loss: 0.01318481
INFO:root:[94,   350] training loss: 0.00844037
INFO:root:[94,   400] training loss: 0.00003582
INFO:root:[94,   450] training loss: 0.00001641
INFO:root:[94,   500] training loss: 0.00011160
INFO:root:[94,   550] training loss: 0.00027634
INFO:root:[94,   600] training loss: 0.00040112
INFO:root:[94,   650] training loss: 0.00005953
INFO:root:[94,   700] training loss: 0.00005397
INFO:root:[94,   750] training loss: 0.00018660
INFO:root:[94,   800] training loss: 0.00008592
INFO:root:[94,   850] training loss: 0.00003770
INFO:root:[94,   900] training loss: 0.00900106
INFO:root:[94,   950] training loss: 0.00189197
INFO:root:[94,  1000] training loss: 0.00015486
INFO:root:[94,  1050] training loss: 0.00004386
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9226    0.7683    0.8384      1722
    Prophase     0.7083    0.7921    0.7478      1039
    Anaphase     0.8750    0.7000    0.7778        10
          G1     0.2886    0.7838    0.4218        74
   Metaphase     0.5655    0.5855    0.5753      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7273      3872
   macro avg     0.6728    0.8042    0.7098      3872
weighted avg     0.7584    0.7273    0.7368      3872

INFO:root:epoch94
INFO:root:[95,    50] training loss: 0.01077920
INFO:root:[95,   100] training loss: 0.01023828
INFO:root:[95,   150] training loss: 0.01036906
INFO:root:[95,   200] training loss: 0.01166301
INFO:root:[95,   250] training loss: 0.00924083
INFO:root:[95,   300] training loss: 0.01017859
INFO:root:[95,   350] training loss: 0.00940187
INFO:root:[95,   400] training loss: 0.00003670
INFO:root:[95,   450] training loss: 0.00001464
INFO:root:[95,   500] training loss: 0.00023253
INFO:root:[95,   550] training loss: 0.00026802
INFO:root:[95,   600] training loss: 0.00036891
INFO:root:[95,   650] training loss: 0.00008649
INFO:root:[95,   700] training loss: 0.00002542
INFO:root:[95,   750] training loss: 0.00031527
INFO:root:[95,   800] training loss: 0.00013613
INFO:root:[95,   850] training loss: 0.00003760
INFO:root:[95,   900] training loss: 0.00657360
INFO:root:[95,   950] training loss: 0.00217833
INFO:root:[95,  1000] training loss: 0.00007645
INFO:root:[95,  1050] training loss: 0.00003164
INFO:root:              precision    recall  f1-score   support

          G2     0.4000    0.6667    0.5000         3
           S     0.8938    0.8600    0.8766      1722
    Prophase     0.7469    0.7555    0.7512      1039
    Anaphase     0.6667    0.8000    0.7273        10
          G1     0.2351    0.7973    0.3631        74
   Metaphase     0.6588    0.5747    0.6139      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7557      3872
   macro avg     0.6216    0.7792    0.6699      3872
weighted avg     0.7788    0.7557    0.7633      3872

INFO:root:epoch95
INFO:root:[96,    50] training loss: 0.00954480
INFO:root:[96,   100] training loss: 0.01018672
INFO:root:[96,   150] training loss: 0.01485878
INFO:root:[96,   200] training loss: 0.01297483
INFO:root:[96,   250] training loss: 0.01205724
INFO:root:[96,   300] training loss: 0.01023363
INFO:root:[96,   350] training loss: 0.00918463
INFO:root:[96,   400] training loss: 0.00004857
INFO:root:[96,   450] training loss: 0.00002018
INFO:root:[96,   500] training loss: 0.00009925
INFO:root:[96,   550] training loss: 0.00035630
INFO:root:[96,   600] training loss: 0.00036073
INFO:root:[96,   650] training loss: 0.00005483
INFO:root:[96,   700] training loss: 0.00003442
INFO:root:[96,   750] training loss: 0.00022578
INFO:root:[96,   800] training loss: 0.00007857
INFO:root:[96,   850] training loss: 0.00005128
INFO:root:[96,   900] training loss: 0.00621743
INFO:root:[96,   950] training loss: 0.00204136
INFO:root:[96,  1000] training loss: 0.00005560
INFO:root:[96,  1050] training loss: 0.00003489
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9184    0.8107    0.8612      1722
    Prophase     0.7228    0.8181    0.7675      1039
    Anaphase     0.5333    0.8000    0.6400        10
          G1     0.2727    0.7297    0.3971        74
   Metaphase     0.6236    0.5825    0.6023      1018
   Telophase     0.6250    0.8333    0.7143         6

    accuracy                         0.7513      3872
   macro avg     0.6351    0.7963    0.6914      3872
weighted avg     0.7745    0.7513    0.7583      3872

INFO:root:epoch96
INFO:root:[97,    50] training loss: 0.01040003
INFO:root:[97,   100] training loss: 0.00982558
INFO:root:[97,   150] training loss: 0.00987251
INFO:root:[97,   200] training loss: 0.00992383
INFO:root:[97,   250] training loss: 0.00899899
INFO:root:[97,   300] training loss: 0.00993628
INFO:root:[97,   350] training loss: 0.00834045
INFO:root:[97,   400] training loss: 0.00000879
INFO:root:[97,   450] training loss: 0.00001121
INFO:root:[97,   500] training loss: 0.00006038
INFO:root:[97,   550] training loss: 0.00029109
INFO:root:[97,   600] training loss: 0.00021714
INFO:root:[97,   650] training loss: 0.00005756
INFO:root:[97,   700] training loss: 0.00003117
INFO:root:[97,   750] training loss: 0.00029833
INFO:root:[97,   800] training loss: 0.00005174
INFO:root:[97,   850] training loss: 0.00003579
INFO:root:[97,   900] training loss: 0.00516951
INFO:root:[97,   950] training loss: 0.00181793
INFO:root:[97,  1000] training loss: 0.00005005
INFO:root:[97,  1050] training loss: 0.00002591
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    1.0000    0.6667         3
           S     0.9012    0.8374    0.8682      1722
    Prophase     0.7759    0.6997    0.7358      1039
    Anaphase     0.5625    0.9000    0.6923        10
          G1     0.2214    0.7838    0.3452        74
   Metaphase     0.6318    0.6473    0.6395      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7500      3872
   macro avg     0.6204    0.8383    0.6864      3872
weighted avg     0.7824    0.7500    0.7619      3872

INFO:root:epoch97
INFO:root:[98,    50] training loss: 0.00961974
INFO:root:[98,   100] training loss: 0.00969344
INFO:root:[98,   150] training loss: 0.00948324
INFO:root:[98,   200] training loss: 0.00872965
INFO:root:[98,   250] training loss: 0.01064163
INFO:root:[98,   300] training loss: 0.01028538
INFO:root:[98,   350] training loss: 0.00862510
INFO:root:[98,   400] training loss: 0.00001050
INFO:root:[98,   450] training loss: 0.00001207
INFO:root:[98,   500] training loss: 0.00007680
INFO:root:[98,   550] training loss: 0.00029691
INFO:root:[98,   600] training loss: 0.00031607
INFO:root:[98,   650] training loss: 0.00006175
INFO:root:[98,   700] training loss: 0.00003141
INFO:root:[98,   750] training loss: 0.00027762
INFO:root:[98,   800] training loss: 0.00008474
INFO:root:[98,   850] training loss: 0.00004017
INFO:root:[98,   900] training loss: 0.00568208
INFO:root:[98,   950] training loss: 0.00196066
INFO:root:[98,  1000] training loss: 0.00006446
INFO:root:[98,  1050] training loss: 0.00003116
INFO:root:              precision    recall  f1-score   support

          G2     0.4286    1.0000    0.6000         3
           S     0.9142    0.8235    0.8665      1722
    Prophase     0.7923    0.6939    0.7399      1039
    Anaphase     0.5000    1.0000    0.6667        10
          G1     0.2151    0.7297    0.3323        74
   Metaphase     0.6231    0.6886    0.6542      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7523      3872
   macro avg     0.6033    0.8480    0.6738      3872
weighted avg     0.7899    0.7523    0.7658      3872

INFO:root:epoch98
INFO:root:[99,    50] training loss: 0.00950627
INFO:root:[99,   100] training loss: 0.00919236
INFO:root:[99,   150] training loss: 0.01025200
INFO:root:[99,   200] training loss: 0.00981235
INFO:root:[99,   250] training loss: 0.00820317
INFO:root:[99,   300] training loss: 0.00985818
INFO:root:[99,   350] training loss: 0.00994241
INFO:root:[99,   400] training loss: 0.00001681
INFO:root:[99,   450] training loss: 0.00001687
INFO:root:[99,   500] training loss: 0.00014160
INFO:root:[99,   550] training loss: 0.00025792
INFO:root:[99,   600] training loss: 0.00037629
INFO:root:[99,   650] training loss: 0.00003490
INFO:root:[99,   700] training loss: 0.00003622
INFO:root:[99,   750] training loss: 0.00028135
INFO:root:[99,   800] training loss: 0.00005186
INFO:root:[99,   850] training loss: 0.00003407
INFO:root:[99,   900] training loss: 0.00759458
INFO:root:[99,   950] training loss: 0.00197005
INFO:root:[99,  1000] training loss: 0.00006456
INFO:root:[99,  1050] training loss: 0.00005281
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.8920    0.8298    0.8598      1722
    Prophase     0.7324    0.7796    0.7552      1039
    Anaphase     0.5625    0.9000    0.6923        10
          G1     0.2582    0.7432    0.3833        74
   Metaphase     0.6074    0.5501    0.5773      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7417      3872
   macro avg     0.6289    0.8290    0.6964      3872
weighted avg     0.7609    0.7417    0.7479      3872

INFO:root:epoch99
INFO:root:[100,    50] training loss: 0.00954468
INFO:root:[100,   100] training loss: 0.00892785
INFO:root:[100,   150] training loss: 0.01105883
INFO:root:[100,   200] training loss: 0.00897543
INFO:root:[100,   250] training loss: 0.00942391
INFO:root:[100,   300] training loss: 0.00953447
INFO:root:[100,   350] training loss: 0.00827734
INFO:root:[100,   400] training loss: 0.00000953
INFO:root:[100,   450] training loss: 0.00000808
INFO:root:[100,   500] training loss: 0.00007614
INFO:root:[100,   550] training loss: 0.00034337
INFO:root:[100,   600] training loss: 0.00025336
INFO:root:[100,   650] training loss: 0.00005429
INFO:root:[100,   700] training loss: 0.00002767
INFO:root:[100,   750] training loss: 0.00028916
INFO:root:[100,   800] training loss: 0.00006470
INFO:root:[100,   850] training loss: 0.00003765
INFO:root:[100,   900] training loss: 0.00916006
INFO:root:[100,   950] training loss: 0.00168151
INFO:root:[100,  1000] training loss: 0.00006366
INFO:root:[100,  1050] training loss: 0.00003360
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9351    0.7700    0.8446      1722
    Prophase     0.7164    0.7950    0.7536      1039
    Anaphase     0.5625    0.9000    0.6923        10
          G1     0.2646    0.7973    0.3973        74
   Metaphase     0.5840    0.6012    0.5924      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.7337      3872
   macro avg     0.6185    0.8376    0.6900      3872
weighted avg     0.7697    0.7337    0.7448      3872

INFO:root:epoch100
INFO:root:[101,    50] training loss: 0.00950807
INFO:root:[101,   100] training loss: 0.00899075
INFO:root:[101,   150] training loss: 0.01066804
INFO:root:[101,   200] training loss: 0.00948895
INFO:root:[101,   250] training loss: 0.00881193
INFO:root:[101,   300] training loss: 0.00941107
INFO:root:[101,   350] training loss: 0.00843676
INFO:root:[101,   400] training loss: 0.00001078
INFO:root:[101,   450] training loss: 0.00002322
INFO:root:[101,   500] training loss: 0.00013245
INFO:root:[101,   550] training loss: 0.00029757
INFO:root:[101,   600] training loss: 0.00049776
INFO:root:[101,   650] training loss: 0.00004478
INFO:root:[101,   700] training loss: 0.00003667
INFO:root:[101,   750] training loss: 0.00021831
INFO:root:[101,   800] training loss: 0.00007046
INFO:root:[101,   850] training loss: 0.00006262
INFO:root:[101,   900] training loss: 0.00623624
INFO:root:[101,   950] training loss: 0.00121088
INFO:root:[101,  1000] training loss: 0.00006092
INFO:root:[101,  1050] training loss: 0.00004505
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.8882    0.8583    0.8730      1722
    Prophase     0.7509    0.8152    0.7817      1039
    Anaphase     1.0000    0.8000    0.8889        10
          G1     0.3240    0.7838    0.4585        74
   Metaphase     0.6542    0.5648    0.6062      1018
   Telophase     0.6667    1.0000    0.8000         6

    accuracy                         0.7683      3872
   macro avg     0.6977    0.8317    0.7369      3872
weighted avg     0.7788    0.7683    0.7703      3872

INFO:root:epoch101
INFO:root:[102,    50] training loss: 0.01027773
INFO:root:[102,   100] training loss: 0.00846950
INFO:root:[102,   150] training loss: 0.01235323
INFO:root:[102,   200] training loss: 0.01133747
INFO:root:[102,   250] training loss: 0.00931400
INFO:root:[102,   300] training loss: 0.01009589
INFO:root:[102,   350] training loss: 0.00771244
INFO:root:[102,   400] training loss: 0.00002634
INFO:root:[102,   450] training loss: 0.00001055
INFO:root:[102,   500] training loss: 0.00009749
INFO:root:[102,   550] training loss: 0.00024947
INFO:root:[102,   600] training loss: 0.00020147
INFO:root:[102,   650] training loss: 0.00006473
INFO:root:[102,   700] training loss: 0.00004866
INFO:root:[102,   750] training loss: 0.00033083
INFO:root:[102,   800] training loss: 0.00004650
INFO:root:[102,   850] training loss: 0.00003253
INFO:root:[102,   900] training loss: 0.00613010
INFO:root:[102,   950] training loss: 0.00198633
INFO:root:[102,  1000] training loss: 0.00005306
INFO:root:[102,  1050] training loss: 0.00002574
INFO:root:              precision    recall  f1-score   support

          G2     1.0000    0.3333    0.5000         3
           S     0.9291    0.7538    0.8323      1722
    Prophase     0.7688    0.7680    0.7684      1039
    Anaphase     0.5833    0.7000    0.6364        10
          G1     0.2596    0.8243    0.3948        74
   Metaphase     0.5816    0.6758    0.6252      1018
   Telophase     0.8333    0.8333    0.8333         6

    accuracy                         0.7381      3872
   macro avg     0.7080    0.6984    0.6558      3872
weighted avg     0.7809    0.7381    0.7516      3872

INFO:root:epoch102
INFO:root:[103,    50] training loss: 0.01157736
INFO:root:[103,   100] training loss: 0.00951933
INFO:root:[103,   150] training loss: 0.01263573
INFO:root:[103,   200] training loss: 0.00941507
INFO:root:[103,   250] training loss: 0.00884655
INFO:root:[103,   300] training loss: 0.01121588
INFO:root:[103,   350] training loss: 0.00941482
INFO:root:[103,   400] training loss: 0.00000520
INFO:root:[103,   450] training loss: 0.00000641
INFO:root:[103,   500] training loss: 0.00007138
INFO:root:[103,   550] training loss: 0.00027535
INFO:root:[103,   600] training loss: 0.00032374
INFO:root:[103,   650] training loss: 0.00003885
INFO:root:[103,   700] training loss: 0.00002973
INFO:root:[103,   750] training loss: 0.00027875
INFO:root:[103,   800] training loss: 0.00006478
INFO:root:[103,   850] training loss: 0.00004662
INFO:root:[103,   900] training loss: 0.00533292
INFO:root:[103,   950] training loss: 0.00152633
INFO:root:[103,  1000] training loss: 0.00004102
INFO:root:[103,  1050] training loss: 0.00002536
INFO:root:              precision    recall  f1-score   support

          G2     0.4000    0.6667    0.5000         3
           S     0.9173    0.8055    0.8578      1722
    Prophase     0.7543    0.8393    0.7945      1039
    Anaphase     0.6000    0.9000    0.7200        10
          G1     0.3333    0.8108    0.4724        74
   Metaphase     0.6219    0.6090    0.6154      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7634      3872
   macro avg     0.6406    0.8045    0.6976      3872
weighted avg     0.7834    0.7634    0.7692      3872

INFO:root:epoch103
INFO:root:[104,    50] training loss: 0.00843693
INFO:root:[104,   100] training loss: 0.00884925
INFO:root:[104,   150] training loss: 0.00925210
INFO:root:[104,   200] training loss: 0.00833168
INFO:root:[104,   250] training loss: 0.01019809
INFO:root:[104,   300] training loss: 0.01384085
INFO:root:[104,   350] training loss: 0.00874885
INFO:root:[104,   400] training loss: 0.00002160
INFO:root:[104,   450] training loss: 0.00001597
INFO:root:[104,   500] training loss: 0.00012259
INFO:root:[104,   550] training loss: 0.00025760
INFO:root:[104,   600] training loss: 0.00020424
INFO:root:[104,   650] training loss: 0.00005629
INFO:root:[104,   700] training loss: 0.00003263
INFO:root:[104,   750] training loss: 0.00028444
INFO:root:[104,   800] training loss: 0.00005532
INFO:root:[104,   850] training loss: 0.00003526
INFO:root:[104,   900] training loss: 0.00617749
INFO:root:[104,   950] training loss: 0.00164099
INFO:root:[104,  1000] training loss: 0.00004024
INFO:root:[104,  1050] training loss: 0.00002568
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9105    0.8211    0.8635      1722
    Prophase     0.7293    0.8296    0.7762      1039
    Anaphase     0.5294    0.9000    0.6667        10
          G1     0.2880    0.7432    0.4151        74
   Metaphase     0.6198    0.5589    0.5878      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7536      3872
   macro avg     0.6692    0.8361    0.7271      3872
weighted avg     0.7724    0.7536    0.7586      3872

INFO:root:epoch104
INFO:root:[105,    50] training loss: 0.00909505
INFO:root:[105,   100] training loss: 0.00878166
INFO:root:[105,   150] training loss: 0.00957440
INFO:root:[105,   200] training loss: 0.00833268
INFO:root:[105,   250] training loss: 0.00794793
INFO:root:[105,   300] training loss: 0.00937600
INFO:root:[105,   350] training loss: 0.00765228
INFO:root:[105,   400] training loss: 0.00000647
INFO:root:[105,   450] training loss: 0.00004813
INFO:root:[105,   500] training loss: 0.00011565
INFO:root:[105,   550] training loss: 0.00025197
INFO:root:[105,   600] training loss: 0.00019068
INFO:root:[105,   650] training loss: 0.00004033
INFO:root:[105,   700] training loss: 0.00001527
INFO:root:[105,   750] training loss: 0.00027815
INFO:root:[105,   800] training loss: 0.00004606
INFO:root:[105,   850] training loss: 0.00005247
INFO:root:[105,   900] training loss: 0.00405262
INFO:root:[105,   950] training loss: 0.00105312
INFO:root:[105,  1000] training loss: 0.00003686
INFO:root:[105,  1050] training loss: 0.00002453
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    1.0000    0.6667         3
           S     0.9129    0.8281    0.8685      1722
    Prophase     0.7334    0.8104    0.7700      1039
    Anaphase     0.8889    0.8000    0.8421        10
          G1     0.2974    0.7838    0.4312        74
   Metaphase     0.6265    0.5815    0.6032      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7580      3872
   macro avg     0.6880    0.8291    0.7292      3872
weighted avg     0.7772    0.7580    0.7638      3872

INFO:root:epoch105
INFO:root:[106,    50] training loss: 0.00850810
INFO:root:[106,   100] training loss: 0.00954218
INFO:root:[106,   150] training loss: 0.00989983
INFO:root:[106,   200] training loss: 0.00754378
INFO:root:[106,   250] training loss: 0.00767096
INFO:root:[106,   300] training loss: 0.00868497
INFO:root:[106,   350] training loss: 0.00938203
INFO:root:[106,   400] training loss: 0.00000410
INFO:root:[106,   450] training loss: 0.00000608
INFO:root:[106,   500] training loss: 0.00008697
INFO:root:[106,   550] training loss: 0.00026170
INFO:root:[106,   600] training loss: 0.00025371
INFO:root:[106,   650] training loss: 0.00002788
INFO:root:[106,   700] training loss: 0.00001766
INFO:root:[106,   750] training loss: 0.00018909
INFO:root:[106,   800] training loss: 0.00004010
INFO:root:[106,   850] training loss: 0.00003279
INFO:root:[106,   900] training loss: 0.00494269
INFO:root:[106,   950] training loss: 0.00117162
INFO:root:[106,  1000] training loss: 0.00004427
INFO:root:[106,  1050] training loss: 0.00002684
INFO:root:              precision    recall  f1-score   support

          G2     0.3750    1.0000    0.5455         3
           S     0.9104    0.8200    0.8628      1722
    Prophase     0.7351    0.8171    0.7739      1039
    Anaphase     0.6429    0.9000    0.7500        10
          G1     0.2921    0.7027    0.4127        74
   Metaphase     0.6148    0.5786    0.5962      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7541      3872
   macro avg     0.6172    0.8312    0.6855      3872
weighted avg     0.7725    0.7541    0.7597      3872

INFO:root:epoch106
INFO:root:[107,    50] training loss: 0.00853581
INFO:root:[107,   100] training loss: 0.00827575
INFO:root:[107,   150] training loss: 0.00939929
INFO:root:[107,   200] training loss: 0.00796192
INFO:root:[107,   250] training loss: 0.01011183
INFO:root:[107,   300] training loss: 0.00913627
INFO:root:[107,   350] training loss: 0.00853525
INFO:root:[107,   400] training loss: 0.00001326
INFO:root:[107,   450] training loss: 0.00001574
INFO:root:[107,   500] training loss: 0.00006944
INFO:root:[107,   550] training loss: 0.00024913
INFO:root:[107,   600] training loss: 0.00022036
INFO:root:[107,   650] training loss: 0.00006474
INFO:root:[107,   700] training loss: 0.00002131
INFO:root:[107,   750] training loss: 0.00018609
INFO:root:[107,   800] training loss: 0.00004133
INFO:root:[107,   850] training loss: 0.00002559
INFO:root:[107,   900] training loss: 0.00438265
INFO:root:[107,   950] training loss: 0.00164597
INFO:root:[107,  1000] training loss: 0.00005891
INFO:root:[107,  1050] training loss: 0.00002785
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9301    0.7724    0.8439      1722
    Prophase     0.7905    0.7700    0.7801      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.2767    0.7703    0.4071        74
   Metaphase     0.5915    0.6984    0.6405      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7528      3872
   macro avg     0.6923    0.8301    0.7350      3872
weighted avg     0.7904    0.7528    0.7649      3872

INFO:root:epoch107
INFO:root:[108,    50] training loss: 0.00877734
INFO:root:[108,   100] training loss: 0.00953718
INFO:root:[108,   150] training loss: 0.01009601
INFO:root:[108,   200] training loss: 0.00960631
INFO:root:[108,   250] training loss: 0.00758437
INFO:root:[108,   300] training loss: 0.00952865
INFO:root:[108,   350] training loss: 0.00830164
INFO:root:[108,   400] training loss: 0.00000974
INFO:root:[108,   450] training loss: 0.00000819
INFO:root:[108,   500] training loss: 0.00009941
INFO:root:[108,   550] training loss: 0.00023303
INFO:root:[108,   600] training loss: 0.00019743
INFO:root:[108,   650] training loss: 0.00003432
INFO:root:[108,   700] training loss: 0.00001886
INFO:root:[108,   750] training loss: 0.00021447
INFO:root:[108,   800] training loss: 0.00005582
INFO:root:[108,   850] training loss: 0.00002935
INFO:root:[108,   900] training loss: 0.00460881
INFO:root:[108,   950] training loss: 0.00130390
INFO:root:[108,  1000] training loss: 0.00003582
INFO:root:[108,  1050] training loss: 0.00002786
INFO:root:              precision    recall  f1-score   support

          G2     0.3333    1.0000    0.5000         3
           S     0.9215    0.7973    0.8549      1722
    Prophase     0.7363    0.8008    0.7672      1039
    Anaphase     0.5294    0.9000    0.6667        10
          G1     0.2842    0.7027    0.4047        74
   Metaphase     0.5932    0.6031    0.5981      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7461      3872
   macro avg     0.5926    0.8291    0.6641      3872
weighted avg     0.7716    0.7461    0.7545      3872

INFO:root:epoch108
INFO:root:[109,    50] training loss: 0.00907291
INFO:root:[109,   100] training loss: 0.00822298
INFO:root:[109,   150] training loss: 0.00852958
INFO:root:[109,   200] training loss: 0.00745909
INFO:root:[109,   250] training loss: 0.00723784
INFO:root:[109,   300] training loss: 0.00788199
INFO:root:[109,   350] training loss: 0.00661248
INFO:root:[109,   400] training loss: 0.00000887
INFO:root:[109,   450] training loss: 0.00001199
INFO:root:[109,   500] training loss: 0.00002766
INFO:root:[109,   550] training loss: 0.00025964
INFO:root:[109,   600] training loss: 0.00021419
INFO:root:[109,   650] training loss: 0.00007085
INFO:root:[109,   700] training loss: 0.00004641
INFO:root:[109,   750] training loss: 0.00077475
INFO:root:[109,   800] training loss: 0.00036072
INFO:root:[109,   850] training loss: 0.00029787
INFO:root:[109,   900] training loss: 0.00437296
INFO:root:[109,   950] training loss: 0.00122020
INFO:root:[109,  1000] training loss: 0.00002374
INFO:root:[109,  1050] training loss: 0.00002250
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9226    0.7962    0.8547      1722
    Prophase     0.7846    0.8412    0.8119      1039
    Anaphase     0.8750    0.7000    0.7778        10
          G1     0.3846    0.7432    0.5069        74
   Metaphase     0.6101    0.6640    0.6359      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7727      3872
   macro avg     0.7038    0.8207    0.7421      3872
weighted avg     0.7925    0.7727    0.7788      3872

INFO:root:epoch109
INFO:root:[110,    50] training loss: 0.00788653
INFO:root:[110,   100] training loss: 0.00849430
INFO:root:[110,   150] training loss: 0.00821882
INFO:root:[110,   200] training loss: 0.00694443
INFO:root:[110,   250] training loss: 0.00619701
INFO:root:[110,   300] training loss: 0.00830740
INFO:root:[110,   350] training loss: 0.00606910
INFO:root:[110,   400] training loss: 0.00002754
INFO:root:[110,   450] training loss: 0.00001169
INFO:root:[110,   500] training loss: 0.00002122
INFO:root:[110,   550] training loss: 0.00030995
INFO:root:[110,   600] training loss: 0.00013569
INFO:root:[110,   650] training loss: 0.00004475
INFO:root:[110,   700] training loss: 0.00004153
INFO:root:[110,   750] training loss: 0.00049465
INFO:root:[110,   800] training loss: 0.00031513
INFO:root:[110,   850] training loss: 0.00025226
INFO:root:[110,   900] training loss: 0.00349608
INFO:root:[110,   950] training loss: 0.00102447
INFO:root:[110,  1000] training loss: 0.00002187
INFO:root:[110,  1050] training loss: 0.00004210
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9217    0.8002    0.8567      1722
    Prophase     0.7896    0.8595    0.8230      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.4615    0.7297    0.5654        74
   Metaphase     0.6143    0.6680    0.6400      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7805      3872
   macro avg     0.7267    0.8368    0.7714      3872
weighted avg     0.7959    0.7805    0.7850      3872

INFO:root:epoch110
INFO:root:[111,    50] training loss: 0.00771515
INFO:root:[111,   100] training loss: 0.00758884
INFO:root:[111,   150] training loss: 0.00774010
INFO:root:[111,   200] training loss: 0.00677602
INFO:root:[111,   250] training loss: 0.00639236
INFO:root:[111,   300] training loss: 0.00744980
INFO:root:[111,   350] training loss: 0.00646836
INFO:root:[111,   400] training loss: 0.00000999
INFO:root:[111,   450] training loss: 0.00000760
INFO:root:[111,   500] training loss: 0.00002728
INFO:root:[111,   550] training loss: 0.00027776
INFO:root:[111,   600] training loss: 0.00012759
INFO:root:[111,   650] training loss: 0.00002759
INFO:root:[111,   700] training loss: 0.00001712
INFO:root:[111,   750] training loss: 0.00039211
INFO:root:[111,   800] training loss: 0.00032746
INFO:root:[111,   850] training loss: 0.00015564
INFO:root:[111,   900] training loss: 0.00367553
INFO:root:[111,   950] training loss: 0.00099637
INFO:root:[111,  1000] training loss: 0.00002255
INFO:root:[111,  1050] training loss: 0.00001698
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9162    0.8188    0.8648      1722
    Prophase     0.7940    0.8460    0.8192      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.4000    0.7297    0.5167        74
   Metaphase     0.6292    0.6601    0.6443      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7831      3872
   macro avg     0.6985    0.8364    0.7503      3872
weighted avg     0.7973    0.7831    0.7877      3872

INFO:root:epoch111
INFO:root:[112,    50] training loss: 0.00726286
INFO:root:[112,   100] training loss: 0.00727495
INFO:root:[112,   150] training loss: 0.00689149
INFO:root:[112,   200] training loss: 0.00680084
INFO:root:[112,   250] training loss: 0.00623128
INFO:root:[112,   300] training loss: 0.00754305
INFO:root:[112,   350] training loss: 0.00596702
INFO:root:[112,   400] training loss: 0.00001480
INFO:root:[112,   450] training loss: 0.00000935
INFO:root:[112,   500] training loss: 0.00002079
INFO:root:[112,   550] training loss: 0.00029359
INFO:root:[112,   600] training loss: 0.00012933
INFO:root:[112,   650] training loss: 0.00001964
INFO:root:[112,   700] training loss: 0.00001564
INFO:root:[112,   750] training loss: 0.00036371
INFO:root:[112,   800] training loss: 0.00029979
INFO:root:[112,   850] training loss: 0.00018901
INFO:root:[112,   900] training loss: 0.00337590
INFO:root:[112,   950] training loss: 0.00147167
INFO:root:[112,  1000] training loss: 0.00002477
INFO:root:[112,  1050] training loss: 0.00001632
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9159    0.8281    0.8698      1722
    Prophase     0.7916    0.8518    0.8206      1039
    Anaphase     0.8889    0.8000    0.8421        10
          G1     0.4488    0.7703    0.5672        74
   Metaphase     0.6349    0.6542    0.6444      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7880      3872
   macro avg     0.7400    0.8435    0.7798      3872
weighted avg     0.7993    0.7880    0.7914      3872

INFO:root:epoch112
INFO:root:[113,    50] training loss: 0.00742156
INFO:root:[113,   100] training loss: 0.00710003
INFO:root:[113,   150] training loss: 0.00736060
INFO:root:[113,   200] training loss: 0.00648970
INFO:root:[113,   250] training loss: 0.00634299
INFO:root:[113,   300] training loss: 0.00730107
INFO:root:[113,   350] training loss: 0.00620229
INFO:root:[113,   400] training loss: 0.00001403
INFO:root:[113,   450] training loss: 0.00000958
INFO:root:[113,   500] training loss: 0.00001451
INFO:root:[113,   550] training loss: 0.00025047
INFO:root:[113,   600] training loss: 0.00013375
INFO:root:[113,   650] training loss: 0.00002465
INFO:root:[113,   700] training loss: 0.00002372
INFO:root:[113,   750] training loss: 0.00032287
INFO:root:[113,   800] training loss: 0.00026200
INFO:root:[113,   850] training loss: 0.00014483
INFO:root:[113,   900] training loss: 0.00346434
INFO:root:[113,   950] training loss: 0.00078763
INFO:root:[113,  1000] training loss: 0.00002550
INFO:root:[113,  1050] training loss: 0.00002231
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9240    0.8124    0.8646      1722
    Prophase     0.7923    0.8556    0.8228      1039
    Anaphase     0.8889    0.8000    0.8421        10
          G1     0.4331    0.7432    0.5473        74
   Metaphase     0.6250    0.6680    0.6458      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7851      3872
   macro avg     0.7315    0.8399    0.7708      3872
weighted avg     0.8002    0.7851    0.7897      3872

INFO:root:epoch113
INFO:root:[114,    50] training loss: 0.00693506
INFO:root:[114,   100] training loss: 0.00726668
INFO:root:[114,   150] training loss: 0.00753448
INFO:root:[114,   200] training loss: 0.00628175
INFO:root:[114,   250] training loss: 0.00587171
INFO:root:[114,   300] training loss: 0.00712286
INFO:root:[114,   350] training loss: 0.00605441
INFO:root:[114,   400] training loss: 0.00006601
INFO:root:[114,   450] training loss: 0.00002544
INFO:root:[114,   500] training loss: 0.00002106
INFO:root:[114,   550] training loss: 0.00023102
INFO:root:[114,   600] training loss: 0.00014718
INFO:root:[114,   650] training loss: 0.00002262
INFO:root:[114,   700] training loss: 0.00001631
INFO:root:[114,   750] training loss: 0.00024393
INFO:root:[114,   800] training loss: 0.00021484
INFO:root:[114,   850] training loss: 0.00013333
INFO:root:[114,   900] training loss: 0.00406951
INFO:root:[114,   950] training loss: 0.00105842
INFO:root:[114,  1000] training loss: 0.00001934
INFO:root:[114,  1050] training loss: 0.00001607
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9172    0.8229    0.8675      1722
    Prophase     0.7985    0.8470    0.8220      1039
    Anaphase     0.8889    0.8000    0.8421        10
          G1     0.4426    0.7297    0.5510        74
   Metaphase     0.6331    0.6729    0.6524      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7885      3872
   macro avg     0.7339    0.8389    0.7726      3872
weighted avg     0.8012    0.7885    0.7926      3872

INFO:root:epoch114
INFO:root:[115,    50] training loss: 0.00679073
INFO:root:[115,   100] training loss: 0.00925983
INFO:root:[115,   150] training loss: 0.00734907
INFO:root:[115,   200] training loss: 0.00630937
INFO:root:[115,   250] training loss: 0.00589843
INFO:root:[115,   300] training loss: 0.00749871
INFO:root:[115,   350] training loss: 0.00600127
INFO:root:[115,   400] training loss: 0.00001302
INFO:root:[115,   450] training loss: 0.00000651
INFO:root:[115,   500] training loss: 0.00001374
INFO:root:[115,   550] training loss: 0.00024111
INFO:root:[115,   600] training loss: 0.00012298
INFO:root:[115,   650] training loss: 0.00001534
INFO:root:[115,   700] training loss: 0.00001551
INFO:root:[115,   750] training loss: 0.00033965
INFO:root:[115,   800] training loss: 0.00023364
INFO:root:[115,   850] training loss: 0.00014891
INFO:root:[115,   900] training loss: 0.00278023
INFO:root:[115,   950] training loss: 0.00095092
INFO:root:[115,  1000] training loss: 0.00002480
INFO:root:[115,  1050] training loss: 0.00001743
INFO:root:              precision    recall  f1-score   support

          G2     0.6000    1.0000    0.7500         3
           S     0.9210    0.8188    0.8669      1722
    Prophase     0.7937    0.8518    0.8217      1039
    Anaphase     0.8889    0.8000    0.8421        10
          G1     0.4583    0.7432    0.5670        74
   Metaphase     0.6295    0.6709    0.6495      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7877      3872
   macro avg     0.7355    0.8407    0.7743      3872
weighted avg     0.8009    0.7877    0.7918      3872

INFO:root:epoch115
INFO:root:[116,    50] training loss: 0.00700152
INFO:root:[116,   100] training loss: 0.00887280
INFO:root:[116,   150] training loss: 0.00665370
INFO:root:[116,   200] training loss: 0.00697781
INFO:root:[116,   250] training loss: 0.00866014
INFO:root:[116,   300] training loss: 0.00734388
INFO:root:[116,   350] training loss: 0.00622030
INFO:root:[116,   400] training loss: 0.00000717
INFO:root:[116,   450] training loss: 0.00000662
INFO:root:[116,   500] training loss: 0.00003271
INFO:root:[116,   550] training loss: 0.00021627
INFO:root:[116,   600] training loss: 0.00014574
INFO:root:[116,   650] training loss: 0.00001330
INFO:root:[116,   700] training loss: 0.00001551
INFO:root:[116,   750] training loss: 0.00031003
INFO:root:[116,   800] training loss: 0.00020576
INFO:root:[116,   850] training loss: 0.00013175
INFO:root:[116,   900] training loss: 0.00287158
INFO:root:[116,   950] training loss: 0.00124151
INFO:root:[116,  1000] training loss: 0.00002154
INFO:root:[116,  1050] training loss: 0.00001606
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    1.0000    0.6667         3
           S     0.9184    0.8240    0.8687      1722
    Prophase     0.8064    0.8499    0.8276      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.4219    0.7297    0.5347        74
   Metaphase     0.6417    0.6807    0.6606      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7918      3872
   macro avg     0.6912    0.8406    0.7450      3872
weighted avg     0.8052    0.7918    0.7962      3872

INFO:root:epoch116
INFO:root:[117,    50] training loss: 0.00696694
INFO:root:[117,   100] training loss: 0.00687890
INFO:root:[117,   150] training loss: 0.00665018
INFO:root:[117,   200] training loss: 0.00621791
INFO:root:[117,   250] training loss: 0.00609419
INFO:root:[117,   300] training loss: 0.00722470
INFO:root:[117,   350] training loss: 0.00626815
INFO:root:[117,   400] training loss: 0.00000641
INFO:root:[117,   450] training loss: 0.00000647
INFO:root:[117,   500] training loss: 0.00001215
INFO:root:[117,   550] training loss: 0.00025520
INFO:root:[117,   600] training loss: 0.00009027
INFO:root:[117,   650] training loss: 0.00001788
INFO:root:[117,   700] training loss: 0.00001356
INFO:root:[117,   750] training loss: 0.00020102
INFO:root:[117,   800] training loss: 0.00017519
INFO:root:[117,   850] training loss: 0.00010388
INFO:root:[117,   900] training loss: 0.00309779
INFO:root:[117,   950] training loss: 0.00094413
INFO:root:[117,  1000] training loss: 0.00002287
INFO:root:[117,  1050] training loss: 0.00001753
INFO:root:              precision    recall  f1-score   support

          G2     0.5000    1.0000    0.6667         3
           S     0.9196    0.8240    0.8692      1722
    Prophase     0.8057    0.8499    0.8272      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.4344    0.7162    0.5408        74
   Metaphase     0.6406    0.6847    0.6619      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7926      3872
   macro avg     0.7082    0.8393    0.7556      3872
weighted avg     0.8057    0.7926    0.7969      3872

INFO:root:epoch117
INFO:root:[118,    50] training loss: 0.00776801
INFO:root:[118,   100] training loss: 0.00667919
INFO:root:[118,   150] training loss: 0.00782638
INFO:root:[118,   200] training loss: 0.00632753
INFO:root:[118,   250] training loss: 0.00554261
INFO:root:[118,   300] training loss: 0.00693404
INFO:root:[118,   350] training loss: 0.00550116
INFO:root:[118,   400] training loss: 0.00000925
INFO:root:[118,   450] training loss: 0.00000681
INFO:root:[118,   500] training loss: 0.00001822
INFO:root:[118,   550] training loss: 0.00020818
INFO:root:[118,   600] training loss: 0.00012968
INFO:root:[118,   650] training loss: 0.00001695
INFO:root:[118,   700] training loss: 0.00000980
INFO:root:[118,   750] training loss: 0.00015826
INFO:root:[118,   800] training loss: 0.00015271
INFO:root:[118,   850] training loss: 0.00011829
INFO:root:[118,   900] training loss: 0.00309385
INFO:root:[118,   950] training loss: 0.00107870
INFO:root:[118,  1000] training loss: 0.00002343
INFO:root:[118,  1050] training loss: 0.00002078
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9141    0.8339    0.8722      1722
    Prophase     0.8040    0.8412    0.8222      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.4417    0.7162    0.5464        74
   Metaphase     0.6431    0.6778    0.6600      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7929      3872
   macro avg     0.7443    0.8384    0.7830      3872
weighted avg     0.8038    0.7929    0.7966      3872

INFO:root:epoch118
INFO:root:[119,    50] training loss: 0.00698803
INFO:root:[119,   100] training loss: 0.00668868
INFO:root:[119,   150] training loss: 0.00695649
INFO:root:[119,   200] training loss: 0.00610685
INFO:root:[119,   250] training loss: 0.00545719
INFO:root:[119,   300] training loss: 0.00686755
INFO:root:[119,   350] training loss: 0.00542299
INFO:root:[119,   400] training loss: 0.00000626
INFO:root:[119,   450] training loss: 0.00000684
INFO:root:[119,   500] training loss: 0.00001285
INFO:root:[119,   550] training loss: 0.00024923
INFO:root:[119,   600] training loss: 0.00012961
INFO:root:[119,   650] training loss: 0.00001545
INFO:root:[119,   700] training loss: 0.00000936
INFO:root:[119,   750] training loss: 0.00019431
INFO:root:[119,   800] training loss: 0.00018261
INFO:root:[119,   850] training loss: 0.00012293
INFO:root:[119,   900] training loss: 0.00274748
INFO:root:[119,   950] training loss: 0.00118700
INFO:root:[119,  1000] training loss: 0.00002930
INFO:root:[119,  1050] training loss: 0.00002545
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9166    0.8235    0.8675      1722
    Prophase     0.7989    0.8489    0.8231      1039
    Anaphase     0.6154    0.8000    0.6957        10
          G1     0.4370    0.7027    0.5389        74
   Metaphase     0.6364    0.6739    0.6546      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7890      3872
   macro avg     0.7159    0.8356    0.7657      3872
weighted avg     0.8012    0.7890    0.7930      3872

INFO:root:epoch119
INFO:root:[120,    50] training loss: 0.00705513
INFO:root:[120,   100] training loss: 0.00664411
INFO:root:[120,   150] training loss: 0.00668608
INFO:root:[120,   200] training loss: 0.00612681
INFO:root:[120,   250] training loss: 0.00572145
INFO:root:[120,   300] training loss: 0.00757884
INFO:root:[120,   350] training loss: 0.00563197
INFO:root:[120,   400] training loss: 0.00000563
INFO:root:[120,   450] training loss: 0.00000741
INFO:root:[120,   500] training loss: 0.00001099
INFO:root:[120,   550] training loss: 0.00020768
INFO:root:[120,   600] training loss: 0.00008723
INFO:root:[120,   650] training loss: 0.00001291
INFO:root:[120,   700] training loss: 0.00001237
INFO:root:[120,   750] training loss: 0.00015760
INFO:root:[120,   800] training loss: 0.00030593
INFO:root:[120,   850] training loss: 0.00022615
INFO:root:[120,   900] training loss: 0.00373958
INFO:root:[120,   950] training loss: 0.00095815
INFO:root:[120,  1000] training loss: 0.00001859
INFO:root:[120,  1050] training loss: 0.00001726
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9219    0.8159    0.8657      1722
    Prophase     0.7991    0.8499    0.8237      1039
    Anaphase     0.6667    0.8000    0.7273        10
          G1     0.4595    0.6892    0.5514        74
   Metaphase     0.6285    0.6847    0.6554      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7885      3872
   macro avg     0.7261    0.8342    0.7719      3872
weighted avg     0.8021    0.7885    0.7928      3872

INFO:root:epoch120
INFO:root:[121,    50] training loss: 0.00670267
INFO:root:[121,   100] training loss: 0.00657928
INFO:root:[121,   150] training loss: 0.00743844
INFO:root:[121,   200] training loss: 0.00601399
INFO:root:[121,   250] training loss: 0.00681523
INFO:root:[121,   300] training loss: 0.00765515
INFO:root:[121,   350] training loss: 0.00531073
INFO:root:[121,   400] training loss: 0.00000629
INFO:root:[121,   450] training loss: 0.00000581
INFO:root:[121,   500] training loss: 0.00001282
INFO:root:[121,   550] training loss: 0.00020731
INFO:root:[121,   600] training loss: 0.00012019
INFO:root:[121,   650] training loss: 0.00001331
INFO:root:[121,   700] training loss: 0.00001148
INFO:root:[121,   750] training loss: 0.00014364
INFO:root:[121,   800] training loss: 0.00021534
INFO:root:[121,   850] training loss: 0.00019904
INFO:root:[121,   900] training loss: 0.00287423
INFO:root:[121,   950] training loss: 0.00072338
INFO:root:[121,  1000] training loss: 0.00002727
INFO:root:[121,  1050] training loss: 0.00001677
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9230    0.8142    0.8652      1722
    Prophase     0.8016    0.8518    0.8259      1039
    Anaphase     0.6667    0.8000    0.7273        10
          G1     0.4595    0.6892    0.5514        74
   Metaphase     0.6293    0.6886    0.6576      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7893      3872
   macro avg     0.7114    0.8348    0.7631      3872
weighted avg     0.8033    0.7893    0.7937      3872

INFO:root:epoch121
INFO:root:[122,    50] training loss: 0.00638088
INFO:root:[122,   100] training loss: 0.00685930
INFO:root:[122,   150] training loss: 0.00638353
INFO:root:[122,   200] training loss: 0.00603445
INFO:root:[122,   250] training loss: 0.00523783
INFO:root:[122,   300] training loss: 0.00674180
INFO:root:[122,   350] training loss: 0.00524636
INFO:root:[122,   400] training loss: 0.00000654
INFO:root:[122,   450] training loss: 0.00000636
INFO:root:[122,   500] training loss: 0.00001332
INFO:root:[122,   550] training loss: 0.00021320
INFO:root:[122,   600] training loss: 0.00010846
INFO:root:[122,   650] training loss: 0.00002473
INFO:root:[122,   700] training loss: 0.00000889
INFO:root:[122,   750] training loss: 0.00020401
INFO:root:[122,   800] training loss: 0.00027809
INFO:root:[122,   850] training loss: 0.00018165
INFO:root:[122,   900] training loss: 0.00286089
INFO:root:[122,   950] training loss: 0.00090705
INFO:root:[122,  1000] training loss: 0.00002532
INFO:root:[122,  1050] training loss: 0.00001720
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8004    0.8527    0.8257      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4815    0.7027    0.5714        74
   Metaphase     0.6298    0.6886    0.6579      1018
   Telophase     0.7500    1.0000    0.8571         6

    accuracy                         0.7905      3872
   macro avg     0.7232    0.8371    0.7711      3872
weighted avg     0.8040    0.7905    0.7948      3872

INFO:root:epoch122
INFO:root:[123,    50] training loss: 0.00680841
INFO:root:[123,   100] training loss: 0.00652604
INFO:root:[123,   150] training loss: 0.00704271
INFO:root:[123,   200] training loss: 0.00595992
INFO:root:[123,   250] training loss: 0.00546144
INFO:root:[123,   300] training loss: 0.00690686
INFO:root:[123,   350] training loss: 0.00544684
INFO:root:[123,   400] training loss: 0.00001667
INFO:root:[123,   450] training loss: 0.00000568
INFO:root:[123,   500] training loss: 0.00001314
INFO:root:[123,   550] training loss: 0.00019083
INFO:root:[123,   600] training loss: 0.00012689
INFO:root:[123,   650] training loss: 0.00003739
INFO:root:[123,   700] training loss: 0.00001019
INFO:root:[123,   750] training loss: 0.00022187
INFO:root:[123,   800] training loss: 0.00031315
INFO:root:[123,   850] training loss: 0.00019581
INFO:root:[123,   900] training loss: 0.00273712
INFO:root:[123,   950] training loss: 0.00086901
INFO:root:[123,  1000] training loss: 0.00002227
INFO:root:[123,  1050] training loss: 0.00001564
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9248    0.8142    0.8660      1722
    Prophase     0.7996    0.8527    0.8253      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4727    0.7027    0.5652        74
   Metaphase     0.6299    0.6906    0.6589      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7903      3872
   macro avg     0.7374    0.8372    0.7796      3872
weighted avg     0.8043    0.7903    0.7947      3872

INFO:root:epoch123
INFO:root:[124,    50] training loss: 0.00661671
INFO:root:[124,   100] training loss: 0.00640622
INFO:root:[124,   150] training loss: 0.00654949
INFO:root:[124,   200] training loss: 0.00594438
INFO:root:[124,   250] training loss: 0.00522872
INFO:root:[124,   300] training loss: 0.00637614
INFO:root:[124,   350] training loss: 0.00575925
INFO:root:[124,   400] training loss: 0.00002821
INFO:root:[124,   450] training loss: 0.00000787
INFO:root:[124,   500] training loss: 0.00001089
INFO:root:[124,   550] training loss: 0.00024839
INFO:root:[124,   600] training loss: 0.00009219
INFO:root:[124,   650] training loss: 0.00001384
INFO:root:[124,   700] training loss: 0.00001129
INFO:root:[124,   750] training loss: 0.00013911
INFO:root:[124,   800] training loss: 0.00024683
INFO:root:[124,   850] training loss: 0.00021097
INFO:root:[124,   900] training loss: 0.00346622
INFO:root:[124,   950] training loss: 0.00076641
INFO:root:[124,  1000] training loss: 0.00002036
INFO:root:[124,  1050] training loss: 0.00001772
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9244    0.8165    0.8671      1722
    Prophase     0.8007    0.8508    0.8250      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4771    0.7027    0.5683        74
   Metaphase     0.6308    0.6916    0.6598      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7911      3872
   macro avg     0.7382    0.8374    0.7803      3872
weighted avg     0.8047    0.7911    0.7954      3872

INFO:root:epoch124
INFO:root:[125,    50] training loss: 0.00750698
INFO:root:[125,   100] training loss: 0.00657159
INFO:root:[125,   150] training loss: 0.00638039
INFO:root:[125,   200] training loss: 0.00587443
INFO:root:[125,   250] training loss: 0.00615247
INFO:root:[125,   300] training loss: 0.00673436
INFO:root:[125,   350] training loss: 0.00564491
INFO:root:[125,   400] training loss: 0.00000976
INFO:root:[125,   450] training loss: 0.00000562
INFO:root:[125,   500] training loss: 0.00001114
INFO:root:[125,   550] training loss: 0.00019568
INFO:root:[125,   600] training loss: 0.00010629
INFO:root:[125,   650] training loss: 0.00001303
INFO:root:[125,   700] training loss: 0.00001011
INFO:root:[125,   750] training loss: 0.00012595
INFO:root:[125,   800] training loss: 0.00023024
INFO:root:[125,   850] training loss: 0.00017704
INFO:root:[125,   900] training loss: 0.00234595
INFO:root:[125,   950] training loss: 0.00076413
INFO:root:[125,  1000] training loss: 0.00002296
INFO:root:[125,  1050] training loss: 0.00001640
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8004    0.8489    0.8239      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4727    0.7027    0.5652        74
   Metaphase     0.6294    0.6906    0.6585      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7900      3872
   macro avg     0.7372    0.8369    0.7795      3872
weighted avg     0.8039    0.7900    0.7944      3872

INFO:root:epoch125
INFO:root:[126,    50] training loss: 0.00652071
INFO:root:[126,   100] training loss: 0.00680486
INFO:root:[126,   150] training loss: 0.00680748
INFO:root:[126,   200] training loss: 0.00592085
INFO:root:[126,   250] training loss: 0.00579352
INFO:root:[126,   300] training loss: 0.00678508
INFO:root:[126,   350] training loss: 0.00545332
INFO:root:[126,   400] training loss: 0.00000518
INFO:root:[126,   450] training loss: 0.00000811
INFO:root:[126,   500] training loss: 0.00001345
INFO:root:[126,   550] training loss: 0.00026261
INFO:root:[126,   600] training loss: 0.00009256
INFO:root:[126,   650] training loss: 0.00001502
INFO:root:[126,   700] training loss: 0.00000869
INFO:root:[126,   750] training loss: 0.00015851
INFO:root:[126,   800] training loss: 0.00014955
INFO:root:[126,   850] training loss: 0.00014479
INFO:root:[126,   900] training loss: 0.00291574
INFO:root:[126,   950] training loss: 0.00068164
INFO:root:[126,  1000] training loss: 0.00002064
INFO:root:[126,  1050] training loss: 0.00001894
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9239    0.8177    0.8675      1722
    Prophase     0.8000    0.8470    0.8228      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4685    0.7027    0.5622        74
   Metaphase     0.6305    0.6906    0.6592      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7903      3872
   macro avg     0.7368    0.8368    0.7791      3872
weighted avg     0.8041    0.7903    0.7947      3872

INFO:root:epoch126
INFO:root:[127,    50] training loss: 0.00642491
INFO:root:[127,   100] training loss: 0.00763936
INFO:root:[127,   150] training loss: 0.00674614
INFO:root:[127,   200] training loss: 0.00630236
INFO:root:[127,   250] training loss: 0.00580207
INFO:root:[127,   300] training loss: 0.00688991
INFO:root:[127,   350] training loss: 0.00569870
INFO:root:[127,   400] training loss: 0.00002213
INFO:root:[127,   450] training loss: 0.00000661
INFO:root:[127,   500] training loss: 0.00001186
INFO:root:[127,   550] training loss: 0.00020481
INFO:root:[127,   600] training loss: 0.00010744
INFO:root:[127,   650] training loss: 0.00001238
INFO:root:[127,   700] training loss: 0.00001144
INFO:root:[127,   750] training loss: 0.00015407
INFO:root:[127,   800] training loss: 0.00016188
INFO:root:[127,   850] training loss: 0.00014143
INFO:root:[127,   900] training loss: 0.00265646
INFO:root:[127,   950] training loss: 0.00098165
INFO:root:[127,  1000] training loss: 0.00002191
INFO:root:[127,  1050] training loss: 0.00001790
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9242    0.8142    0.8657      1722
    Prophase     0.8020    0.8537    0.8270      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4815    0.7027    0.5714        74
   Metaphase     0.6300    0.6925    0.6598      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7911      3872
   macro avg     0.7389    0.8376    0.7809      3872
weighted avg     0.8049    0.7911    0.7954      3872

INFO:root:epoch127
INFO:root:[128,    50] training loss: 0.00668272
INFO:root:[128,   100] training loss: 0.00697250
INFO:root:[128,   150] training loss: 0.00655028
INFO:root:[128,   200] training loss: 0.00591489
INFO:root:[128,   250] training loss: 0.00567957
INFO:root:[128,   300] training loss: 0.00679224
INFO:root:[128,   350] training loss: 0.00538988
INFO:root:[128,   400] training loss: 0.00001078
INFO:root:[128,   450] training loss: 0.00000724
INFO:root:[128,   500] training loss: 0.00001210
INFO:root:[128,   550] training loss: 0.00024137
INFO:root:[128,   600] training loss: 0.00011144
INFO:root:[128,   650] training loss: 0.00001553
INFO:root:[128,   700] training loss: 0.00000952
INFO:root:[128,   750] training loss: 0.00015214
INFO:root:[128,   800] training loss: 0.00015848
INFO:root:[128,   850] training loss: 0.00014198
INFO:root:[128,   900] training loss: 0.00337367
INFO:root:[128,   950] training loss: 0.00164618
INFO:root:[128,  1000] training loss: 0.00002023
INFO:root:[128,  1050] training loss: 0.00001687
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9239    0.8177    0.8675      1722
    Prophase     0.8022    0.8508    0.8258      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4815    0.7027    0.5714        74
   Metaphase     0.6317    0.6925    0.6607      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7391    0.8377    0.7811      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch128
INFO:root:[129,    50] training loss: 0.00652008
INFO:root:[129,   100] training loss: 0.00667680
INFO:root:[129,   150] training loss: 0.00621292
INFO:root:[129,   200] training loss: 0.00589155
INFO:root:[129,   250] training loss: 0.00545402
INFO:root:[129,   300] training loss: 0.00643218
INFO:root:[129,   350] training loss: 0.00577011
INFO:root:[129,   400] training loss: 0.00000763
INFO:root:[129,   450] training loss: 0.00000651
INFO:root:[129,   500] training loss: 0.00001223
INFO:root:[129,   550] training loss: 0.00019732
INFO:root:[129,   600] training loss: 0.00008877
INFO:root:[129,   650] training loss: 0.00001589
INFO:root:[129,   700] training loss: 0.00001070
INFO:root:[129,   750] training loss: 0.00010088
INFO:root:[129,   800] training loss: 0.00019238
INFO:root:[129,   850] training loss: 0.00015806
INFO:root:[129,   900] training loss: 0.00334886
INFO:root:[129,   950] training loss: 0.00078824
INFO:root:[129,  1000] training loss: 0.00001882
INFO:root:[129,  1050] training loss: 0.00001605
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9240    0.8188    0.8682      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6329    0.6925    0.6614      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7929      3872
   macro avg     0.7406    0.8381    0.7823      3872
weighted avg     0.8058    0.7929    0.7970      3872

INFO:root:epoch129
INFO:root:[130,    50] training loss: 0.00677695
INFO:root:[130,   100] training loss: 0.00723329
INFO:root:[130,   150] training loss: 0.00592015
INFO:root:[130,   200] training loss: 0.00628937
INFO:root:[130,   250] training loss: 0.00600407
INFO:root:[130,   300] training loss: 0.00668555
INFO:root:[130,   350] training loss: 0.00530000
INFO:root:[130,   400] training loss: 0.00000528
INFO:root:[130,   450] training loss: 0.00000586
INFO:root:[130,   500] training loss: 0.00001147
INFO:root:[130,   550] training loss: 0.00026902
INFO:root:[130,   600] training loss: 0.00012017
INFO:root:[130,   650] training loss: 0.00001132
INFO:root:[130,   700] training loss: 0.00000968
INFO:root:[130,   750] training loss: 0.00015642
INFO:root:[130,   800] training loss: 0.00016110
INFO:root:[130,   850] training loss: 0.00019544
INFO:root:[130,   900] training loss: 0.00303680
INFO:root:[130,   950] training loss: 0.00074775
INFO:root:[130,  1000] training loss: 0.00002106
INFO:root:[130,  1050] training loss: 0.00001623
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8153    0.8661      1722
    Prophase     0.8034    0.8576    0.8296      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5096    0.7162    0.5955        74
   Metaphase     0.6306    0.6925    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7929      3872
   macro avg     0.7535    0.8402    0.7902      3872
weighted avg     0.8059    0.7929    0.7969      3872

INFO:root:epoch130
INFO:root:[131,    50] training loss: 0.00697311
INFO:root:[131,   100] training loss: 0.00651577
INFO:root:[131,   150] training loss: 0.00682494
INFO:root:[131,   200] training loss: 0.00575338
INFO:root:[131,   250] training loss: 0.00530013
INFO:root:[131,   300] training loss: 0.00651230
INFO:root:[131,   350] training loss: 0.00523059
INFO:root:[131,   400] training loss: 0.00002296
INFO:root:[131,   450] training loss: 0.00002633
INFO:root:[131,   500] training loss: 0.00001245
INFO:root:[131,   550] training loss: 0.00022704
INFO:root:[131,   600] training loss: 0.00010401
INFO:root:[131,   650] training loss: 0.00001163
INFO:root:[131,   700] training loss: 0.00002613
INFO:root:[131,   750] training loss: 0.00012388
INFO:root:[131,   800] training loss: 0.00017408
INFO:root:[131,   850] training loss: 0.00021724
INFO:root:[131,   900] training loss: 0.00297954
INFO:root:[131,   950] training loss: 0.00074046
INFO:root:[131,  1000] training loss: 0.00002111
INFO:root:[131,  1050] training loss: 0.00003985
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8040    0.8566    0.8295      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5048    0.7162    0.5922        74
   Metaphase     0.6315    0.6935    0.6610      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7931      3872
   macro avg     0.7530    0.8403    0.7899      3872
weighted avg     0.8062    0.7931    0.7972      3872

INFO:root:epoch131
INFO:root:[132,    50] training loss: 0.00642317
INFO:root:[132,   100] training loss: 0.00646274
INFO:root:[132,   150] training loss: 0.00634833
INFO:root:[132,   200] training loss: 0.00638163
INFO:root:[132,   250] training loss: 0.00613262
INFO:root:[132,   300] training loss: 0.00659843
INFO:root:[132,   350] training loss: 0.00536163
INFO:root:[132,   400] training loss: 0.00000560
INFO:root:[132,   450] training loss: 0.00000714
INFO:root:[132,   500] training loss: 0.00001360
INFO:root:[132,   550] training loss: 0.00017145
INFO:root:[132,   600] training loss: 0.00011566
INFO:root:[132,   650] training loss: 0.00001781
INFO:root:[132,   700] training loss: 0.00001036
INFO:root:[132,   750] training loss: 0.00015583
INFO:root:[132,   800] training loss: 0.00019978
INFO:root:[132,   850] training loss: 0.00021154
INFO:root:[132,   900] training loss: 0.00280778
INFO:root:[132,   950] training loss: 0.00091504
INFO:root:[132,  1000] training loss: 0.00001941
INFO:root:[132,  1050] training loss: 0.00001660
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8153    0.8661      1722
    Prophase     0.8020    0.8537    0.8270      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6291    0.6916    0.6589      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7916      3872
   macro avg     0.7517    0.8395    0.7887      3872
weighted avg     0.8049    0.7916    0.7958      3872

INFO:root:epoch132
INFO:root:[133,    50] training loss: 0.00696198
INFO:root:[133,   100] training loss: 0.00676411
INFO:root:[133,   150] training loss: 0.00618017
INFO:root:[133,   200] training loss: 0.00615579
INFO:root:[133,   250] training loss: 0.00543586
INFO:root:[133,   300] training loss: 0.00679409
INFO:root:[133,   350] training loss: 0.00585144
INFO:root:[133,   400] training loss: 0.00000604
INFO:root:[133,   450] training loss: 0.00000564
INFO:root:[133,   500] training loss: 0.00001176
INFO:root:[133,   550] training loss: 0.00019308
INFO:root:[133,   600] training loss: 0.00010482
INFO:root:[133,   650] training loss: 0.00001275
INFO:root:[133,   700] training loss: 0.00001179
INFO:root:[133,   750] training loss: 0.00011669
INFO:root:[133,   800] training loss: 0.00016449
INFO:root:[133,   850] training loss: 0.00018556
INFO:root:[133,   900] training loss: 0.00299867
INFO:root:[133,   950] training loss: 0.00125873
INFO:root:[133,  1000] training loss: 0.00002019
INFO:root:[133,  1050] training loss: 0.00001787
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9236    0.8148    0.8658      1722
    Prophase     0.8018    0.8527    0.8265      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6280    0.6916    0.6583      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7911      3872
   macro avg     0.7515    0.8393    0.7885      3872
weighted avg     0.8046    0.7911    0.7953      3872

INFO:root:epoch133
INFO:root:[134,    50] training loss: 0.00861201
INFO:root:[134,   100] training loss: 0.00670892
INFO:root:[134,   150] training loss: 0.00639024
INFO:root:[134,   200] training loss: 0.00612655
INFO:root:[134,   250] training loss: 0.00518465
INFO:root:[134,   300] training loss: 0.00689568
INFO:root:[134,   350] training loss: 0.00570155
INFO:root:[134,   400] training loss: 0.00000884
INFO:root:[134,   450] training loss: 0.00000575
INFO:root:[134,   500] training loss: 0.00001250
INFO:root:[134,   550] training loss: 0.00021906
INFO:root:[134,   600] training loss: 0.00008681
INFO:root:[134,   650] training loss: 0.00001197
INFO:root:[134,   700] training loss: 0.00000847
INFO:root:[134,   750] training loss: 0.00020011
INFO:root:[134,   800] training loss: 0.00017422
INFO:root:[134,   850] training loss: 0.00015974
INFO:root:[134,   900] training loss: 0.00276636
INFO:root:[134,   950] training loss: 0.00069120
INFO:root:[134,  1000] training loss: 0.00002076
INFO:root:[134,  1050] training loss: 0.00002319
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9236    0.8148    0.8658      1722
    Prophase     0.8011    0.8527    0.8261      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6277    0.6906    0.6576      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7908      3872
   macro avg     0.7514    0.8392    0.7884      3872
weighted avg     0.8043    0.7908    0.7950      3872

INFO:root:epoch134
INFO:root:[135,    50] training loss: 0.00631614
INFO:root:[135,   100] training loss: 0.00795180
INFO:root:[135,   150] training loss: 0.00705746
INFO:root:[135,   200] training loss: 0.00590752
INFO:root:[135,   250] training loss: 0.00541102
INFO:root:[135,   300] training loss: 0.00698167
INFO:root:[135,   350] training loss: 0.00606190
INFO:root:[135,   400] training loss: 0.00000581
INFO:root:[135,   450] training loss: 0.00000629
INFO:root:[135,   500] training loss: 0.00001199
INFO:root:[135,   550] training loss: 0.00016550
INFO:root:[135,   600] training loss: 0.00010291
INFO:root:[135,   650] training loss: 0.00001604
INFO:root:[135,   700] training loss: 0.00000887
INFO:root:[135,   750] training loss: 0.00012380
INFO:root:[135,   800] training loss: 0.00014733
INFO:root:[135,   850] training loss: 0.00013416
INFO:root:[135,   900] training loss: 0.00260243
INFO:root:[135,   950] training loss: 0.00089820
INFO:root:[135,  1000] training loss: 0.00001980
INFO:root:[135,  1050] training loss: 0.00001503
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9236    0.8148    0.8658      1722
    Prophase     0.8018    0.8527    0.8265      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6280    0.6916    0.6583      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7911      3872
   macro avg     0.7515    0.8393    0.7885      3872
weighted avg     0.8046    0.7911    0.7953      3872

INFO:root:epoch135
INFO:root:[136,    50] training loss: 0.00724019
INFO:root:[136,   100] training loss: 0.00626653
INFO:root:[136,   150] training loss: 0.00664586
INFO:root:[136,   200] training loss: 0.00631702
INFO:root:[136,   250] training loss: 0.00533703
INFO:root:[136,   300] training loss: 0.00674433
INFO:root:[136,   350] training loss: 0.00590844
INFO:root:[136,   400] training loss: 0.00000747
INFO:root:[136,   450] training loss: 0.00000585
INFO:root:[136,   500] training loss: 0.00001389
INFO:root:[136,   550] training loss: 0.00017925
INFO:root:[136,   600] training loss: 0.00009420
INFO:root:[136,   650] training loss: 0.00001125
INFO:root:[136,   700] training loss: 0.00000948
INFO:root:[136,   750] training loss: 0.00013604
INFO:root:[136,   800] training loss: 0.00018825
INFO:root:[136,   850] training loss: 0.00019507
INFO:root:[136,   900] training loss: 0.00326654
INFO:root:[136,   950] training loss: 0.00109268
INFO:root:[136,  1000] training loss: 0.00001822
INFO:root:[136,  1050] training loss: 0.00001689
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9236    0.8148    0.8658      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6283    0.6925    0.6589      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7913      3872
   macro avg     0.7517    0.8395    0.7887      3872
weighted avg     0.8048    0.7913    0.7956      3872

INFO:root:epoch136
INFO:root:[137,    50] training loss: 0.00640720
INFO:root:[137,   100] training loss: 0.00646517
INFO:root:[137,   150] training loss: 0.00698041
INFO:root:[137,   200] training loss: 0.00625846
INFO:root:[137,   250] training loss: 0.00565068
INFO:root:[137,   300] training loss: 0.00650870
INFO:root:[137,   350] training loss: 0.00549587
INFO:root:[137,   400] training loss: 0.00000742
INFO:root:[137,   450] training loss: 0.00000991
INFO:root:[137,   500] training loss: 0.00001130
INFO:root:[137,   550] training loss: 0.00016713
INFO:root:[137,   600] training loss: 0.00010078
INFO:root:[137,   650] training loss: 0.00001289
INFO:root:[137,   700] training loss: 0.00000881
INFO:root:[137,   750] training loss: 0.00013918
INFO:root:[137,   800] training loss: 0.00020276
INFO:root:[137,   850] training loss: 0.00024950
INFO:root:[137,   900] training loss: 0.00263713
INFO:root:[137,   950] training loss: 0.00066402
INFO:root:[137,  1000] training loss: 0.00003000
INFO:root:[137,  1050] training loss: 0.00001912
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8153    0.8661      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6289    0.6925    0.6592      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7916      3872
   macro avg     0.7518    0.8395    0.7888      3872
weighted avg     0.8050    0.7916    0.7958      3872

INFO:root:epoch137
INFO:root:[138,    50] training loss: 0.00637850
INFO:root:[138,   100] training loss: 0.00625942
INFO:root:[138,   150] training loss: 0.00614107
INFO:root:[138,   200] training loss: 0.00579833
INFO:root:[138,   250] training loss: 0.00565672
INFO:root:[138,   300] training loss: 0.00679607
INFO:root:[138,   350] training loss: 0.00579169
INFO:root:[138,   400] training loss: 0.00000860
INFO:root:[138,   450] training loss: 0.00000679
INFO:root:[138,   500] training loss: 0.00001245
INFO:root:[138,   550] training loss: 0.00019889
INFO:root:[138,   600] training loss: 0.00009291
INFO:root:[138,   650] training loss: 0.00001110
INFO:root:[138,   700] training loss: 0.00001171
INFO:root:[138,   750] training loss: 0.00012905
INFO:root:[138,   800] training loss: 0.00017498
INFO:root:[138,   850] training loss: 0.00014335
INFO:root:[138,   900] training loss: 0.00224623
INFO:root:[138,   950] training loss: 0.00078811
INFO:root:[138,  1000] training loss: 0.00001802
INFO:root:[138,  1050] training loss: 0.00001851
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8153    0.8661      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6289    0.6925    0.6592      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7916      3872
   macro avg     0.7518    0.8395    0.7888      3872
weighted avg     0.8050    0.7916    0.7958      3872

INFO:root:epoch138
INFO:root:[139,    50] training loss: 0.00668201
INFO:root:[139,   100] training loss: 0.00725833
INFO:root:[139,   150] training loss: 0.00634740
INFO:root:[139,   200] training loss: 0.00676764
INFO:root:[139,   250] training loss: 0.00547206
INFO:root:[139,   300] training loss: 0.00771808
INFO:root:[139,   350] training loss: 0.00557705
INFO:root:[139,   400] training loss: 0.00000606
INFO:root:[139,   450] training loss: 0.00000544
INFO:root:[139,   500] training loss: 0.00001583
INFO:root:[139,   550] training loss: 0.00024580
INFO:root:[139,   600] training loss: 0.00009131
INFO:root:[139,   650] training loss: 0.00001183
INFO:root:[139,   700] training loss: 0.00001454
INFO:root:[139,   750] training loss: 0.00013712
INFO:root:[139,   800] training loss: 0.00014688
INFO:root:[139,   850] training loss: 0.00012670
INFO:root:[139,   900] training loss: 0.00239994
INFO:root:[139,   950] training loss: 0.00065693
INFO:root:[139,  1000] training loss: 0.00002663
INFO:root:[139,  1050] training loss: 0.00001687
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9236    0.8148    0.8658      1722
    Prophase     0.8027    0.8537    0.8274      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6289    0.6925    0.6592      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7916      3872
   macro avg     0.7518    0.8396    0.7888      3872
weighted avg     0.8050    0.7916    0.7958      3872

INFO:root:epoch139
INFO:root:[140,    50] training loss: 0.00659322
INFO:root:[140,   100] training loss: 0.00660602
INFO:root:[140,   150] training loss: 0.00649318
INFO:root:[140,   200] training loss: 0.00598847
INFO:root:[140,   250] training loss: 0.00763198
INFO:root:[140,   300] training loss: 0.00686175
INFO:root:[140,   350] training loss: 0.00571139
INFO:root:[140,   400] training loss: 0.00000711
INFO:root:[140,   450] training loss: 0.00000831
INFO:root:[140,   500] training loss: 0.00003070
INFO:root:[140,   550] training loss: 0.00021333
INFO:root:[140,   600] training loss: 0.00010440
INFO:root:[140,   650] training loss: 0.00001704
INFO:root:[140,   700] training loss: 0.00000952
INFO:root:[140,   750] training loss: 0.00012383
INFO:root:[140,   800] training loss: 0.00018537
INFO:root:[140,   850] training loss: 0.00021775
INFO:root:[140,   900] training loss: 0.00274467
INFO:root:[140,   950] training loss: 0.00067687
INFO:root:[140,  1000] training loss: 0.00001949
INFO:root:[140,  1050] training loss: 0.00001667
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6295    0.6925    0.6595      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7518    0.8396    0.7889      3872
weighted avg     0.8052    0.7918    0.7960      3872

INFO:root:epoch140
INFO:root:[141,    50] training loss: 0.00672257
INFO:root:[141,   100] training loss: 0.00707343
INFO:root:[141,   150] training loss: 0.00640643
INFO:root:[141,   200] training loss: 0.00588780
INFO:root:[141,   250] training loss: 0.00545812
INFO:root:[141,   300] training loss: 0.00653386
INFO:root:[141,   350] training loss: 0.00531795
INFO:root:[141,   400] training loss: 0.00000586
INFO:root:[141,   450] training loss: 0.00000605
INFO:root:[141,   500] training loss: 0.00001185
INFO:root:[141,   550] training loss: 0.00019810
INFO:root:[141,   600] training loss: 0.00009040
INFO:root:[141,   650] training loss: 0.00001455
INFO:root:[141,   700] training loss: 0.00000763
INFO:root:[141,   750] training loss: 0.00011635
INFO:root:[141,   800] training loss: 0.00027960
INFO:root:[141,   850] training loss: 0.00012526
INFO:root:[141,   900] training loss: 0.00267377
INFO:root:[141,   950] training loss: 0.00094858
INFO:root:[141,  1000] training loss: 0.00001986
INFO:root:[141,  1050] training loss: 0.00002394
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9236    0.8148    0.8658      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6283    0.6925    0.6589      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7913      3872
   macro avg     0.7517    0.8395    0.7887      3872
weighted avg     0.8048    0.7913    0.7956      3872

INFO:root:epoch141
INFO:root:[142,    50] training loss: 0.00639976
INFO:root:[142,   100] training loss: 0.00845691
INFO:root:[142,   150] training loss: 0.00681153
INFO:root:[142,   200] training loss: 0.00590779
INFO:root:[142,   250] training loss: 0.00540453
INFO:root:[142,   300] training loss: 0.00667919
INFO:root:[142,   350] training loss: 0.00524567
INFO:root:[142,   400] training loss: 0.00000623
INFO:root:[142,   450] training loss: 0.00000660
INFO:root:[142,   500] training loss: 0.00001178
INFO:root:[142,   550] training loss: 0.00024619
INFO:root:[142,   600] training loss: 0.00007268
INFO:root:[142,   650] training loss: 0.00000990
INFO:root:[142,   700] training loss: 0.00001167
INFO:root:[142,   750] training loss: 0.00016313
INFO:root:[142,   800] training loss: 0.00018831
INFO:root:[142,   850] training loss: 0.00013891
INFO:root:[142,   900] training loss: 0.00305273
INFO:root:[142,   950] training loss: 0.00119677
INFO:root:[142,  1000] training loss: 0.00002036
INFO:root:[142,  1050] training loss: 0.00001547
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9236    0.8148    0.8658      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6283    0.6925    0.6589      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7913      3872
   macro avg     0.7517    0.8395    0.7887      3872
weighted avg     0.8048    0.7913    0.7956      3872

INFO:root:epoch142
INFO:root:[143,    50] training loss: 0.00728745
INFO:root:[143,   100] training loss: 0.00599473
INFO:root:[143,   150] training loss: 0.00678164
INFO:root:[143,   200] training loss: 0.00611469
INFO:root:[143,   250] training loss: 0.00553491
INFO:root:[143,   300] training loss: 0.00779711
INFO:root:[143,   350] training loss: 0.00570164
INFO:root:[143,   400] training loss: 0.00000850
INFO:root:[143,   450] training loss: 0.00000877
INFO:root:[143,   500] training loss: 0.00001625
INFO:root:[143,   550] training loss: 0.00018790
INFO:root:[143,   600] training loss: 0.00010030
INFO:root:[143,   650] training loss: 0.00001307
INFO:root:[143,   700] training loss: 0.00001449
INFO:root:[143,   750] training loss: 0.00019379
INFO:root:[143,   800] training loss: 0.00015516
INFO:root:[143,   850] training loss: 0.00018382
INFO:root:[143,   900] training loss: 0.00310910
INFO:root:[143,   950] training loss: 0.00095827
INFO:root:[143,  1000] training loss: 0.00002292
INFO:root:[143,  1050] training loss: 0.00001550
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9236    0.8148    0.8658      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6283    0.6925    0.6589      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7913      3872
   macro avg     0.7517    0.8395    0.7887      3872
weighted avg     0.8048    0.7913    0.7956      3872

INFO:root:epoch143
INFO:root:[144,    50] training loss: 0.00666837
INFO:root:[144,   100] training loss: 0.00637001
INFO:root:[144,   150] training loss: 0.00645899
INFO:root:[144,   200] training loss: 0.00592287
INFO:root:[144,   250] training loss: 0.00548087
INFO:root:[144,   300] training loss: 0.00658191
INFO:root:[144,   350] training loss: 0.00566493
INFO:root:[144,   400] training loss: 0.00000611
INFO:root:[144,   450] training loss: 0.00000631
INFO:root:[144,   500] training loss: 0.00001568
INFO:root:[144,   550] training loss: 0.00023391
INFO:root:[144,   600] training loss: 0.00008724
INFO:root:[144,   650] training loss: 0.00001019
INFO:root:[144,   700] training loss: 0.00001377
INFO:root:[144,   750] training loss: 0.00010515
INFO:root:[144,   800] training loss: 0.00019841
INFO:root:[144,   850] training loss: 0.00016967
INFO:root:[144,   900] training loss: 0.00244868
INFO:root:[144,   950] training loss: 0.00068436
INFO:root:[144,  1000] training loss: 0.00001781
INFO:root:[144,  1050] training loss: 0.00002615
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9236    0.8148    0.8658      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6283    0.6925    0.6589      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7913      3872
   macro avg     0.7517    0.8395    0.7887      3872
weighted avg     0.8048    0.7913    0.7956      3872

INFO:root:epoch144
INFO:root:[145,    50] training loss: 0.00633929
INFO:root:[145,   100] training loss: 0.00617969
INFO:root:[145,   150] training loss: 0.00637929
INFO:root:[145,   200] training loss: 0.00606097
INFO:root:[145,   250] training loss: 0.00513845
INFO:root:[145,   300] training loss: 0.00652737
INFO:root:[145,   350] training loss: 0.00548021
INFO:root:[145,   400] training loss: 0.00000682
INFO:root:[145,   450] training loss: 0.00000569
INFO:root:[145,   500] training loss: 0.00001520
INFO:root:[145,   550] training loss: 0.00024851
INFO:root:[145,   600] training loss: 0.00009454
INFO:root:[145,   650] training loss: 0.00001721
INFO:root:[145,   700] training loss: 0.00001020
INFO:root:[145,   750] training loss: 0.00013429
INFO:root:[145,   800] training loss: 0.00016328
INFO:root:[145,   850] training loss: 0.00016820
INFO:root:[145,   900] training loss: 0.00260082
INFO:root:[145,   950] training loss: 0.00086820
INFO:root:[145,  1000] training loss: 0.00001935
INFO:root:[145,  1050] training loss: 0.00001836
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8153    0.8661      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6289    0.6925    0.6592      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7916      3872
   macro avg     0.7518    0.8395    0.7888      3872
weighted avg     0.8050    0.7916    0.7958      3872

INFO:root:epoch145
INFO:root:[146,    50] training loss: 0.00673739
INFO:root:[146,   100] training loss: 0.00628208
INFO:root:[146,   150] training loss: 0.00623972
INFO:root:[146,   200] training loss: 0.00574571
INFO:root:[146,   250] training loss: 0.00669909
INFO:root:[146,   300] training loss: 0.00709111
INFO:root:[146,   350] training loss: 0.00510055
INFO:root:[146,   400] training loss: 0.00000732
INFO:root:[146,   450] training loss: 0.00000582
INFO:root:[146,   500] training loss: 0.00001342
INFO:root:[146,   550] training loss: 0.00016125
INFO:root:[146,   600] training loss: 0.00009903
INFO:root:[146,   650] training loss: 0.00001263
INFO:root:[146,   700] training loss: 0.00000892
INFO:root:[146,   750] training loss: 0.00009968
INFO:root:[146,   800] training loss: 0.00014166
INFO:root:[146,   850] training loss: 0.00021250
INFO:root:[146,   900] training loss: 0.00319922
INFO:root:[146,   950] training loss: 0.00081148
INFO:root:[146,  1000] training loss: 0.00002214
INFO:root:[146,  1050] training loss: 0.00002395
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6295    0.6925    0.6595      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7518    0.8396    0.7889      3872
weighted avg     0.8052    0.7918    0.7960      3872

INFO:root:epoch146
INFO:root:[147,    50] training loss: 0.00641187
INFO:root:[147,   100] training loss: 0.00665078
INFO:root:[147,   150] training loss: 0.00638080
INFO:root:[147,   200] training loss: 0.00600651
INFO:root:[147,   250] training loss: 0.00556203
INFO:root:[147,   300] training loss: 0.00674020
INFO:root:[147,   350] training loss: 0.00530348
INFO:root:[147,   400] training loss: 0.00000635
INFO:root:[147,   450] training loss: 0.00000741
INFO:root:[147,   500] training loss: 0.00001271
INFO:root:[147,   550] training loss: 0.00019762
INFO:root:[147,   600] training loss: 0.00009700
INFO:root:[147,   650] training loss: 0.00001016
INFO:root:[147,   700] training loss: 0.00001043
INFO:root:[147,   750] training loss: 0.00009049
INFO:root:[147,   800] training loss: 0.00017126
INFO:root:[147,   850] training loss: 0.00012616
INFO:root:[147,   900] training loss: 0.00271069
INFO:root:[147,   950] training loss: 0.00092389
INFO:root:[147,  1000] training loss: 0.00002246
INFO:root:[147,  1050] training loss: 0.00001576
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6295    0.6925    0.6595      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7518    0.8396    0.7889      3872
weighted avg     0.8052    0.7918    0.7960      3872

INFO:root:epoch147
INFO:root:[148,    50] training loss: 0.00683153
INFO:root:[148,   100] training loss: 0.00625083
INFO:root:[148,   150] training loss: 0.00613336
INFO:root:[148,   200] training loss: 0.00598128
INFO:root:[148,   250] training loss: 0.00844119
INFO:root:[148,   300] training loss: 0.00826273
INFO:root:[148,   350] training loss: 0.00561242
INFO:root:[148,   400] training loss: 0.00000594
INFO:root:[148,   450] training loss: 0.00000999
INFO:root:[148,   500] training loss: 0.00001120
INFO:root:[148,   550] training loss: 0.00016311
INFO:root:[148,   600] training loss: 0.00009455
INFO:root:[148,   650] training loss: 0.00001298
INFO:root:[148,   700] training loss: 0.00001082
INFO:root:[148,   750] training loss: 0.00011660
INFO:root:[148,   800] training loss: 0.00017585
INFO:root:[148,   850] training loss: 0.00018378
INFO:root:[148,   900] training loss: 0.00261990
INFO:root:[148,   950] training loss: 0.00149441
INFO:root:[148,  1000] training loss: 0.00001868
INFO:root:[148,  1050] training loss: 0.00001690
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.8000    0.8000    0.8000        10
          G1     0.5000    0.7162    0.5889        74
   Metaphase     0.6295    0.6925    0.6595      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7518    0.8396    0.7889      3872
weighted avg     0.8052    0.7918    0.7960      3872

INFO:root:epoch148
INFO:root:[149,    50] training loss: 0.00824587
INFO:root:[149,   100] training loss: 0.00636281
INFO:root:[149,   150] training loss: 0.00628764
INFO:root:[149,   200] training loss: 0.00642557
INFO:root:[149,   250] training loss: 0.00557429
INFO:root:[149,   300] training loss: 0.00667224
INFO:root:[149,   350] training loss: 0.00539802
INFO:root:[149,   400] training loss: 0.00000654
INFO:root:[149,   450] training loss: 0.00000640
INFO:root:[149,   500] training loss: 0.00001255
INFO:root:[149,   550] training loss: 0.00019637
INFO:root:[149,   600] training loss: 0.00009545
INFO:root:[149,   650] training loss: 0.00001187
INFO:root:[149,   700] training loss: 0.00001037
INFO:root:[149,   750] training loss: 0.00009642
INFO:root:[149,   800] training loss: 0.00019869
INFO:root:[149,   850] training loss: 0.00014275
INFO:root:[149,   900] training loss: 0.00272221
INFO:root:[149,   950] training loss: 0.00085573
INFO:root:[149,  1000] training loss: 0.00001858
INFO:root:[149,  1050] training loss: 0.00001790
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8025    0.8527    0.8269      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6295    0.6925    0.6595      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7916      3872
   macro avg     0.7408    0.8377    0.7823      3872
weighted avg     0.8049    0.7916    0.7958      3872

INFO:root:epoch149
INFO:root:[150,    50] training loss: 0.00727439
INFO:root:[150,   100] training loss: 0.00616172
INFO:root:[150,   150] training loss: 0.00628629
INFO:root:[150,   200] training loss: 0.00592997
INFO:root:[150,   250] training loss: 0.00535194
INFO:root:[150,   300] training loss: 0.00652276
INFO:root:[150,   350] training loss: 0.00558285
INFO:root:[150,   400] training loss: 0.00000921
INFO:root:[150,   450] training loss: 0.00000682
INFO:root:[150,   500] training loss: 0.00001208
INFO:root:[150,   550] training loss: 0.00019531
INFO:root:[150,   600] training loss: 0.00010413
INFO:root:[150,   650] training loss: 0.00001164
INFO:root:[150,   700] training loss: 0.00000914
INFO:root:[150,   750] training loss: 0.00009988
INFO:root:[150,   800] training loss: 0.00020048
INFO:root:[150,   850] training loss: 0.00022819
INFO:root:[150,   900] training loss: 0.00273957
INFO:root:[150,   950] training loss: 0.00088218
INFO:root:[150,  1000] training loss: 0.00002368
INFO:root:[150,  1050] training loss: 0.00001574
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch150
INFO:root:[151,    50] training loss: 0.00665181
INFO:root:[151,   100] training loss: 0.00621920
INFO:root:[151,   150] training loss: 0.00612714
INFO:root:[151,   200] training loss: 0.00605582
INFO:root:[151,   250] training loss: 0.00540441
INFO:root:[151,   300] training loss: 0.00672792
INFO:root:[151,   350] training loss: 0.00541836
INFO:root:[151,   400] training loss: 0.00000813
INFO:root:[151,   450] training loss: 0.00007254
INFO:root:[151,   500] training loss: 0.00001442
INFO:root:[151,   550] training loss: 0.00019414
INFO:root:[151,   600] training loss: 0.00011613
INFO:root:[151,   650] training loss: 0.00001351
INFO:root:[151,   700] training loss: 0.00000889
INFO:root:[151,   750] training loss: 0.00015012
INFO:root:[151,   800] training loss: 0.00017959
INFO:root:[151,   850] training loss: 0.00018407
INFO:root:[151,   900] training loss: 0.00304124
INFO:root:[151,   950] training loss: 0.00091663
INFO:root:[151,  1000] training loss: 0.00002101
INFO:root:[151,  1050] training loss: 0.00001754
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch151
INFO:root:[152,    50] training loss: 0.00667167
INFO:root:[152,   100] training loss: 0.00774064
INFO:root:[152,   150] training loss: 0.00624876
INFO:root:[152,   200] training loss: 0.00597534
INFO:root:[152,   250] training loss: 0.00660505
INFO:root:[152,   300] training loss: 0.00686086
INFO:root:[152,   350] training loss: 0.00593241
INFO:root:[152,   400] training loss: 0.00000520
INFO:root:[152,   450] training loss: 0.00000549
INFO:root:[152,   500] training loss: 0.00002026
INFO:root:[152,   550] training loss: 0.00020830
INFO:root:[152,   600] training loss: 0.00009885
INFO:root:[152,   650] training loss: 0.00001494
INFO:root:[152,   700] training loss: 0.00000927
INFO:root:[152,   750] training loss: 0.00013842
INFO:root:[152,   800] training loss: 0.00015919
INFO:root:[152,   850] training loss: 0.00013380
INFO:root:[152,   900] training loss: 0.00375589
INFO:root:[152,   950] training loss: 0.00166423
INFO:root:[152,  1000] training loss: 0.00002133
INFO:root:[152,  1050] training loss: 0.00002090
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch152
INFO:root:[153,    50] training loss: 0.00650869
INFO:root:[153,   100] training loss: 0.00802252
INFO:root:[153,   150] training loss: 0.00717434
INFO:root:[153,   200] training loss: 0.00614530
INFO:root:[153,   250] training loss: 0.00547939
INFO:root:[153,   300] training loss: 0.00669858
INFO:root:[153,   350] training loss: 0.00509051
INFO:root:[153,   400] training loss: 0.00000486
INFO:root:[153,   450] training loss: 0.00000665
INFO:root:[153,   500] training loss: 0.00001307
INFO:root:[153,   550] training loss: 0.00018440
INFO:root:[153,   600] training loss: 0.00009991
INFO:root:[153,   650] training loss: 0.00001430
INFO:root:[153,   700] training loss: 0.00001116
INFO:root:[153,   750] training loss: 0.00013231
INFO:root:[153,   800] training loss: 0.00015590
INFO:root:[153,   850] training loss: 0.00025431
INFO:root:[153,   900] training loss: 0.00248078
INFO:root:[153,   950] training loss: 0.00138952
INFO:root:[153,  1000] training loss: 0.00001988
INFO:root:[153,  1050] training loss: 0.00001607
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch153
INFO:root:[154,    50] training loss: 0.00682480
INFO:root:[154,   100] training loss: 0.00646120
INFO:root:[154,   150] training loss: 0.00612570
INFO:root:[154,   200] training loss: 0.00673428
INFO:root:[154,   250] training loss: 0.00615145
INFO:root:[154,   300] training loss: 0.00732706
INFO:root:[154,   350] training loss: 0.00543767
INFO:root:[154,   400] training loss: 0.00001919
INFO:root:[154,   450] training loss: 0.00000532
INFO:root:[154,   500] training loss: 0.00001184
INFO:root:[154,   550] training loss: 0.00021792
INFO:root:[154,   600] training loss: 0.00009940
INFO:root:[154,   650] training loss: 0.00001190
INFO:root:[154,   700] training loss: 0.00001138
INFO:root:[154,   750] training loss: 0.00014032
INFO:root:[154,   800] training loss: 0.00016722
INFO:root:[154,   850] training loss: 0.00018172
INFO:root:[154,   900] training loss: 0.00236238
INFO:root:[154,   950] training loss: 0.00078302
INFO:root:[154,  1000] training loss: 0.00002125
INFO:root:[154,  1050] training loss: 0.00001566
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch154
INFO:root:[155,    50] training loss: 0.00661233
INFO:root:[155,   100] training loss: 0.00599748
INFO:root:[155,   150] training loss: 0.00657714
INFO:root:[155,   200] training loss: 0.00627832
INFO:root:[155,   250] training loss: 0.00569200
INFO:root:[155,   300] training loss: 0.00660646
INFO:root:[155,   350] training loss: 0.00553544
INFO:root:[155,   400] training loss: 0.00000757
INFO:root:[155,   450] training loss: 0.00000849
INFO:root:[155,   500] training loss: 0.00001472
INFO:root:[155,   550] training loss: 0.00022900
INFO:root:[155,   600] training loss: 0.00009753
INFO:root:[155,   650] training loss: 0.00001020
INFO:root:[155,   700] training loss: 0.00000836
INFO:root:[155,   750] training loss: 0.00015060
INFO:root:[155,   800] training loss: 0.00027723
INFO:root:[155,   850] training loss: 0.00019043
INFO:root:[155,   900] training loss: 0.00262883
INFO:root:[155,   950] training loss: 0.00103197
INFO:root:[155,  1000] training loss: 0.00002301
INFO:root:[155,  1050] training loss: 0.00001652
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch155
INFO:root:[156,    50] training loss: 0.00631151
INFO:root:[156,   100] training loss: 0.00650168
INFO:root:[156,   150] training loss: 0.00640406
INFO:root:[156,   200] training loss: 0.00617754
INFO:root:[156,   250] training loss: 0.00541858
INFO:root:[156,   300] training loss: 0.00651919
INFO:root:[156,   350] training loss: 0.00582232
INFO:root:[156,   400] training loss: 0.00000501
INFO:root:[156,   450] training loss: 0.00000679
INFO:root:[156,   500] training loss: 0.00001369
INFO:root:[156,   550] training loss: 0.00023331
INFO:root:[156,   600] training loss: 0.00009909
INFO:root:[156,   650] training loss: 0.00001123
INFO:root:[156,   700] training loss: 0.00001082
INFO:root:[156,   750] training loss: 0.00014653
INFO:root:[156,   800] training loss: 0.00013364
INFO:root:[156,   850] training loss: 0.00018060
INFO:root:[156,   900] training loss: 0.00257633
INFO:root:[156,   950] training loss: 0.00098551
INFO:root:[156,  1000] training loss: 0.00002091
INFO:root:[156,  1050] training loss: 0.00001812
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch156
INFO:root:[157,    50] training loss: 0.00670470
INFO:root:[157,   100] training loss: 0.00627877
INFO:root:[157,   150] training loss: 0.00674827
INFO:root:[157,   200] training loss: 0.00578898
INFO:root:[157,   250] training loss: 0.00549343
INFO:root:[157,   300] training loss: 0.00658438
INFO:root:[157,   350] training loss: 0.00564551
INFO:root:[157,   400] training loss: 0.00000671
INFO:root:[157,   450] training loss: 0.00000613
INFO:root:[157,   500] training loss: 0.00001158
INFO:root:[157,   550] training loss: 0.00023198
INFO:root:[157,   600] training loss: 0.00010072
INFO:root:[157,   650] training loss: 0.00001321
INFO:root:[157,   700] training loss: 0.00000905
INFO:root:[157,   750] training loss: 0.00013799
INFO:root:[157,   800] training loss: 0.00013886
INFO:root:[157,   850] training loss: 0.00015696
INFO:root:[157,   900] training loss: 0.00272167
INFO:root:[157,   950] training loss: 0.00081056
INFO:root:[157,  1000] training loss: 0.00002233
INFO:root:[157,  1050] training loss: 0.00001556
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch157
INFO:root:[158,    50] training loss: 0.00640450
INFO:root:[158,   100] training loss: 0.00649410
INFO:root:[158,   150] training loss: 0.00621869
INFO:root:[158,   200] training loss: 0.00601888
INFO:root:[158,   250] training loss: 0.00584803
INFO:root:[158,   300] training loss: 0.00662146
INFO:root:[158,   350] training loss: 0.00549821
INFO:root:[158,   400] training loss: 0.00000732
INFO:root:[158,   450] training loss: 0.00000505
INFO:root:[158,   500] training loss: 0.00001148
INFO:root:[158,   550] training loss: 0.00017972
INFO:root:[158,   600] training loss: 0.00012879
INFO:root:[158,   650] training loss: 0.00001470
INFO:root:[158,   700] training loss: 0.00000850
INFO:root:[158,   750] training loss: 0.00013913
INFO:root:[158,   800] training loss: 0.00022274
INFO:root:[158,   850] training loss: 0.00018869
INFO:root:[158,   900] training loss: 0.00311514
INFO:root:[158,   950] training loss: 0.00120569
INFO:root:[158,  1000] training loss: 0.00001913
INFO:root:[158,  1050] training loss: 0.00001508
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch158
INFO:root:[159,    50] training loss: 0.00650103
INFO:root:[159,   100] training loss: 0.00657613
INFO:root:[159,   150] training loss: 0.00624869
INFO:root:[159,   200] training loss: 0.00601007
INFO:root:[159,   250] training loss: 0.00528365
INFO:root:[159,   300] training loss: 0.00690417
INFO:root:[159,   350] training loss: 0.00510612
INFO:root:[159,   400] training loss: 0.00000668
INFO:root:[159,   450] training loss: 0.00000830
INFO:root:[159,   500] training loss: 0.00001379
INFO:root:[159,   550] training loss: 0.00021603
INFO:root:[159,   600] training loss: 0.00010483
INFO:root:[159,   650] training loss: 0.00001087
INFO:root:[159,   700] training loss: 0.00001399
INFO:root:[159,   750] training loss: 0.00010307
INFO:root:[159,   800] training loss: 0.00025580
INFO:root:[159,   850] training loss: 0.00012464
INFO:root:[159,   900] training loss: 0.00295049
INFO:root:[159,   950] training loss: 0.00081466
INFO:root:[159,  1000] training loss: 0.00002228
INFO:root:[159,  1050] training loss: 0.00002741
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch159
INFO:root:[160,    50] training loss: 0.00634610
INFO:root:[160,   100] training loss: 0.00632437
INFO:root:[160,   150] training loss: 0.00663866
INFO:root:[160,   200] training loss: 0.00607562
INFO:root:[160,   250] training loss: 0.00544101
INFO:root:[160,   300] training loss: 0.00667523
INFO:root:[160,   350] training loss: 0.00520954
INFO:root:[160,   400] training loss: 0.00000566
INFO:root:[160,   450] training loss: 0.00000834
INFO:root:[160,   500] training loss: 0.00001175
INFO:root:[160,   550] training loss: 0.00027039
INFO:root:[160,   600] training loss: 0.00010204
INFO:root:[160,   650] training loss: 0.00001071
INFO:root:[160,   700] training loss: 0.00000936
INFO:root:[160,   750] training loss: 0.00012753
INFO:root:[160,   800] training loss: 0.00022627
INFO:root:[160,   850] training loss: 0.00017227
INFO:root:[160,   900] training loss: 0.00233270
INFO:root:[160,   950] training loss: 0.00097509
INFO:root:[160,  1000] training loss: 0.00002300
INFO:root:[160,  1050] training loss: 0.00001992
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch160
INFO:root:[161,    50] training loss: 0.00684737
INFO:root:[161,   100] training loss: 0.00634154
INFO:root:[161,   150] training loss: 0.00698737
INFO:root:[161,   200] training loss: 0.00589077
INFO:root:[161,   250] training loss: 0.00543046
INFO:root:[161,   300] training loss: 0.00653299
INFO:root:[161,   350] training loss: 0.00528816
INFO:root:[161,   400] training loss: 0.00000759
INFO:root:[161,   450] training loss: 0.00001063
INFO:root:[161,   500] training loss: 0.00001052
INFO:root:[161,   550] training loss: 0.00018837
INFO:root:[161,   600] training loss: 0.00009753
INFO:root:[161,   650] training loss: 0.00001380
INFO:root:[161,   700] training loss: 0.00000898
INFO:root:[161,   750] training loss: 0.00011489
INFO:root:[161,   800] training loss: 0.00017555
INFO:root:[161,   850] training loss: 0.00015382
INFO:root:[161,   900] training loss: 0.00313470
INFO:root:[161,   950] training loss: 0.00092962
INFO:root:[161,  1000] training loss: 0.00002071
INFO:root:[161,  1050] training loss: 0.00001654
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch161
INFO:root:[162,    50] training loss: 0.00651916
INFO:root:[162,   100] training loss: 0.00639456
INFO:root:[162,   150] training loss: 0.00645848
INFO:root:[162,   200] training loss: 0.00645161
INFO:root:[162,   250] training loss: 0.00552259
INFO:root:[162,   300] training loss: 0.00715467
INFO:root:[162,   350] training loss: 0.00555723
INFO:root:[162,   400] training loss: 0.00000596
INFO:root:[162,   450] training loss: 0.00000548
INFO:root:[162,   500] training loss: 0.00001224
INFO:root:[162,   550] training loss: 0.00021774
INFO:root:[162,   600] training loss: 0.00009694
INFO:root:[162,   650] training loss: 0.00001244
INFO:root:[162,   700] training loss: 0.00001813
INFO:root:[162,   750] training loss: 0.00010945
INFO:root:[162,   800] training loss: 0.00017758
INFO:root:[162,   850] training loss: 0.00019029
INFO:root:[162,   900] training loss: 0.00365892
INFO:root:[162,   950] training loss: 0.00073690
INFO:root:[162,  1000] training loss: 0.00002090
INFO:root:[162,  1050] training loss: 0.00001532
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch162
INFO:root:[163,    50] training loss: 0.00807787
INFO:root:[163,   100] training loss: 0.00632356
INFO:root:[163,   150] training loss: 0.00633678
INFO:root:[163,   200] training loss: 0.00582798
INFO:root:[163,   250] training loss: 0.00582514
INFO:root:[163,   300] training loss: 0.00635713
INFO:root:[163,   350] training loss: 0.00539054
INFO:root:[163,   400] training loss: 0.00000490
INFO:root:[163,   450] training loss: 0.00000738
INFO:root:[163,   500] training loss: 0.00001230
INFO:root:[163,   550] training loss: 0.00017944
INFO:root:[163,   600] training loss: 0.00010089
INFO:root:[163,   650] training loss: 0.00001213
INFO:root:[163,   700] training loss: 0.00001093
INFO:root:[163,   750] training loss: 0.00012901
INFO:root:[163,   800] training loss: 0.00020292
INFO:root:[163,   850] training loss: 0.00016301
INFO:root:[163,   900] training loss: 0.00249336
INFO:root:[163,   950] training loss: 0.00098085
INFO:root:[163,  1000] training loss: 0.00002329
INFO:root:[163,  1050] training loss: 0.00014549
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch163
INFO:root:[164,    50] training loss: 0.00695198
INFO:root:[164,   100] training loss: 0.00608606
INFO:root:[164,   150] training loss: 0.00619357
INFO:root:[164,   200] training loss: 0.00611885
INFO:root:[164,   250] training loss: 0.00539539
INFO:root:[164,   300] training loss: 0.00732374
INFO:root:[164,   350] training loss: 0.00532874
INFO:root:[164,   400] training loss: 0.00000528
INFO:root:[164,   450] training loss: 0.00000540
INFO:root:[164,   500] training loss: 0.00001256
INFO:root:[164,   550] training loss: 0.00020422
INFO:root:[164,   600] training loss: 0.00009401
INFO:root:[164,   650] training loss: 0.00001372
INFO:root:[164,   700] training loss: 0.00001085
INFO:root:[164,   750] training loss: 0.00014913
INFO:root:[164,   800] training loss: 0.00019319
INFO:root:[164,   850] training loss: 0.00017822
INFO:root:[164,   900] training loss: 0.00291780
INFO:root:[164,   950] training loss: 0.00078025
INFO:root:[164,  1000] training loss: 0.00002197
INFO:root:[164,  1050] training loss: 0.00008359
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch164
INFO:root:[165,    50] training loss: 0.00676302
INFO:root:[165,   100] training loss: 0.00661014
INFO:root:[165,   150] training loss: 0.00673017
INFO:root:[165,   200] training loss: 0.00684680
INFO:root:[165,   250] training loss: 0.00606488
INFO:root:[165,   300] training loss: 0.00671530
INFO:root:[165,   350] training loss: 0.00596789
INFO:root:[165,   400] training loss: 0.00000914
INFO:root:[165,   450] training loss: 0.00000806
INFO:root:[165,   500] training loss: 0.00001517
INFO:root:[165,   550] training loss: 0.00016987
INFO:root:[165,   600] training loss: 0.00009109
INFO:root:[165,   650] training loss: 0.00001309
INFO:root:[165,   700] training loss: 0.00001732
INFO:root:[165,   750] training loss: 0.00011977
INFO:root:[165,   800] training loss: 0.00017285
INFO:root:[165,   850] training loss: 0.00017273
INFO:root:[165,   900] training loss: 0.00341314
INFO:root:[165,   950] training loss: 0.00127115
INFO:root:[165,  1000] training loss: 0.00002026
INFO:root:[165,  1050] training loss: 0.00001604
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch165
INFO:root:[166,    50] training loss: 0.00653819
INFO:root:[166,   100] training loss: 0.00646617
INFO:root:[166,   150] training loss: 0.00744416
INFO:root:[166,   200] training loss: 0.00612059
INFO:root:[166,   250] training loss: 0.00535249
INFO:root:[166,   300] training loss: 0.00741914
INFO:root:[166,   350] training loss: 0.00535656
INFO:root:[166,   400] training loss: 0.00000603
INFO:root:[166,   450] training loss: 0.00000531
INFO:root:[166,   500] training loss: 0.00001237
INFO:root:[166,   550] training loss: 0.00020698
INFO:root:[166,   600] training loss: 0.00011005
INFO:root:[166,   650] training loss: 0.00001129
INFO:root:[166,   700] training loss: 0.00001083
INFO:root:[166,   750] training loss: 0.00010828
INFO:root:[166,   800] training loss: 0.00019386
INFO:root:[166,   850] training loss: 0.00013465
INFO:root:[166,   900] training loss: 0.00287784
INFO:root:[166,   950] training loss: 0.00083595
INFO:root:[166,  1000] training loss: 0.00002208
INFO:root:[166,  1050] training loss: 0.00001886
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch166
INFO:root:[167,    50] training loss: 0.00672187
INFO:root:[167,   100] training loss: 0.00684703
INFO:root:[167,   150] training loss: 0.00605161
INFO:root:[167,   200] training loss: 0.00600487
INFO:root:[167,   250] training loss: 0.00524962
INFO:root:[167,   300] training loss: 0.00686951
INFO:root:[167,   350] training loss: 0.00553349
INFO:root:[167,   400] training loss: 0.00000757
INFO:root:[167,   450] training loss: 0.00000806
INFO:root:[167,   500] training loss: 0.00001191
INFO:root:[167,   550] training loss: 0.00018394
INFO:root:[167,   600] training loss: 0.00008683
INFO:root:[167,   650] training loss: 0.00001047
INFO:root:[167,   700] training loss: 0.00001108
INFO:root:[167,   750] training loss: 0.00010361
INFO:root:[167,   800] training loss: 0.00019766
INFO:root:[167,   850] training loss: 0.00021327
INFO:root:[167,   900] training loss: 0.00321025
INFO:root:[167,   950] training loss: 0.00074276
INFO:root:[167,  1000] training loss: 0.00001902
INFO:root:[167,  1050] training loss: 0.00001726
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch167
INFO:root:[168,    50] training loss: 0.00637639
INFO:root:[168,   100] training loss: 0.00679075
INFO:root:[168,   150] training loss: 0.00621215
INFO:root:[168,   200] training loss: 0.00589269
INFO:root:[168,   250] training loss: 0.00504836
INFO:root:[168,   300] training loss: 0.00665838
INFO:root:[168,   350] training loss: 0.00529381
INFO:root:[168,   400] training loss: 0.00000605
INFO:root:[168,   450] training loss: 0.00000574
INFO:root:[168,   500] training loss: 0.00002143
INFO:root:[168,   550] training loss: 0.00020124
INFO:root:[168,   600] training loss: 0.00008461
INFO:root:[168,   650] training loss: 0.00001380
INFO:root:[168,   700] training loss: 0.00001123
INFO:root:[168,   750] training loss: 0.00012151
INFO:root:[168,   800] training loss: 0.00023727
INFO:root:[168,   850] training loss: 0.00021241
INFO:root:[168,   900] training loss: 0.00267533
INFO:root:[168,   950] training loss: 0.00087064
INFO:root:[168,  1000] training loss: 0.00002143
INFO:root:[168,  1050] training loss: 0.00001574
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch168
INFO:root:[169,    50] training loss: 0.00887178
INFO:root:[169,   100] training loss: 0.00694650
INFO:root:[169,   150] training loss: 0.00615049
INFO:root:[169,   200] training loss: 0.00602130
INFO:root:[169,   250] training loss: 0.00558471
INFO:root:[169,   300] training loss: 0.00695273
INFO:root:[169,   350] training loss: 0.00563710
INFO:root:[169,   400] training loss: 0.00000559
INFO:root:[169,   450] training loss: 0.00003495
INFO:root:[169,   500] training loss: 0.00001158
INFO:root:[169,   550] training loss: 0.00023947
INFO:root:[169,   600] training loss: 0.00013385
INFO:root:[169,   650] training loss: 0.00001344
INFO:root:[169,   700] training loss: 0.00001096
INFO:root:[169,   750] training loss: 0.00012851
INFO:root:[169,   800] training loss: 0.00016085
INFO:root:[169,   850] training loss: 0.00017005
INFO:root:[169,   900] training loss: 0.00526332
INFO:root:[169,   950] training loss: 0.00099619
INFO:root:[169,  1000] training loss: 0.00002112
INFO:root:[169,  1050] training loss: 0.00001626
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch169
INFO:root:[170,    50] training loss: 0.00693440
INFO:root:[170,   100] training loss: 0.00612082
INFO:root:[170,   150] training loss: 0.00637551
INFO:root:[170,   200] training loss: 0.00602983
INFO:root:[170,   250] training loss: 0.00527211
INFO:root:[170,   300] training loss: 0.00704465
INFO:root:[170,   350] training loss: 0.00524622
INFO:root:[170,   400] training loss: 0.00001115
INFO:root:[170,   450] training loss: 0.00000721
INFO:root:[170,   500] training loss: 0.00001314
INFO:root:[170,   550] training loss: 0.00018819
INFO:root:[170,   600] training loss: 0.00010938
INFO:root:[170,   650] training loss: 0.00001761
INFO:root:[170,   700] training loss: 0.00002163
INFO:root:[170,   750] training loss: 0.00010459
INFO:root:[170,   800] training loss: 0.00020125
INFO:root:[170,   850] training loss: 0.00019935
INFO:root:[170,   900] training loss: 0.00277983
INFO:root:[170,   950] training loss: 0.00086304
INFO:root:[170,  1000] training loss: 0.00002696
INFO:root:[170,  1050] training loss: 0.00001620
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch170
INFO:root:[171,    50] training loss: 0.00660544
INFO:root:[171,   100] training loss: 0.00652300
INFO:root:[171,   150] training loss: 0.00605970
INFO:root:[171,   200] training loss: 0.00587084
INFO:root:[171,   250] training loss: 0.00591314
INFO:root:[171,   300] training loss: 0.00666797
INFO:root:[171,   350] training loss: 0.00509815
INFO:root:[171,   400] training loss: 0.00000623
INFO:root:[171,   450] training loss: 0.00000584
INFO:root:[171,   500] training loss: 0.00001344
INFO:root:[171,   550] training loss: 0.00018780
INFO:root:[171,   600] training loss: 0.00012417
INFO:root:[171,   650] training loss: 0.00002081
INFO:root:[171,   700] training loss: 0.00000834
INFO:root:[171,   750] training loss: 0.00011698
INFO:root:[171,   800] training loss: 0.00015383
INFO:root:[171,   850] training loss: 0.00019173
INFO:root:[171,   900] training loss: 0.00252613
INFO:root:[171,   950] training loss: 0.00064460
INFO:root:[171,  1000] training loss: 0.00001955
INFO:root:[171,  1050] training loss: 0.00001552
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch171
INFO:root:[172,    50] training loss: 0.00670895
INFO:root:[172,   100] training loss: 0.00735390
INFO:root:[172,   150] training loss: 0.00664991
INFO:root:[172,   200] training loss: 0.00623342
INFO:root:[172,   250] training loss: 0.00528591
INFO:root:[172,   300] training loss: 0.00645245
INFO:root:[172,   350] training loss: 0.00552850
INFO:root:[172,   400] training loss: 0.00000513
INFO:root:[172,   450] training loss: 0.00000593
INFO:root:[172,   500] training loss: 0.00001406
INFO:root:[172,   550] training loss: 0.00018906
INFO:root:[172,   600] training loss: 0.00009372
INFO:root:[172,   650] training loss: 0.00001404
INFO:root:[172,   700] training loss: 0.00001007
INFO:root:[172,   750] training loss: 0.00016241
INFO:root:[172,   800] training loss: 0.00017391
INFO:root:[172,   850] training loss: 0.00016401
INFO:root:[172,   900] training loss: 0.00357256
INFO:root:[172,   950] training loss: 0.00109029
INFO:root:[172,  1000] training loss: 0.00001998
INFO:root:[172,  1050] training loss: 0.00001652
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6298    0.6935    0.6601      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7409    0.8378    0.7824      3872
weighted avg     0.8052    0.7918    0.7961      3872

INFO:root:epoch172
INFO:root:[173,    50] training loss: 0.00664710
INFO:root:[173,   100] training loss: 0.00667162
INFO:root:[173,   150] training loss: 0.00786003
INFO:root:[173,   200] training loss: 0.00615994
INFO:root:[173,   250] training loss: 0.00720762
INFO:root:[173,   300] training loss: 0.00662719
INFO:root:[173,   350] training loss: 0.00556716
INFO:root:[173,   400] training loss: 0.00000723
INFO:root:[173,   450] training loss: 0.00001016
INFO:root:[173,   500] training loss: 0.00001161
INFO:root:[173,   550] training loss: 0.00025084
INFO:root:[173,   600] training loss: 0.00009045
INFO:root:[173,   650] training loss: 0.00001704
INFO:root:[173,   700] training loss: 0.00001528
INFO:root:[173,   750] training loss: 0.00011977
INFO:root:[173,   800] training loss: 0.00015705
INFO:root:[173,   850] training loss: 0.00017953
INFO:root:[173,   900] training loss: 0.00280129
INFO:root:[173,   950] training loss: 0.00079918
INFO:root:[173,  1000] training loss: 0.00002041
INFO:root:[173,  1050] training loss: 0.00001529
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8034    0.8537    0.8278      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7921      3872
   macro avg     0.7410    0.8380    0.7826      3872
weighted avg     0.8054    0.7921    0.7963      3872

INFO:root:epoch173
INFO:root:[174,    50] training loss: 0.00913383
INFO:root:[174,   100] training loss: 0.00642167
INFO:root:[174,   150] training loss: 0.00711235
INFO:root:[174,   200] training loss: 0.00587885
INFO:root:[174,   250] training loss: 0.00540127
INFO:root:[174,   300] training loss: 0.00708664
INFO:root:[174,   350] training loss: 0.00536638
INFO:root:[174,   400] training loss: 0.00000670
INFO:root:[174,   450] training loss: 0.00000498
INFO:root:[174,   500] training loss: 0.00001756
INFO:root:[174,   550] training loss: 0.00021141
INFO:root:[174,   600] training loss: 0.00009667
INFO:root:[174,   650] training loss: 0.00001285
INFO:root:[174,   700] training loss: 0.00000844
INFO:root:[174,   750] training loss: 0.00012590
INFO:root:[174,   800] training loss: 0.00015855
INFO:root:[174,   850] training loss: 0.00020500
INFO:root:[174,   900] training loss: 0.00265620
INFO:root:[174,   950] training loss: 0.00091611
INFO:root:[174,  1000] training loss: 0.00002322
INFO:root:[174,  1050] training loss: 0.00001882
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8034    0.8537    0.8278      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4952    0.7027    0.5810        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7921      3872
   macro avg     0.7410    0.8380    0.7826      3872
weighted avg     0.8054    0.7921    0.7963      3872

INFO:root:epoch174
INFO:root:[175,    50] training loss: 0.00652110
INFO:root:[175,   100] training loss: 0.00786703
INFO:root:[175,   150] training loss: 0.00641949
INFO:root:[175,   200] training loss: 0.00646623
INFO:root:[175,   250] training loss: 0.00523157
INFO:root:[175,   300] training loss: 0.00674477
INFO:root:[175,   350] training loss: 0.00528866
INFO:root:[175,   400] training loss: 0.00000653
INFO:root:[175,   450] training loss: 0.00000611
INFO:root:[175,   500] training loss: 0.00001269
INFO:root:[175,   550] training loss: 0.00018453
INFO:root:[175,   600] training loss: 0.00011561
INFO:root:[175,   650] training loss: 0.00001323
INFO:root:[175,   700] training loss: 0.00001002
INFO:root:[175,   750] training loss: 0.00012070
INFO:root:[175,   800] training loss: 0.00012340
INFO:root:[175,   850] training loss: 0.00022847
INFO:root:[175,   900] training loss: 0.00263138
INFO:root:[175,   950] training loss: 0.00083591
INFO:root:[175,  1000] training loss: 0.00002002
INFO:root:[175,  1050] training loss: 0.00001645
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch175
INFO:root:[176,    50] training loss: 0.00633418
INFO:root:[176,   100] training loss: 0.00673030
INFO:root:[176,   150] training loss: 0.00671600
INFO:root:[176,   200] training loss: 0.00598635
INFO:root:[176,   250] training loss: 0.00533432
INFO:root:[176,   300] training loss: 0.00685174
INFO:root:[176,   350] training loss: 0.00558876
INFO:root:[176,   400] training loss: 0.00000909
INFO:root:[176,   450] training loss: 0.00000621
INFO:root:[176,   500] training loss: 0.00001268
INFO:root:[176,   550] training loss: 0.00018361
INFO:root:[176,   600] training loss: 0.00008640
INFO:root:[176,   650] training loss: 0.00001186
INFO:root:[176,   700] training loss: 0.00000814
INFO:root:[176,   750] training loss: 0.00010275
INFO:root:[176,   800] training loss: 0.00016548
INFO:root:[176,   850] training loss: 0.00017894
INFO:root:[176,   900] training loss: 0.00439477
INFO:root:[176,   950] training loss: 0.00113847
INFO:root:[176,  1000] training loss: 0.00002241
INFO:root:[176,  1050] training loss: 0.00001590
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch176
INFO:root:[177,    50] training loss: 0.00643462
INFO:root:[177,   100] training loss: 0.00659809
INFO:root:[177,   150] training loss: 0.00610504
INFO:root:[177,   200] training loss: 0.00603024
INFO:root:[177,   250] training loss: 0.00581252
INFO:root:[177,   300] training loss: 0.00728204
INFO:root:[177,   350] training loss: 0.00571031
INFO:root:[177,   400] training loss: 0.00000828
INFO:root:[177,   450] training loss: 0.00000695
INFO:root:[177,   500] training loss: 0.00001090
INFO:root:[177,   550] training loss: 0.00019311
INFO:root:[177,   600] training loss: 0.00008655
INFO:root:[177,   650] training loss: 0.00001539
INFO:root:[177,   700] training loss: 0.00001180
INFO:root:[177,   750] training loss: 0.00012751
INFO:root:[177,   800] training loss: 0.00026425
INFO:root:[177,   850] training loss: 0.00017724
INFO:root:[177,   900] training loss: 0.00283992
INFO:root:[177,   950] training loss: 0.00059708
INFO:root:[177,  1000] training loss: 0.00002334
INFO:root:[177,  1050] training loss: 0.00001799
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch177
INFO:root:[178,    50] training loss: 0.00637233
INFO:root:[178,   100] training loss: 0.00622796
INFO:root:[178,   150] training loss: 0.00618201
INFO:root:[178,   200] training loss: 0.00580308
INFO:root:[178,   250] training loss: 0.00503341
INFO:root:[178,   300] training loss: 0.00650425
INFO:root:[178,   350] training loss: 0.00574860
INFO:root:[178,   400] training loss: 0.00000682
INFO:root:[178,   450] training loss: 0.00002204
INFO:root:[178,   500] training loss: 0.00001164
INFO:root:[178,   550] training loss: 0.00017830
INFO:root:[178,   600] training loss: 0.00009273
INFO:root:[178,   650] training loss: 0.00001409
INFO:root:[178,   700] training loss: 0.00001497
INFO:root:[178,   750] training loss: 0.00014938
INFO:root:[178,   800] training loss: 0.00018986
INFO:root:[178,   850] training loss: 0.00016969
INFO:root:[178,   900] training loss: 0.00319394
INFO:root:[178,   950] training loss: 0.00111392
INFO:root:[178,  1000] training loss: 0.00002132
INFO:root:[178,  1050] training loss: 0.00001569
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch178
INFO:root:[179,    50] training loss: 0.00646558
INFO:root:[179,   100] training loss: 0.00673895
INFO:root:[179,   150] training loss: 0.00771972
INFO:root:[179,   200] training loss: 0.00590561
INFO:root:[179,   250] training loss: 0.00627957
INFO:root:[179,   300] training loss: 0.00668380
INFO:root:[179,   350] training loss: 0.00537592
INFO:root:[179,   400] training loss: 0.00000586
INFO:root:[179,   450] training loss: 0.00000711
INFO:root:[179,   500] training loss: 0.00002107
INFO:root:[179,   550] training loss: 0.00022585
INFO:root:[179,   600] training loss: 0.00009719
INFO:root:[179,   650] training loss: 0.00001242
INFO:root:[179,   700] training loss: 0.00001003
INFO:root:[179,   750] training loss: 0.00009180
INFO:root:[179,   800] training loss: 0.00014471
INFO:root:[179,   850] training loss: 0.00016802
INFO:root:[179,   900] training loss: 0.00261261
INFO:root:[179,   950] training loss: 0.00140010
INFO:root:[179,  1000] training loss: 0.00002229
INFO:root:[179,  1050] training loss: 0.00001576
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch179
INFO:root:[180,    50] training loss: 0.00650542
INFO:root:[180,   100] training loss: 0.00601415
INFO:root:[180,   150] training loss: 0.00624233
INFO:root:[180,   200] training loss: 0.00597589
INFO:root:[180,   250] training loss: 0.00543967
INFO:root:[180,   300] training loss: 0.00655636
INFO:root:[180,   350] training loss: 0.00503901
INFO:root:[180,   400] training loss: 0.00000869
INFO:root:[180,   450] training loss: 0.00000620
INFO:root:[180,   500] training loss: 0.00001463
INFO:root:[180,   550] training loss: 0.00018785
INFO:root:[180,   600] training loss: 0.00010293
INFO:root:[180,   650] training loss: 0.00001286
INFO:root:[180,   700] training loss: 0.00001091
INFO:root:[180,   750] training loss: 0.00008950
INFO:root:[180,   800] training loss: 0.00013390
INFO:root:[180,   850] training loss: 0.00018981
INFO:root:[180,   900] training loss: 0.00391764
INFO:root:[180,   950] training loss: 0.00108925
INFO:root:[180,  1000] training loss: 0.00002002
INFO:root:[180,  1050] training loss: 0.00001502
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch180
INFO:root:[181,    50] training loss: 0.00683815
INFO:root:[181,   100] training loss: 0.00654450
INFO:root:[181,   150] training loss: 0.00676988
INFO:root:[181,   200] training loss: 0.00618502
INFO:root:[181,   250] training loss: 0.00528588
INFO:root:[181,   300] training loss: 0.00672408
INFO:root:[181,   350] training loss: 0.00522767
INFO:root:[181,   400] training loss: 0.00000911
INFO:root:[181,   450] training loss: 0.00000765
INFO:root:[181,   500] training loss: 0.00001178
INFO:root:[181,   550] training loss: 0.00018461
INFO:root:[181,   600] training loss: 0.00011403
INFO:root:[181,   650] training loss: 0.00000989
INFO:root:[181,   700] training loss: 0.00001421
INFO:root:[181,   750] training loss: 0.00014436
INFO:root:[181,   800] training loss: 0.00017839
INFO:root:[181,   850] training loss: 0.00015865
INFO:root:[181,   900] training loss: 0.00276683
INFO:root:[181,   950] training loss: 0.00086711
INFO:root:[181,  1000] training loss: 0.00001945
INFO:root:[181,  1050] training loss: 0.00001707
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch181
INFO:root:[182,    50] training loss: 0.00735774
INFO:root:[182,   100] training loss: 0.00618837
INFO:root:[182,   150] training loss: 0.00680474
INFO:root:[182,   200] training loss: 0.00598757
INFO:root:[182,   250] training loss: 0.00519001
INFO:root:[182,   300] training loss: 0.00673541
INFO:root:[182,   350] training loss: 0.00586966
INFO:root:[182,   400] training loss: 0.00005498
INFO:root:[182,   450] training loss: 0.00000731
INFO:root:[182,   500] training loss: 0.00001399
INFO:root:[182,   550] training loss: 0.00016353
INFO:root:[182,   600] training loss: 0.00012201
INFO:root:[182,   650] training loss: 0.00001279
INFO:root:[182,   700] training loss: 0.00001023
INFO:root:[182,   750] training loss: 0.00017085
INFO:root:[182,   800] training loss: 0.00024027
INFO:root:[182,   850] training loss: 0.00021348
INFO:root:[182,   900] training loss: 0.00290537
INFO:root:[182,   950] training loss: 0.00071815
INFO:root:[182,  1000] training loss: 0.00001971
INFO:root:[182,  1050] training loss: 0.00002183
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch182
INFO:root:[183,    50] training loss: 0.00644681
INFO:root:[183,   100] training loss: 0.00621771
INFO:root:[183,   150] training loss: 0.00681000
INFO:root:[183,   200] training loss: 0.00860268
INFO:root:[183,   250] training loss: 0.00515754
INFO:root:[183,   300] training loss: 0.00686840
INFO:root:[183,   350] training loss: 0.00538753
INFO:root:[183,   400] training loss: 0.00000673
INFO:root:[183,   450] training loss: 0.00000605
INFO:root:[183,   500] training loss: 0.00001257
INFO:root:[183,   550] training loss: 0.00022030
INFO:root:[183,   600] training loss: 0.00008037
INFO:root:[183,   650] training loss: 0.00001143
INFO:root:[183,   700] training loss: 0.00001011
INFO:root:[183,   750] training loss: 0.00012172
INFO:root:[183,   800] training loss: 0.00014766
INFO:root:[183,   850] training loss: 0.00021915
INFO:root:[183,   900] training loss: 0.00294394
INFO:root:[183,   950] training loss: 0.00071786
INFO:root:[183,  1000] training loss: 0.00001975
INFO:root:[183,  1050] training loss: 0.00001612
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch183
INFO:root:[184,    50] training loss: 0.00643566
INFO:root:[184,   100] training loss: 0.00608955
INFO:root:[184,   150] training loss: 0.00629558
INFO:root:[184,   200] training loss: 0.00609455
INFO:root:[184,   250] training loss: 0.00614752
INFO:root:[184,   300] training loss: 0.00735055
INFO:root:[184,   350] training loss: 0.00528944
INFO:root:[184,   400] training loss: 0.00000473
INFO:root:[184,   450] training loss: 0.00000632
INFO:root:[184,   500] training loss: 0.00001510
INFO:root:[184,   550] training loss: 0.00022051
INFO:root:[184,   600] training loss: 0.00010803
INFO:root:[184,   650] training loss: 0.00001195
INFO:root:[184,   700] training loss: 0.00001042
INFO:root:[184,   750] training loss: 0.00021111
INFO:root:[184,   800] training loss: 0.00020610
INFO:root:[184,   850] training loss: 0.00016543
INFO:root:[184,   900] training loss: 0.00278335
INFO:root:[184,   950] training loss: 0.00078730
INFO:root:[184,  1000] training loss: 0.00001995
INFO:root:[184,  1050] training loss: 0.00002123
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch184
INFO:root:[185,    50] training loss: 0.00743012
INFO:root:[185,   100] training loss: 0.00622875
INFO:root:[185,   150] training loss: 0.01005650
INFO:root:[185,   200] training loss: 0.00608400
INFO:root:[185,   250] training loss: 0.00535470
INFO:root:[185,   300] training loss: 0.00664342
INFO:root:[185,   350] training loss: 0.00561258
INFO:root:[185,   400] training loss: 0.00000680
INFO:root:[185,   450] training loss: 0.00000663
INFO:root:[185,   500] training loss: 0.00002210
INFO:root:[185,   550] training loss: 0.00020003
INFO:root:[185,   600] training loss: 0.00010826
INFO:root:[185,   650] training loss: 0.00001263
INFO:root:[185,   700] training loss: 0.00001350
INFO:root:[185,   750] training loss: 0.00012842
INFO:root:[185,   800] training loss: 0.00017345
INFO:root:[185,   850] training loss: 0.00018212
INFO:root:[185,   900] training loss: 0.00307776
INFO:root:[185,   950] training loss: 0.00102626
INFO:root:[185,  1000] training loss: 0.00002206
INFO:root:[185,  1050] training loss: 0.00001630
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch185
INFO:root:[186,    50] training loss: 0.00684763
INFO:root:[186,   100] training loss: 0.00608345
INFO:root:[186,   150] training loss: 0.00682967
INFO:root:[186,   200] training loss: 0.00820733
INFO:root:[186,   250] training loss: 0.00628091
INFO:root:[186,   300] training loss: 0.00706322
INFO:root:[186,   350] training loss: 0.00492296
INFO:root:[186,   400] training loss: 0.00000691
INFO:root:[186,   450] training loss: 0.00000661
INFO:root:[186,   500] training loss: 0.00001125
INFO:root:[186,   550] training loss: 0.00021993
INFO:root:[186,   600] training loss: 0.00008099
INFO:root:[186,   650] training loss: 0.00001089
INFO:root:[186,   700] training loss: 0.00001169
INFO:root:[186,   750] training loss: 0.00012111
INFO:root:[186,   800] training loss: 0.00015279
INFO:root:[186,   850] training loss: 0.00018891
INFO:root:[186,   900] training loss: 0.00294489
INFO:root:[186,   950] training loss: 0.00084270
INFO:root:[186,  1000] training loss: 0.00002297
INFO:root:[186,  1050] training loss: 0.00001887
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch186
INFO:root:[187,    50] training loss: 0.00654756
INFO:root:[187,   100] training loss: 0.00617815
INFO:root:[187,   150] training loss: 0.00641495
INFO:root:[187,   200] training loss: 0.00592331
INFO:root:[187,   250] training loss: 0.00561682
INFO:root:[187,   300] training loss: 0.00702200
INFO:root:[187,   350] training loss: 0.00559577
INFO:root:[187,   400] training loss: 0.00000582
INFO:root:[187,   450] training loss: 0.00000930
INFO:root:[187,   500] training loss: 0.00001164
INFO:root:[187,   550] training loss: 0.00015369
INFO:root:[187,   600] training loss: 0.00009436
INFO:root:[187,   650] training loss: 0.00001546
INFO:root:[187,   700] training loss: 0.00000927
INFO:root:[187,   750] training loss: 0.00009116
INFO:root:[187,   800] training loss: 0.00016160
INFO:root:[187,   850] training loss: 0.00017265
INFO:root:[187,   900] training loss: 0.00238206
INFO:root:[187,   950] training loss: 0.00075554
INFO:root:[187,  1000] training loss: 0.00002322
INFO:root:[187,  1050] training loss: 0.00002291
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch187
INFO:root:[188,    50] training loss: 0.00672327
INFO:root:[188,   100] training loss: 0.00649604
INFO:root:[188,   150] training loss: 0.00630496
INFO:root:[188,   200] training loss: 0.00670897
INFO:root:[188,   250] training loss: 0.00526292
INFO:root:[188,   300] training loss: 0.00692636
INFO:root:[188,   350] training loss: 0.00541924
INFO:root:[188,   400] training loss: 0.00000586
INFO:root:[188,   450] training loss: 0.00000596
INFO:root:[188,   500] training loss: 0.00001111
INFO:root:[188,   550] training loss: 0.00021364
INFO:root:[188,   600] training loss: 0.00012724
INFO:root:[188,   650] training loss: 0.00001935
INFO:root:[188,   700] training loss: 0.00000859
INFO:root:[188,   750] training loss: 0.00012670
INFO:root:[188,   800] training loss: 0.00018924
INFO:root:[188,   850] training loss: 0.00011779
INFO:root:[188,   900] training loss: 0.00309230
INFO:root:[188,   950] training loss: 0.00123495
INFO:root:[188,  1000] training loss: 0.00002006
INFO:root:[188,  1050] training loss: 0.00001548
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch188
INFO:root:[189,    50] training loss: 0.00737542
INFO:root:[189,   100] training loss: 0.00625184
INFO:root:[189,   150] training loss: 0.00634809
INFO:root:[189,   200] training loss: 0.00625530
INFO:root:[189,   250] training loss: 0.00529583
INFO:root:[189,   300] training loss: 0.00691356
INFO:root:[189,   350] training loss: 0.00550009
INFO:root:[189,   400] training loss: 0.00000574
INFO:root:[189,   450] training loss: 0.00000541
INFO:root:[189,   500] training loss: 0.00001284
INFO:root:[189,   550] training loss: 0.00019088
INFO:root:[189,   600] training loss: 0.00009281
INFO:root:[189,   650] training loss: 0.00001952
INFO:root:[189,   700] training loss: 0.00000860
INFO:root:[189,   750] training loss: 0.00016046
INFO:root:[189,   800] training loss: 0.00013120
INFO:root:[189,   850] training loss: 0.00017844
INFO:root:[189,   900] training loss: 0.00255083
INFO:root:[189,   950] training loss: 0.00074667
INFO:root:[189,  1000] training loss: 0.00001951
INFO:root:[189,  1050] training loss: 0.00001800
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch189
INFO:root:[190,    50] training loss: 0.00638042
INFO:root:[190,   100] training loss: 0.00684225
INFO:root:[190,   150] training loss: 0.00693313
INFO:root:[190,   200] training loss: 0.00636030
INFO:root:[190,   250] training loss: 0.00533485
INFO:root:[190,   300] training loss: 0.00659390
INFO:root:[190,   350] training loss: 0.00539913
INFO:root:[190,   400] training loss: 0.00001478
INFO:root:[190,   450] training loss: 0.00000643
INFO:root:[190,   500] training loss: 0.00001315
INFO:root:[190,   550] training loss: 0.00024306
INFO:root:[190,   600] training loss: 0.00011350
INFO:root:[190,   650] training loss: 0.00001156
INFO:root:[190,   700] training loss: 0.00001029
INFO:root:[190,   750] training loss: 0.00009639
INFO:root:[190,   800] training loss: 0.00015589
INFO:root:[190,   850] training loss: 0.00017237
INFO:root:[190,   900] training loss: 0.00272122
INFO:root:[190,   950] training loss: 0.00079742
INFO:root:[190,  1000] training loss: 0.00002123
INFO:root:[190,  1050] training loss: 0.00001760
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch190
INFO:root:[191,    50] training loss: 0.00717988
INFO:root:[191,   100] training loss: 0.00624214
INFO:root:[191,   150] training loss: 0.00622414
INFO:root:[191,   200] training loss: 0.00595985
INFO:root:[191,   250] training loss: 0.00558787
INFO:root:[191,   300] training loss: 0.00654465
INFO:root:[191,   350] training loss: 0.00558749
INFO:root:[191,   400] training loss: 0.00000600
INFO:root:[191,   450] training loss: 0.00000729
INFO:root:[191,   500] training loss: 0.00001369
INFO:root:[191,   550] training loss: 0.00021828
INFO:root:[191,   600] training loss: 0.00009072
INFO:root:[191,   650] training loss: 0.00001833
INFO:root:[191,   700] training loss: 0.00000956
INFO:root:[191,   750] training loss: 0.00012511
INFO:root:[191,   800] training loss: 0.00020799
INFO:root:[191,   850] training loss: 0.00029109
INFO:root:[191,   900] training loss: 0.00252276
INFO:root:[191,   950] training loss: 0.00093978
INFO:root:[191,  1000] training loss: 0.00001913
INFO:root:[191,  1050] training loss: 0.00002685
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch191
INFO:root:[192,    50] training loss: 0.00622757
INFO:root:[192,   100] training loss: 0.00649437
INFO:root:[192,   150] training loss: 0.00636268
INFO:root:[192,   200] training loss: 0.00587540
INFO:root:[192,   250] training loss: 0.00549739
INFO:root:[192,   300] training loss: 0.00728342
INFO:root:[192,   350] training loss: 0.00590961
INFO:root:[192,   400] training loss: 0.00000669
INFO:root:[192,   450] training loss: 0.00002820
INFO:root:[192,   500] training loss: 0.00001360
INFO:root:[192,   550] training loss: 0.00020030
INFO:root:[192,   600] training loss: 0.00010126
INFO:root:[192,   650] training loss: 0.00001071
INFO:root:[192,   700] training loss: 0.00001289
INFO:root:[192,   750] training loss: 0.00009935
INFO:root:[192,   800] training loss: 0.00018866
INFO:root:[192,   850] training loss: 0.00018086
INFO:root:[192,   900] training loss: 0.00286140
INFO:root:[192,   950] training loss: 0.00099667
INFO:root:[192,  1000] training loss: 0.00002037
INFO:root:[192,  1050] training loss: 0.00003141
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch192
INFO:root:[193,    50] training loss: 0.00651663
INFO:root:[193,   100] training loss: 0.00662025
INFO:root:[193,   150] training loss: 0.00609074
INFO:root:[193,   200] training loss: 0.00659112
INFO:root:[193,   250] training loss: 0.00689603
INFO:root:[193,   300] training loss: 0.00655641
INFO:root:[193,   350] training loss: 0.00606494
INFO:root:[193,   400] training loss: 0.00000571
INFO:root:[193,   450] training loss: 0.00000812
INFO:root:[193,   500] training loss: 0.00001292
INFO:root:[193,   550] training loss: 0.00018894
INFO:root:[193,   600] training loss: 0.00009981
INFO:root:[193,   650] training loss: 0.00001191
INFO:root:[193,   700] training loss: 0.00001013
INFO:root:[193,   750] training loss: 0.00017099
INFO:root:[193,   800] training loss: 0.00019143
INFO:root:[193,   850] training loss: 0.00019452
INFO:root:[193,   900] training loss: 0.00260579
INFO:root:[193,   950] training loss: 0.00081170
INFO:root:[193,  1000] training loss: 0.00002021
INFO:root:[193,  1050] training loss: 0.00001838
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch193
INFO:root:[194,    50] training loss: 0.00634349
INFO:root:[194,   100] training loss: 0.00640779
INFO:root:[194,   150] training loss: 0.00642508
INFO:root:[194,   200] training loss: 0.00600163
INFO:root:[194,   250] training loss: 0.00551915
INFO:root:[194,   300] training loss: 0.00715708
INFO:root:[194,   350] training loss: 0.00512353
INFO:root:[194,   400] training loss: 0.00000762
INFO:root:[194,   450] training loss: 0.00000590
INFO:root:[194,   500] training loss: 0.00001501
INFO:root:[194,   550] training loss: 0.00021126
INFO:root:[194,   600] training loss: 0.00011528
INFO:root:[194,   650] training loss: 0.00001419
INFO:root:[194,   700] training loss: 0.00001081
INFO:root:[194,   750] training loss: 0.00012110
INFO:root:[194,   800] training loss: 0.00014546
INFO:root:[194,   850] training loss: 0.00012975
INFO:root:[194,   900] training loss: 0.00297566
INFO:root:[194,   950] training loss: 0.00086930
INFO:root:[194,  1000] training loss: 0.00002039
INFO:root:[194,  1050] training loss: 0.00001923
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch194
INFO:root:[195,    50] training loss: 0.00655717
INFO:root:[195,   100] training loss: 0.00635332
INFO:root:[195,   150] training loss: 0.00625733
INFO:root:[195,   200] training loss: 0.00597182
INFO:root:[195,   250] training loss: 0.00546298
INFO:root:[195,   300] training loss: 0.00716123
INFO:root:[195,   350] training loss: 0.00557523
INFO:root:[195,   400] training loss: 0.00000582
INFO:root:[195,   450] training loss: 0.00000702
INFO:root:[195,   500] training loss: 0.00002256
INFO:root:[195,   550] training loss: 0.00019172
INFO:root:[195,   600] training loss: 0.00010648
INFO:root:[195,   650] training loss: 0.00001170
INFO:root:[195,   700] training loss: 0.00000948
INFO:root:[195,   750] training loss: 0.00011904
INFO:root:[195,   800] training loss: 0.00018705
INFO:root:[195,   850] training loss: 0.00015568
INFO:root:[195,   900] training loss: 0.00313851
INFO:root:[195,   950] training loss: 0.00080486
INFO:root:[195,  1000] training loss: 0.00001969
INFO:root:[195,  1050] training loss: 0.00001669
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch195
INFO:root:[196,    50] training loss: 0.00686194
INFO:root:[196,   100] training loss: 0.00634670
INFO:root:[196,   150] training loss: 0.00627971
INFO:root:[196,   200] training loss: 0.00600131
INFO:root:[196,   250] training loss: 0.00524899
INFO:root:[196,   300] training loss: 0.00659297
INFO:root:[196,   350] training loss: 0.00560305
INFO:root:[196,   400] training loss: 0.00004313
INFO:root:[196,   450] training loss: 0.00000613
INFO:root:[196,   500] training loss: 0.00001134
INFO:root:[196,   550] training loss: 0.00022860
INFO:root:[196,   600] training loss: 0.00009715
INFO:root:[196,   650] training loss: 0.00001098
INFO:root:[196,   700] training loss: 0.00001016
INFO:root:[196,   750] training loss: 0.00016510
INFO:root:[196,   800] training loss: 0.00025908
INFO:root:[196,   850] training loss: 0.00018352
INFO:root:[196,   900] training loss: 0.00262666
INFO:root:[196,   950] training loss: 0.00069681
INFO:root:[196,  1000] training loss: 0.00002270
INFO:root:[196,  1050] training loss: 0.00001855
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch196
INFO:root:[197,    50] training loss: 0.00629195
INFO:root:[197,   100] training loss: 0.00647971
INFO:root:[197,   150] training loss: 0.00645648
INFO:root:[197,   200] training loss: 0.00597280
INFO:root:[197,   250] training loss: 0.00524504
INFO:root:[197,   300] training loss: 0.00660737
INFO:root:[197,   350] training loss: 0.00598562
INFO:root:[197,   400] training loss: 0.00000975
INFO:root:[197,   450] training loss: 0.00000593
INFO:root:[197,   500] training loss: 0.00001187
INFO:root:[197,   550] training loss: 0.00022236
INFO:root:[197,   600] training loss: 0.00011559
INFO:root:[197,   650] training loss: 0.00001312
INFO:root:[197,   700] training loss: 0.00001286
INFO:root:[197,   750] training loss: 0.00011229
INFO:root:[197,   800] training loss: 0.00018766
INFO:root:[197,   850] training loss: 0.00018799
INFO:root:[197,   900] training loss: 0.00285557
INFO:root:[197,   950] training loss: 0.00095868
INFO:root:[197,  1000] training loss: 0.00002220
INFO:root:[197,  1050] training loss: 0.00001734
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch197
INFO:root:[198,    50] training loss: 0.00622988
INFO:root:[198,   100] training loss: 0.00581031
INFO:root:[198,   150] training loss: 0.00623721
INFO:root:[198,   200] training loss: 0.00625921
INFO:root:[198,   250] training loss: 0.00521535
INFO:root:[198,   300] training loss: 0.00668806
INFO:root:[198,   350] training loss: 0.00544516
INFO:root:[198,   400] training loss: 0.00005909
INFO:root:[198,   450] training loss: 0.00000673
INFO:root:[198,   500] training loss: 0.00001336
INFO:root:[198,   550] training loss: 0.00016389
INFO:root:[198,   600] training loss: 0.00011447
INFO:root:[198,   650] training loss: 0.00001154
INFO:root:[198,   700] training loss: 0.00000986
INFO:root:[198,   750] training loss: 0.00012930
INFO:root:[198,   800] training loss: 0.00022671
INFO:root:[198,   850] training loss: 0.00013881
INFO:root:[198,   900] training loss: 0.00284427
INFO:root:[198,   950] training loss: 0.00116485
INFO:root:[198,  1000] training loss: 0.00002001
INFO:root:[198,  1050] training loss: 0.00002051
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch198
INFO:root:[199,    50] training loss: 0.00631301
INFO:root:[199,   100] training loss: 0.00672446
INFO:root:[199,   150] training loss: 0.00644415
INFO:root:[199,   200] training loss: 0.00585415
INFO:root:[199,   250] training loss: 0.00588441
INFO:root:[199,   300] training loss: 0.00691146
INFO:root:[199,   350] training loss: 0.00534009
INFO:root:[199,   400] training loss: 0.00000672
INFO:root:[199,   450] training loss: 0.00000774
INFO:root:[199,   500] training loss: 0.00001138
INFO:root:[199,   550] training loss: 0.00017360
INFO:root:[199,   600] training loss: 0.00011626
INFO:root:[199,   650] training loss: 0.00001215
INFO:root:[199,   700] training loss: 0.00000939
INFO:root:[199,   750] training loss: 0.00012957
INFO:root:[199,   800] training loss: 0.00025447
INFO:root:[199,   850] training loss: 0.00017093
INFO:root:[199,   900] training loss: 0.00266770
INFO:root:[199,   950] training loss: 0.00075680
INFO:root:[199,  1000] training loss: 0.00001877
INFO:root:[199,  1050] training loss: 0.00001676
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch199
INFO:root:[200,    50] training loss: 0.00688847
INFO:root:[200,   100] training loss: 0.00596603
INFO:root:[200,   150] training loss: 0.00654956
INFO:root:[200,   200] training loss: 0.00625520
INFO:root:[200,   250] training loss: 0.00515848
INFO:root:[200,   300] training loss: 0.00707173
INFO:root:[200,   350] training loss: 0.00518552
INFO:root:[200,   400] training loss: 0.00001147
INFO:root:[200,   450] training loss: 0.00000568
INFO:root:[200,   500] training loss: 0.00001215
INFO:root:[200,   550] training loss: 0.00018997
INFO:root:[200,   600] training loss: 0.00009012
INFO:root:[200,   650] training loss: 0.00001451
INFO:root:[200,   700] training loss: 0.00000885
INFO:root:[200,   750] training loss: 0.00013590
INFO:root:[200,   800] training loss: 0.00014695
INFO:root:[200,   850] training loss: 0.00019346
INFO:root:[200,   900] training loss: 0.00274153
INFO:root:[200,   950] training loss: 0.00081492
INFO:root:[200,  1000] training loss: 0.00002144
INFO:root:[200,  1050] training loss: 0.00002082
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch200
INFO:root:[201,    50] training loss: 0.00679530
INFO:root:[201,   100] training loss: 0.00619884
INFO:root:[201,   150] training loss: 0.00677459
INFO:root:[201,   200] training loss: 0.00582833
INFO:root:[201,   250] training loss: 0.00534215
INFO:root:[201,   300] training loss: 0.00693315
INFO:root:[201,   350] training loss: 0.00528005
INFO:root:[201,   400] training loss: 0.00002150
INFO:root:[201,   450] training loss: 0.00000710
INFO:root:[201,   500] training loss: 0.00001273
INFO:root:[201,   550] training loss: 0.00027175
INFO:root:[201,   600] training loss: 0.00011759
INFO:root:[201,   650] training loss: 0.00001422
INFO:root:[201,   700] training loss: 0.00000860
INFO:root:[201,   750] training loss: 0.00019672
INFO:root:[201,   800] training loss: 0.00017781
INFO:root:[201,   850] training loss: 0.00015191
INFO:root:[201,   900] training loss: 0.00270222
INFO:root:[201,   950] training loss: 0.00072317
INFO:root:[201,  1000] training loss: 0.00002188
INFO:root:[201,  1050] training loss: 0.00001491
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch201
INFO:root:[202,    50] training loss: 0.00639829
INFO:root:[202,   100] training loss: 0.00623855
INFO:root:[202,   150] training loss: 0.00615421
INFO:root:[202,   200] training loss: 0.00597994
INFO:root:[202,   250] training loss: 0.00541715
INFO:root:[202,   300] training loss: 0.00674452
INFO:root:[202,   350] training loss: 0.00598699
INFO:root:[202,   400] training loss: 0.00000885
INFO:root:[202,   450] training loss: 0.00000689
INFO:root:[202,   500] training loss: 0.00001650
INFO:root:[202,   550] training loss: 0.00019233
INFO:root:[202,   600] training loss: 0.00010813
INFO:root:[202,   650] training loss: 0.00001703
INFO:root:[202,   700] training loss: 0.00001296
INFO:root:[202,   750] training loss: 0.00014143
INFO:root:[202,   800] training loss: 0.00027277
INFO:root:[202,   850] training loss: 0.00018471
INFO:root:[202,   900] training loss: 0.00320422
INFO:root:[202,   950] training loss: 0.00078721
INFO:root:[202,  1000] training loss: 0.00002413
INFO:root:[202,  1050] training loss: 0.00002317
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch202
INFO:root:[203,    50] training loss: 0.00691376
INFO:root:[203,   100] training loss: 0.00708057
INFO:root:[203,   150] training loss: 0.00597841
INFO:root:[203,   200] training loss: 0.00630054
INFO:root:[203,   250] training loss: 0.00581145
INFO:root:[203,   300] training loss: 0.00660710
INFO:root:[203,   350] training loss: 0.00564793
INFO:root:[203,   400] training loss: 0.00003697
INFO:root:[203,   450] training loss: 0.00000612
INFO:root:[203,   500] training loss: 0.00002099
INFO:root:[203,   550] training loss: 0.00018642
INFO:root:[203,   600] training loss: 0.00009928
INFO:root:[203,   650] training loss: 0.00001243
INFO:root:[203,   700] training loss: 0.00000833
INFO:root:[203,   750] training loss: 0.00011238
INFO:root:[203,   800] training loss: 0.00016504
INFO:root:[203,   850] training loss: 0.00020864
INFO:root:[203,   900] training loss: 0.00247652
INFO:root:[203,   950] training loss: 0.00095977
INFO:root:[203,  1000] training loss: 0.00002187
INFO:root:[203,  1050] training loss: 0.00001620
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch203
INFO:root:[204,    50] training loss: 0.00659334
INFO:root:[204,   100] training loss: 0.00647166
INFO:root:[204,   150] training loss: 0.00636473
INFO:root:[204,   200] training loss: 0.00705871
INFO:root:[204,   250] training loss: 0.00543292
INFO:root:[204,   300] training loss: 0.00699288
INFO:root:[204,   350] training loss: 0.00552997
INFO:root:[204,   400] training loss: 0.00000500
INFO:root:[204,   450] training loss: 0.00000801
INFO:root:[204,   500] training loss: 0.00001460
INFO:root:[204,   550] training loss: 0.00017900
INFO:root:[204,   600] training loss: 0.00011376
INFO:root:[204,   650] training loss: 0.00001201
INFO:root:[204,   700] training loss: 0.00000983
INFO:root:[204,   750] training loss: 0.00014116
INFO:root:[204,   800] training loss: 0.00017048
INFO:root:[204,   850] training loss: 0.00018424
INFO:root:[204,   900] training loss: 0.00276059
INFO:root:[204,   950] training loss: 0.00087500
INFO:root:[204,  1000] training loss: 0.00001956
INFO:root:[204,  1050] training loss: 0.00001543
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch204
INFO:root:[205,    50] training loss: 0.00726693
INFO:root:[205,   100] training loss: 0.00693641
INFO:root:[205,   150] training loss: 0.00608711
INFO:root:[205,   200] training loss: 0.00700005
INFO:root:[205,   250] training loss: 0.00525610
INFO:root:[205,   300] training loss: 0.00677271
INFO:root:[205,   350] training loss: 0.00566879
INFO:root:[205,   400] training loss: 0.00000646
INFO:root:[205,   450] training loss: 0.00000713
INFO:root:[205,   500] training loss: 0.00000995
INFO:root:[205,   550] training loss: 0.00020979
INFO:root:[205,   600] training loss: 0.00010589
INFO:root:[205,   650] training loss: 0.00001139
INFO:root:[205,   700] training loss: 0.00000923
INFO:root:[205,   750] training loss: 0.00011445
INFO:root:[205,   800] training loss: 0.00020448
INFO:root:[205,   850] training loss: 0.00017648
INFO:root:[205,   900] training loss: 0.00241286
INFO:root:[205,   950] training loss: 0.00084986
INFO:root:[205,  1000] training loss: 0.00002190
INFO:root:[205,  1050] training loss: 0.00002412
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch205
INFO:root:[206,    50] training loss: 0.00657094
INFO:root:[206,   100] training loss: 0.00607028
INFO:root:[206,   150] training loss: 0.00629082
INFO:root:[206,   200] training loss: 0.00606442
INFO:root:[206,   250] training loss: 0.00574901
INFO:root:[206,   300] training loss: 0.00711116
INFO:root:[206,   350] training loss: 0.00553160
INFO:root:[206,   400] training loss: 0.00000594
INFO:root:[206,   450] training loss: 0.00000982
INFO:root:[206,   500] training loss: 0.00001261
INFO:root:[206,   550] training loss: 0.00019861
INFO:root:[206,   600] training loss: 0.00010861
INFO:root:[206,   650] training loss: 0.00001293
INFO:root:[206,   700] training loss: 0.00000933
INFO:root:[206,   750] training loss: 0.00011113
INFO:root:[206,   800] training loss: 0.00012363
INFO:root:[206,   850] training loss: 0.00017483
INFO:root:[206,   900] training loss: 0.00266724
INFO:root:[206,   950] training loss: 0.00083689
INFO:root:[206,  1000] training loss: 0.00002012
INFO:root:[206,  1050] training loss: 0.00001599
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch206
INFO:root:[207,    50] training loss: 0.00618898
INFO:root:[207,   100] training loss: 0.00634579
INFO:root:[207,   150] training loss: 0.00602583
INFO:root:[207,   200] training loss: 0.00609376
INFO:root:[207,   250] training loss: 0.00542561
INFO:root:[207,   300] training loss: 0.00677886
INFO:root:[207,   350] training loss: 0.00550834
INFO:root:[207,   400] training loss: 0.00000854
INFO:root:[207,   450] training loss: 0.00000731
INFO:root:[207,   500] training loss: 0.00001229
INFO:root:[207,   550] training loss: 0.00022478
INFO:root:[207,   600] training loss: 0.00008973
INFO:root:[207,   650] training loss: 0.00001298
INFO:root:[207,   700] training loss: 0.00001237
INFO:root:[207,   750] training loss: 0.00011259
INFO:root:[207,   800] training loss: 0.00018628
INFO:root:[207,   850] training loss: 0.00017116
INFO:root:[207,   900] training loss: 0.00325005
INFO:root:[207,   950] training loss: 0.00079997
INFO:root:[207,  1000] training loss: 0.00002380
INFO:root:[207,  1050] training loss: 0.00001492
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch207
INFO:root:[208,    50] training loss: 0.00701682
INFO:root:[208,   100] training loss: 0.00631075
INFO:root:[208,   150] training loss: 0.00656743
INFO:root:[208,   200] training loss: 0.00589004
INFO:root:[208,   250] training loss: 0.00574795
INFO:root:[208,   300] training loss: 0.00688588
INFO:root:[208,   350] training loss: 0.00531914
INFO:root:[208,   400] training loss: 0.00000615
INFO:root:[208,   450] training loss: 0.00001041
INFO:root:[208,   500] training loss: 0.00001197
INFO:root:[208,   550] training loss: 0.00020054
INFO:root:[208,   600] training loss: 0.00011837
INFO:root:[208,   650] training loss: 0.00001206
INFO:root:[208,   700] training loss: 0.00000972
INFO:root:[208,   750] training loss: 0.00011094
INFO:root:[208,   800] training loss: 0.00021687
INFO:root:[208,   850] training loss: 0.00016203
INFO:root:[208,   900] training loss: 0.00264706
INFO:root:[208,   950] training loss: 0.00126426
INFO:root:[208,  1000] training loss: 0.00001949
INFO:root:[208,  1050] training loss: 0.00001570
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch208
INFO:root:[209,    50] training loss: 0.00689062
INFO:root:[209,   100] training loss: 0.00778982
INFO:root:[209,   150] training loss: 0.00716282
INFO:root:[209,   200] training loss: 0.00591176
INFO:root:[209,   250] training loss: 0.00549536
INFO:root:[209,   300] training loss: 0.00660773
INFO:root:[209,   350] training loss: 0.00534725
INFO:root:[209,   400] training loss: 0.00000703
INFO:root:[209,   450] training loss: 0.00001042
INFO:root:[209,   500] training loss: 0.00001548
INFO:root:[209,   550] training loss: 0.00019558
INFO:root:[209,   600] training loss: 0.00010761
INFO:root:[209,   650] training loss: 0.00001205
INFO:root:[209,   700] training loss: 0.00000919
INFO:root:[209,   750] training loss: 0.00013802
INFO:root:[209,   800] training loss: 0.00015064
INFO:root:[209,   850] training loss: 0.00020054
INFO:root:[209,   900] training loss: 0.00264197
INFO:root:[209,   950] training loss: 0.00078652
INFO:root:[209,  1000] training loss: 0.00001901
INFO:root:[209,  1050] training loss: 0.00001831
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch209
INFO:root:[210,    50] training loss: 0.00691420
INFO:root:[210,   100] training loss: 0.00679754
INFO:root:[210,   150] training loss: 0.00615162
INFO:root:[210,   200] training loss: 0.00576348
INFO:root:[210,   250] training loss: 0.00616133
INFO:root:[210,   300] training loss: 0.00704908
INFO:root:[210,   350] training loss: 0.00540876
INFO:root:[210,   400] training loss: 0.00000725
INFO:root:[210,   450] training loss: 0.00000585
INFO:root:[210,   500] training loss: 0.00001613
INFO:root:[210,   550] training loss: 0.00021818
INFO:root:[210,   600] training loss: 0.00009864
INFO:root:[210,   650] training loss: 0.00001417
INFO:root:[210,   700] training loss: 0.00000893
INFO:root:[210,   750] training loss: 0.00009582
INFO:root:[210,   800] training loss: 0.00023481
INFO:root:[210,   850] training loss: 0.00016762
INFO:root:[210,   900] training loss: 0.00300684
INFO:root:[210,   950] training loss: 0.00084959
INFO:root:[210,  1000] training loss: 0.00002165
INFO:root:[210,  1050] training loss: 0.00001593
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch210
INFO:root:[211,    50] training loss: 0.00684657
INFO:root:[211,   100] training loss: 0.00633122
INFO:root:[211,   150] training loss: 0.00644308
INFO:root:[211,   200] training loss: 0.00620549
INFO:root:[211,   250] training loss: 0.00542466
INFO:root:[211,   300] training loss: 0.00678862
INFO:root:[211,   350] training loss: 0.00547559
INFO:root:[211,   400] training loss: 0.00000726
INFO:root:[211,   450] training loss: 0.00000714
INFO:root:[211,   500] training loss: 0.00001277
INFO:root:[211,   550] training loss: 0.00016652
INFO:root:[211,   600] training loss: 0.00011102
INFO:root:[211,   650] training loss: 0.00001269
INFO:root:[211,   700] training loss: 0.00000846
INFO:root:[211,   750] training loss: 0.00012536
INFO:root:[211,   800] training loss: 0.00019095
INFO:root:[211,   850] training loss: 0.00016805
INFO:root:[211,   900] training loss: 0.00406545
INFO:root:[211,   950] training loss: 0.00072466
INFO:root:[211,  1000] training loss: 0.00002106
INFO:root:[211,  1050] training loss: 0.00001565
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch211
INFO:root:[212,    50] training loss: 0.00700144
INFO:root:[212,   100] training loss: 0.00618931
INFO:root:[212,   150] training loss: 0.00718690
INFO:root:[212,   200] training loss: 0.00617573
INFO:root:[212,   250] training loss: 0.00528868
INFO:root:[212,   300] training loss: 0.00661521
INFO:root:[212,   350] training loss: 0.00546630
INFO:root:[212,   400] training loss: 0.00000695
INFO:root:[212,   450] training loss: 0.00000656
INFO:root:[212,   500] training loss: 0.00003360
INFO:root:[212,   550] training loss: 0.00018193
INFO:root:[212,   600] training loss: 0.00013343
INFO:root:[212,   650] training loss: 0.00000868
INFO:root:[212,   700] training loss: 0.00001373
INFO:root:[212,   750] training loss: 0.00015650
INFO:root:[212,   800] training loss: 0.00014257
INFO:root:[212,   850] training loss: 0.00022105
INFO:root:[212,   900] training loss: 0.00335845
INFO:root:[212,   950] training loss: 0.00068690
INFO:root:[212,  1000] training loss: 0.00001945
INFO:root:[212,  1050] training loss: 0.00001630
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch212
INFO:root:[213,    50] training loss: 0.00673215
INFO:root:[213,   100] training loss: 0.00672154
INFO:root:[213,   150] training loss: 0.00676956
INFO:root:[213,   200] training loss: 0.00610881
INFO:root:[213,   250] training loss: 0.00561876
INFO:root:[213,   300] training loss: 0.00667209
INFO:root:[213,   350] training loss: 0.00590026
INFO:root:[213,   400] training loss: 0.00000673
INFO:root:[213,   450] training loss: 0.00000754
INFO:root:[213,   500] training loss: 0.00001985
INFO:root:[213,   550] training loss: 0.00021493
INFO:root:[213,   600] training loss: 0.00010073
INFO:root:[213,   650] training loss: 0.00001057
INFO:root:[213,   700] training loss: 0.00000989
INFO:root:[213,   750] training loss: 0.00015476
INFO:root:[213,   800] training loss: 0.00013759
INFO:root:[213,   850] training loss: 0.00018795
INFO:root:[213,   900] training loss: 0.00232710
INFO:root:[213,   950] training loss: 0.00112505
INFO:root:[213,  1000] training loss: 0.00001923
INFO:root:[213,  1050] training loss: 0.00001733
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch213
INFO:root:[214,    50] training loss: 0.00641087
INFO:root:[214,   100] training loss: 0.00636823
INFO:root:[214,   150] training loss: 0.00592785
INFO:root:[214,   200] training loss: 0.00609209
INFO:root:[214,   250] training loss: 0.00577691
INFO:root:[214,   300] training loss: 0.00656948
INFO:root:[214,   350] training loss: 0.00554772
INFO:root:[214,   400] training loss: 0.00000721
INFO:root:[214,   450] training loss: 0.00000697
INFO:root:[214,   500] training loss: 0.00001113
INFO:root:[214,   550] training loss: 0.00019080
INFO:root:[214,   600] training loss: 0.00009712
INFO:root:[214,   650] training loss: 0.00001483
INFO:root:[214,   700] training loss: 0.00001026
INFO:root:[214,   750] training loss: 0.00009041
INFO:root:[214,   800] training loss: 0.00020536
INFO:root:[214,   850] training loss: 0.00020948
INFO:root:[214,   900] training loss: 0.00252645
INFO:root:[214,   950] training loss: 0.00146832
INFO:root:[214,  1000] training loss: 0.00002123
INFO:root:[214,  1050] training loss: 0.00001600
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch214
INFO:root:[215,    50] training loss: 0.00710582
INFO:root:[215,   100] training loss: 0.00620520
INFO:root:[215,   150] training loss: 0.00681488
INFO:root:[215,   200] training loss: 0.00677059
INFO:root:[215,   250] training loss: 0.00553972
INFO:root:[215,   300] training loss: 0.00721044
INFO:root:[215,   350] training loss: 0.00550883
INFO:root:[215,   400] training loss: 0.00000562
INFO:root:[215,   450] training loss: 0.00001011
INFO:root:[215,   500] training loss: 0.00001566
INFO:root:[215,   550] training loss: 0.00019760
INFO:root:[215,   600] training loss: 0.00010342
INFO:root:[215,   650] training loss: 0.00001206
INFO:root:[215,   700] training loss: 0.00001100
INFO:root:[215,   750] training loss: 0.00009580
INFO:root:[215,   800] training loss: 0.00020974
INFO:root:[215,   850] training loss: 0.00024328
INFO:root:[215,   900] training loss: 0.00263286
INFO:root:[215,   950] training loss: 0.00116193
INFO:root:[215,  1000] training loss: 0.00001995
INFO:root:[215,  1050] training loss: 0.00002177
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch215
INFO:root:[216,    50] training loss: 0.00674609
INFO:root:[216,   100] training loss: 0.00760207
INFO:root:[216,   150] training loss: 0.00701750
INFO:root:[216,   200] training loss: 0.00603590
INFO:root:[216,   250] training loss: 0.00540335
INFO:root:[216,   300] training loss: 0.00650311
INFO:root:[216,   350] training loss: 0.00519976
INFO:root:[216,   400] training loss: 0.00000777
INFO:root:[216,   450] training loss: 0.00000766
INFO:root:[216,   500] training loss: 0.00001361
INFO:root:[216,   550] training loss: 0.00019500
INFO:root:[216,   600] training loss: 0.00008985
INFO:root:[216,   650] training loss: 0.00001347
INFO:root:[216,   700] training loss: 0.00000961
INFO:root:[216,   750] training loss: 0.00009907
INFO:root:[216,   800] training loss: 0.00025132
INFO:root:[216,   850] training loss: 0.00014170
INFO:root:[216,   900] training loss: 0.00364208
INFO:root:[216,   950] training loss: 0.00101216
INFO:root:[216,  1000] training loss: 0.00001870
INFO:root:[216,  1050] training loss: 0.00006379
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch216
INFO:root:[217,    50] training loss: 0.00731955
INFO:root:[217,   100] training loss: 0.00647100
INFO:root:[217,   150] training loss: 0.00639071
INFO:root:[217,   200] training loss: 0.00602504
INFO:root:[217,   250] training loss: 0.00533209
INFO:root:[217,   300] training loss: 0.00727419
INFO:root:[217,   350] training loss: 0.00520614
INFO:root:[217,   400] training loss: 0.00000566
INFO:root:[217,   450] training loss: 0.00000575
INFO:root:[217,   500] training loss: 0.00001522
INFO:root:[217,   550] training loss: 0.00017286
INFO:root:[217,   600] training loss: 0.00009749
INFO:root:[217,   650] training loss: 0.00001388
INFO:root:[217,   700] training loss: 0.00001060
INFO:root:[217,   750] training loss: 0.00014381
INFO:root:[217,   800] training loss: 0.00017674
INFO:root:[217,   850] training loss: 0.00015725
INFO:root:[217,   900] training loss: 0.00258992
INFO:root:[217,   950] training loss: 0.00149993
INFO:root:[217,  1000] training loss: 0.00002483
INFO:root:[217,  1050] training loss: 0.00001760
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch217
INFO:root:[218,    50] training loss: 0.00619607
INFO:root:[218,   100] training loss: 0.00744499
INFO:root:[218,   150] training loss: 0.00607042
INFO:root:[218,   200] training loss: 0.00602329
INFO:root:[218,   250] training loss: 0.00529490
INFO:root:[218,   300] training loss: 0.00819086
INFO:root:[218,   350] training loss: 0.00538960
INFO:root:[218,   400] training loss: 0.00000563
INFO:root:[218,   450] training loss: 0.00000810
INFO:root:[218,   500] training loss: 0.00001094
INFO:root:[218,   550] training loss: 0.00016682
INFO:root:[218,   600] training loss: 0.00014052
INFO:root:[218,   650] training loss: 0.00000980
INFO:root:[218,   700] training loss: 0.00000849
INFO:root:[218,   750] training loss: 0.00010933
INFO:root:[218,   800] training loss: 0.00015429
INFO:root:[218,   850] training loss: 0.00017066
INFO:root:[218,   900] training loss: 0.00266897
INFO:root:[218,   950] training loss: 0.00087407
INFO:root:[218,  1000] training loss: 0.00002079
INFO:root:[218,  1050] training loss: 0.00001668
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch218
INFO:root:[219,    50] training loss: 0.00684033
INFO:root:[219,   100] training loss: 0.00633073
INFO:root:[219,   150] training loss: 0.00618868
INFO:root:[219,   200] training loss: 0.00609609
INFO:root:[219,   250] training loss: 0.00565154
INFO:root:[219,   300] training loss: 0.00675755
INFO:root:[219,   350] training loss: 0.00538866
INFO:root:[219,   400] training loss: 0.00002437
INFO:root:[219,   450] training loss: 0.00000630
INFO:root:[219,   500] training loss: 0.00001355
INFO:root:[219,   550] training loss: 0.00018308
INFO:root:[219,   600] training loss: 0.00011166
INFO:root:[219,   650] training loss: 0.00001864
INFO:root:[219,   700] training loss: 0.00001005
INFO:root:[219,   750] training loss: 0.00011906
INFO:root:[219,   800] training loss: 0.00014525
INFO:root:[219,   850] training loss: 0.00020158
INFO:root:[219,   900] training loss: 0.00265440
INFO:root:[219,   950] training loss: 0.00095490
INFO:root:[219,  1000] training loss: 0.00002252
INFO:root:[219,  1050] training loss: 0.00001681
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch219
INFO:root:[220,    50] training loss: 0.00696090
INFO:root:[220,   100] training loss: 0.00622331
INFO:root:[220,   150] training loss: 0.00593588
INFO:root:[220,   200] training loss: 0.00598458
INFO:root:[220,   250] training loss: 0.00577827
INFO:root:[220,   300] training loss: 0.00681389
INFO:root:[220,   350] training loss: 0.00527989
INFO:root:[220,   400] training loss: 0.00000598
INFO:root:[220,   450] training loss: 0.00000596
INFO:root:[220,   500] training loss: 0.00001665
INFO:root:[220,   550] training loss: 0.00022235
INFO:root:[220,   600] training loss: 0.00009913
INFO:root:[220,   650] training loss: 0.00001098
INFO:root:[220,   700] training loss: 0.00001645
INFO:root:[220,   750] training loss: 0.00015689
INFO:root:[220,   800] training loss: 0.00015751
INFO:root:[220,   850] training loss: 0.00015788
INFO:root:[220,   900] training loss: 0.00277780
INFO:root:[220,   950] training loss: 0.00095838
INFO:root:[220,  1000] training loss: 0.00001955
INFO:root:[220,  1050] training loss: 0.00001580
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch220
INFO:root:[221,    50] training loss: 0.00658502
INFO:root:[221,   100] training loss: 0.00642775
INFO:root:[221,   150] training loss: 0.00689064
INFO:root:[221,   200] training loss: 0.00626549
INFO:root:[221,   250] training loss: 0.00571176
INFO:root:[221,   300] training loss: 0.00683177
INFO:root:[221,   350] training loss: 0.00543033
INFO:root:[221,   400] training loss: 0.00000956
INFO:root:[221,   450] training loss: 0.00000685
INFO:root:[221,   500] training loss: 0.00001903
INFO:root:[221,   550] training loss: 0.00018880
INFO:root:[221,   600] training loss: 0.00008592
INFO:root:[221,   650] training loss: 0.00001503
INFO:root:[221,   700] training loss: 0.00001372
INFO:root:[221,   750] training loss: 0.00015293
INFO:root:[221,   800] training loss: 0.00020446
INFO:root:[221,   850] training loss: 0.00014884
INFO:root:[221,   900] training loss: 0.00282351
INFO:root:[221,   950] training loss: 0.00083047
INFO:root:[221,  1000] training loss: 0.00001911
INFO:root:[221,  1050] training loss: 0.00001651
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch221
INFO:root:[222,    50] training loss: 0.00659068
INFO:root:[222,   100] training loss: 0.00828616
INFO:root:[222,   150] training loss: 0.00661161
INFO:root:[222,   200] training loss: 0.00597627
INFO:root:[222,   250] training loss: 0.00626887
INFO:root:[222,   300] training loss: 0.00684346
INFO:root:[222,   350] training loss: 0.00526842
INFO:root:[222,   400] training loss: 0.00000768
INFO:root:[222,   450] training loss: 0.00000570
INFO:root:[222,   500] training loss: 0.00001201
INFO:root:[222,   550] training loss: 0.00020580
INFO:root:[222,   600] training loss: 0.00009365
INFO:root:[222,   650] training loss: 0.00002012
INFO:root:[222,   700] training loss: 0.00000784
INFO:root:[222,   750] training loss: 0.00011399
INFO:root:[222,   800] training loss: 0.00019236
INFO:root:[222,   850] training loss: 0.00017690
INFO:root:[222,   900] training loss: 0.00305289
INFO:root:[222,   950] training loss: 0.00106457
INFO:root:[222,  1000] training loss: 0.00002337
INFO:root:[222,  1050] training loss: 0.00001714
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch222
INFO:root:[223,    50] training loss: 0.00666334
INFO:root:[223,   100] training loss: 0.00615471
INFO:root:[223,   150] training loss: 0.00684770
INFO:root:[223,   200] training loss: 0.00611726
INFO:root:[223,   250] training loss: 0.00537792
INFO:root:[223,   300] training loss: 0.00664903
INFO:root:[223,   350] training loss: 0.00538578
INFO:root:[223,   400] training loss: 0.00000603
INFO:root:[223,   450] training loss: 0.00000586
INFO:root:[223,   500] training loss: 0.00001207
INFO:root:[223,   550] training loss: 0.00022228
INFO:root:[223,   600] training loss: 0.00008941
INFO:root:[223,   650] training loss: 0.00001428
INFO:root:[223,   700] training loss: 0.00000957
INFO:root:[223,   750] training loss: 0.00011361
INFO:root:[223,   800] training loss: 0.00014790
INFO:root:[223,   850] training loss: 0.00016329
INFO:root:[223,   900] training loss: 0.00308632
INFO:root:[223,   950] training loss: 0.00064325
INFO:root:[223,  1000] training loss: 0.00002075
INFO:root:[223,  1050] training loss: 0.00001609
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch223
INFO:root:[224,    50] training loss: 0.00684507
INFO:root:[224,   100] training loss: 0.00598444
INFO:root:[224,   150] training loss: 0.00680182
INFO:root:[224,   200] training loss: 0.00605401
INFO:root:[224,   250] training loss: 0.00551488
INFO:root:[224,   300] training loss: 0.00694411
INFO:root:[224,   350] training loss: 0.00506155
INFO:root:[224,   400] training loss: 0.00000761
INFO:root:[224,   450] training loss: 0.00000703
INFO:root:[224,   500] training loss: 0.00001497
INFO:root:[224,   550] training loss: 0.00024139
INFO:root:[224,   600] training loss: 0.00010542
INFO:root:[224,   650] training loss: 0.00001195
INFO:root:[224,   700] training loss: 0.00000943
INFO:root:[224,   750] training loss: 0.00009692
INFO:root:[224,   800] training loss: 0.00017896
INFO:root:[224,   850] training loss: 0.00015710
INFO:root:[224,   900] training loss: 0.00329517
INFO:root:[224,   950] training loss: 0.00097136
INFO:root:[224,  1000] training loss: 0.00002281
INFO:root:[224,  1050] training loss: 0.00001678
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch224
INFO:root:[225,    50] training loss: 0.00683429
INFO:root:[225,   100] training loss: 0.00682121
INFO:root:[225,   150] training loss: 0.00622050
INFO:root:[225,   200] training loss: 0.00611429
INFO:root:[225,   250] training loss: 0.00523053
INFO:root:[225,   300] training loss: 0.00631902
INFO:root:[225,   350] training loss: 0.00607146
INFO:root:[225,   400] training loss: 0.00000794
INFO:root:[225,   450] training loss: 0.00000687
INFO:root:[225,   500] training loss: 0.00001261
INFO:root:[225,   550] training loss: 0.00018499
INFO:root:[225,   600] training loss: 0.00010337
INFO:root:[225,   650] training loss: 0.00002769
INFO:root:[225,   700] training loss: 0.00001407
INFO:root:[225,   750] training loss: 0.00012631
INFO:root:[225,   800] training loss: 0.00016866
INFO:root:[225,   850] training loss: 0.00013316
INFO:root:[225,   900] training loss: 0.00307508
INFO:root:[225,   950] training loss: 0.00110768
INFO:root:[225,  1000] training loss: 0.00001944
INFO:root:[225,  1050] training loss: 0.00002050
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch225
INFO:root:[226,    50] training loss: 0.00684880
INFO:root:[226,   100] training loss: 0.00611306
INFO:root:[226,   150] training loss: 0.00676118
INFO:root:[226,   200] training loss: 0.00623690
INFO:root:[226,   250] training loss: 0.00532878
INFO:root:[226,   300] training loss: 0.00695986
INFO:root:[226,   350] training loss: 0.00548775
INFO:root:[226,   400] training loss: 0.00000702
INFO:root:[226,   450] training loss: 0.00000564
INFO:root:[226,   500] training loss: 0.00001492
INFO:root:[226,   550] training loss: 0.00021766
INFO:root:[226,   600] training loss: 0.00012707
INFO:root:[226,   650] training loss: 0.00001317
INFO:root:[226,   700] training loss: 0.00000948
INFO:root:[226,   750] training loss: 0.00010597
INFO:root:[226,   800] training loss: 0.00014322
INFO:root:[226,   850] training loss: 0.00022443
INFO:root:[226,   900] training loss: 0.00261445
INFO:root:[226,   950] training loss: 0.00073569
INFO:root:[226,  1000] training loss: 0.00002071
INFO:root:[226,  1050] training loss: 0.00002300
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch226
INFO:root:[227,    50] training loss: 0.00616981
INFO:root:[227,   100] training loss: 0.00669667
INFO:root:[227,   150] training loss: 0.00935272
INFO:root:[227,   200] training loss: 0.00605140
INFO:root:[227,   250] training loss: 0.00557977
INFO:root:[227,   300] training loss: 0.00675353
INFO:root:[227,   350] training loss: 0.00542597
INFO:root:[227,   400] training loss: 0.00000643
INFO:root:[227,   450] training loss: 0.00000748
INFO:root:[227,   500] training loss: 0.00001689
INFO:root:[227,   550] training loss: 0.00022218
INFO:root:[227,   600] training loss: 0.00009664
INFO:root:[227,   650] training loss: 0.00001158
INFO:root:[227,   700] training loss: 0.00000858
INFO:root:[227,   750] training loss: 0.00014459
INFO:root:[227,   800] training loss: 0.00021029
INFO:root:[227,   850] training loss: 0.00017122
INFO:root:[227,   900] training loss: 0.00261277
INFO:root:[227,   950] training loss: 0.00087999
INFO:root:[227,  1000] training loss: 0.00002216
INFO:root:[227,  1050] training loss: 0.00001793
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch227
INFO:root:[228,    50] training loss: 0.00668022
INFO:root:[228,   100] training loss: 0.00634567
INFO:root:[228,   150] training loss: 0.00639144
INFO:root:[228,   200] training loss: 0.00623525
INFO:root:[228,   250] training loss: 0.00651445
INFO:root:[228,   300] training loss: 0.00704782
INFO:root:[228,   350] training loss: 0.00558725
INFO:root:[228,   400] training loss: 0.00000634
INFO:root:[228,   450] training loss: 0.00000742
INFO:root:[228,   500] training loss: 0.00001355
INFO:root:[228,   550] training loss: 0.00021559
INFO:root:[228,   600] training loss: 0.00010660
INFO:root:[228,   650] training loss: 0.00000902
INFO:root:[228,   700] training loss: 0.00001019
INFO:root:[228,   750] training loss: 0.00009387
INFO:root:[228,   800] training loss: 0.00014242
INFO:root:[228,   850] training loss: 0.00020031
INFO:root:[228,   900] training loss: 0.00252425
INFO:root:[228,   950] training loss: 0.00078389
INFO:root:[228,  1000] training loss: 0.00002180
INFO:root:[228,  1050] training loss: 0.00001633
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch228
INFO:root:[229,    50] training loss: 0.00650594
INFO:root:[229,   100] training loss: 0.00628579
INFO:root:[229,   150] training loss: 0.00645439
INFO:root:[229,   200] training loss: 0.00613984
INFO:root:[229,   250] training loss: 0.00582563
INFO:root:[229,   300] training loss: 0.00629524
INFO:root:[229,   350] training loss: 0.00546241
INFO:root:[229,   400] training loss: 0.00001017
INFO:root:[229,   450] training loss: 0.00000788
INFO:root:[229,   500] training loss: 0.00001284
INFO:root:[229,   550] training loss: 0.00022101
INFO:root:[229,   600] training loss: 0.00008178
INFO:root:[229,   650] training loss: 0.00001110
INFO:root:[229,   700] training loss: 0.00000998
INFO:root:[229,   750] training loss: 0.00013401
INFO:root:[229,   800] training loss: 0.00015490
INFO:root:[229,   850] training loss: 0.00013867
INFO:root:[229,   900] training loss: 0.00303943
INFO:root:[229,   950] training loss: 0.00073347
INFO:root:[229,  1000] training loss: 0.00002011
INFO:root:[229,  1050] training loss: 0.00001567
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch229
INFO:root:[230,    50] training loss: 0.00670161
INFO:root:[230,   100] training loss: 0.00633425
INFO:root:[230,   150] training loss: 0.00616006
INFO:root:[230,   200] training loss: 0.00589140
INFO:root:[230,   250] training loss: 0.00603937
INFO:root:[230,   300] training loss: 0.00654317
INFO:root:[230,   350] training loss: 0.00571515
INFO:root:[230,   400] training loss: 0.00000708
INFO:root:[230,   450] training loss: 0.00000671
INFO:root:[230,   500] training loss: 0.00001313
INFO:root:[230,   550] training loss: 0.00018321
INFO:root:[230,   600] training loss: 0.00012154
INFO:root:[230,   650] training loss: 0.00001455
INFO:root:[230,   700] training loss: 0.00000803
INFO:root:[230,   750] training loss: 0.00011824
INFO:root:[230,   800] training loss: 0.00021481
INFO:root:[230,   850] training loss: 0.00014734
INFO:root:[230,   900] training loss: 0.00294576
INFO:root:[230,   950] training loss: 0.00090947
INFO:root:[230,  1000] training loss: 0.00002390
INFO:root:[230,  1050] training loss: 0.00001743
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch230
INFO:root:[231,    50] training loss: 0.00668647
INFO:root:[231,   100] training loss: 0.00638574
INFO:root:[231,   150] training loss: 0.00615877
INFO:root:[231,   200] training loss: 0.00692795
INFO:root:[231,   250] training loss: 0.00579595
INFO:root:[231,   300] training loss: 0.00702982
INFO:root:[231,   350] training loss: 0.00554196
INFO:root:[231,   400] training loss: 0.00001050
INFO:root:[231,   450] training loss: 0.00000446
INFO:root:[231,   500] training loss: 0.00001410
INFO:root:[231,   550] training loss: 0.00020190
INFO:root:[231,   600] training loss: 0.00008106
INFO:root:[231,   650] training loss: 0.00002673
INFO:root:[231,   700] training loss: 0.00000798
INFO:root:[231,   750] training loss: 0.00012248
INFO:root:[231,   800] training loss: 0.00014161
INFO:root:[231,   850] training loss: 0.00019207
INFO:root:[231,   900] training loss: 0.00267406
INFO:root:[231,   950] training loss: 0.00117074
INFO:root:[231,  1000] training loss: 0.00002343
INFO:root:[231,  1050] training loss: 0.00002405
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch231
INFO:root:[232,    50] training loss: 0.00671769
INFO:root:[232,   100] training loss: 0.00644030
INFO:root:[232,   150] training loss: 0.00673594
INFO:root:[232,   200] training loss: 0.00595660
INFO:root:[232,   250] training loss: 0.00508709
INFO:root:[232,   300] training loss: 0.00669934
INFO:root:[232,   350] training loss: 0.00542657
INFO:root:[232,   400] training loss: 0.00000849
INFO:root:[232,   450] training loss: 0.00000488
INFO:root:[232,   500] training loss: 0.00001209
INFO:root:[232,   550] training loss: 0.00017822
INFO:root:[232,   600] training loss: 0.00007863
INFO:root:[232,   650] training loss: 0.00001174
INFO:root:[232,   700] training loss: 0.00000999
INFO:root:[232,   750] training loss: 0.00009227
INFO:root:[232,   800] training loss: 0.00014143
INFO:root:[232,   850] training loss: 0.00015033
INFO:root:[232,   900] training loss: 0.00338711
INFO:root:[232,   950] training loss: 0.00107733
INFO:root:[232,  1000] training loss: 0.00001921
INFO:root:[232,  1050] training loss: 0.00001377
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch232
INFO:root:[233,    50] training loss: 0.00641150
INFO:root:[233,   100] training loss: 0.00651602
INFO:root:[233,   150] training loss: 0.00651925
INFO:root:[233,   200] training loss: 0.00605282
INFO:root:[233,   250] training loss: 0.00703903
INFO:root:[233,   300] training loss: 0.00674382
INFO:root:[233,   350] training loss: 0.00518225
INFO:root:[233,   400] training loss: 0.00000884
INFO:root:[233,   450] training loss: 0.00000797
INFO:root:[233,   500] training loss: 0.00001136
INFO:root:[233,   550] training loss: 0.00017969
INFO:root:[233,   600] training loss: 0.00009701
INFO:root:[233,   650] training loss: 0.00001251
INFO:root:[233,   700] training loss: 0.00000855
INFO:root:[233,   750] training loss: 0.00011795
INFO:root:[233,   800] training loss: 0.00024749
INFO:root:[233,   850] training loss: 0.00019353
INFO:root:[233,   900] training loss: 0.00298285
INFO:root:[233,   950] training loss: 0.00100808
INFO:root:[233,  1000] training loss: 0.00002003
INFO:root:[233,  1050] training loss: 0.00001691
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch233
INFO:root:[234,    50] training loss: 0.00670091
INFO:root:[234,   100] training loss: 0.00632545
INFO:root:[234,   150] training loss: 0.00642168
INFO:root:[234,   200] training loss: 0.00609750
INFO:root:[234,   250] training loss: 0.00554616
INFO:root:[234,   300] training loss: 0.00697896
INFO:root:[234,   350] training loss: 0.00559615
INFO:root:[234,   400] training loss: 0.00000556
INFO:root:[234,   450] training loss: 0.00000721
INFO:root:[234,   500] training loss: 0.00001625
INFO:root:[234,   550] training loss: 0.00023175
INFO:root:[234,   600] training loss: 0.00009052
INFO:root:[234,   650] training loss: 0.00001191
INFO:root:[234,   700] training loss: 0.00000989
INFO:root:[234,   750] training loss: 0.00010469
INFO:root:[234,   800] training loss: 0.00013799
INFO:root:[234,   850] training loss: 0.00018309
INFO:root:[234,   900] training loss: 0.00234912
INFO:root:[234,   950] training loss: 0.00082801
INFO:root:[234,  1000] training loss: 0.00002074
INFO:root:[234,  1050] training loss: 0.00001536
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch234
INFO:root:[235,    50] training loss: 0.00642108
INFO:root:[235,   100] training loss: 0.00642890
INFO:root:[235,   150] training loss: 0.00663198
INFO:root:[235,   200] training loss: 0.00606245
INFO:root:[235,   250] training loss: 0.00564129
INFO:root:[235,   300] training loss: 0.00669503
INFO:root:[235,   350] training loss: 0.00533525
INFO:root:[235,   400] training loss: 0.00000956
INFO:root:[235,   450] training loss: 0.00000745
INFO:root:[235,   500] training loss: 0.00001258
INFO:root:[235,   550] training loss: 0.00017357
INFO:root:[235,   600] training loss: 0.00010481
INFO:root:[235,   650] training loss: 0.00001304
INFO:root:[235,   700] training loss: 0.00001025
INFO:root:[235,   750] training loss: 0.00009417
INFO:root:[235,   800] training loss: 0.00017249
INFO:root:[235,   850] training loss: 0.00021474
INFO:root:[235,   900] training loss: 0.00237290
INFO:root:[235,   950] training loss: 0.00108742
INFO:root:[235,  1000] training loss: 0.00002039
INFO:root:[235,  1050] training loss: 0.00001815
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch235
INFO:root:[236,    50] training loss: 0.00733718
INFO:root:[236,   100] training loss: 0.00648485
INFO:root:[236,   150] training loss: 0.00622338
INFO:root:[236,   200] training loss: 0.00661818
INFO:root:[236,   250] training loss: 0.00555656
INFO:root:[236,   300] training loss: 0.00679122
INFO:root:[236,   350] training loss: 0.00568664
INFO:root:[236,   400] training loss: 0.00000652
INFO:root:[236,   450] training loss: 0.00000867
INFO:root:[236,   500] training loss: 0.00001414
INFO:root:[236,   550] training loss: 0.00022975
INFO:root:[236,   600] training loss: 0.00010846
INFO:root:[236,   650] training loss: 0.00001243
INFO:root:[236,   700] training loss: 0.00001051
INFO:root:[236,   750] training loss: 0.00010495
INFO:root:[236,   800] training loss: 0.00014223
INFO:root:[236,   850] training loss: 0.00019227
INFO:root:[236,   900] training loss: 0.00357617
INFO:root:[236,   950] training loss: 0.00077056
INFO:root:[236,  1000] training loss: 0.00001959
INFO:root:[236,  1050] training loss: 0.00001584
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch236
INFO:root:[237,    50] training loss: 0.00648701
INFO:root:[237,   100] training loss: 0.00694207
INFO:root:[237,   150] training loss: 0.00641736
INFO:root:[237,   200] training loss: 0.00618851
INFO:root:[237,   250] training loss: 0.00564888
INFO:root:[237,   300] training loss: 0.00753670
INFO:root:[237,   350] training loss: 0.00539146
INFO:root:[237,   400] training loss: 0.00001080
INFO:root:[237,   450] training loss: 0.00000996
INFO:root:[237,   500] training loss: 0.00001600
INFO:root:[237,   550] training loss: 0.00021359
INFO:root:[237,   600] training loss: 0.00012275
INFO:root:[237,   650] training loss: 0.00002272
INFO:root:[237,   700] training loss: 0.00000872
INFO:root:[237,   750] training loss: 0.00014211
INFO:root:[237,   800] training loss: 0.00021539
INFO:root:[237,   850] training loss: 0.00017573
INFO:root:[237,   900] training loss: 0.00275809
INFO:root:[237,   950] training loss: 0.00115375
INFO:root:[237,  1000] training loss: 0.00002049
INFO:root:[237,  1050] training loss: 0.00001913
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch237
INFO:root:[238,    50] training loss: 0.00675398
INFO:root:[238,   100] training loss: 0.00616431
INFO:root:[238,   150] training loss: 0.00673493
INFO:root:[238,   200] training loss: 0.00617016
INFO:root:[238,   250] training loss: 0.00530397
INFO:root:[238,   300] training loss: 0.00672870
INFO:root:[238,   350] training loss: 0.00526337
INFO:root:[238,   400] training loss: 0.00000711
INFO:root:[238,   450] training loss: 0.00000670
INFO:root:[238,   500] training loss: 0.00001122
INFO:root:[238,   550] training loss: 0.00025391
INFO:root:[238,   600] training loss: 0.00011286
INFO:root:[238,   650] training loss: 0.00001166
INFO:root:[238,   700] training loss: 0.00000936
INFO:root:[238,   750] training loss: 0.00016165
INFO:root:[238,   800] training loss: 0.00021316
INFO:root:[238,   850] training loss: 0.00014438
INFO:root:[238,   900] training loss: 0.00260186
INFO:root:[238,   950] training loss: 0.00132776
INFO:root:[238,  1000] training loss: 0.00002013
INFO:root:[238,  1050] training loss: 0.00005090
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch238
INFO:root:[239,    50] training loss: 0.00624866
INFO:root:[239,   100] training loss: 0.00827702
INFO:root:[239,   150] training loss: 0.00660572
INFO:root:[239,   200] training loss: 0.00604264
INFO:root:[239,   250] training loss: 0.00576235
INFO:root:[239,   300] training loss: 0.00678310
INFO:root:[239,   350] training loss: 0.00535579
INFO:root:[239,   400] training loss: 0.00000631
INFO:root:[239,   450] training loss: 0.00000594
INFO:root:[239,   500] training loss: 0.00001237
INFO:root:[239,   550] training loss: 0.00019037
INFO:root:[239,   600] training loss: 0.00009973
INFO:root:[239,   650] training loss: 0.00001414
INFO:root:[239,   700] training loss: 0.00000893
INFO:root:[239,   750] training loss: 0.00014269
INFO:root:[239,   800] training loss: 0.00018491
INFO:root:[239,   850] training loss: 0.00018047
INFO:root:[239,   900] training loss: 0.00275879
INFO:root:[239,   950] training loss: 0.00064651
INFO:root:[239,  1000] training loss: 0.00002613
INFO:root:[239,  1050] training loss: 0.00001653
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch239
INFO:root:[240,    50] training loss: 0.00654878
INFO:root:[240,   100] training loss: 0.00644964
INFO:root:[240,   150] training loss: 0.00645455
INFO:root:[240,   200] training loss: 0.00673521
INFO:root:[240,   250] training loss: 0.00547486
INFO:root:[240,   300] training loss: 0.00733365
INFO:root:[240,   350] training loss: 0.00563032
INFO:root:[240,   400] training loss: 0.00000590
INFO:root:[240,   450] training loss: 0.00000670
INFO:root:[240,   500] training loss: 0.00001018
INFO:root:[240,   550] training loss: 0.00018344
INFO:root:[240,   600] training loss: 0.00010423
INFO:root:[240,   650] training loss: 0.00001402
INFO:root:[240,   700] training loss: 0.00001311
INFO:root:[240,   750] training loss: 0.00020442
INFO:root:[240,   800] training loss: 0.00016716
INFO:root:[240,   850] training loss: 0.00030415
INFO:root:[240,   900] training loss: 0.00281775
INFO:root:[240,   950] training loss: 0.00114598
INFO:root:[240,  1000] training loss: 0.00002063
INFO:root:[240,  1050] training loss: 0.00002496
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch240
INFO:root:[241,    50] training loss: 0.00674895
INFO:root:[241,   100] training loss: 0.00640423
INFO:root:[241,   150] training loss: 0.00639836
INFO:root:[241,   200] training loss: 0.00606909
INFO:root:[241,   250] training loss: 0.00547947
INFO:root:[241,   300] training loss: 0.00687598
INFO:root:[241,   350] training loss: 0.00514258
INFO:root:[241,   400] training loss: 0.00000799
INFO:root:[241,   450] training loss: 0.00000581
INFO:root:[241,   500] training loss: 0.00001137
INFO:root:[241,   550] training loss: 0.00018025
INFO:root:[241,   600] training loss: 0.00013857
INFO:root:[241,   650] training loss: 0.00001149
INFO:root:[241,   700] training loss: 0.00001678
INFO:root:[241,   750] training loss: 0.00014258
INFO:root:[241,   800] training loss: 0.00014151
INFO:root:[241,   850] training loss: 0.00018842
INFO:root:[241,   900] training loss: 0.00260010
INFO:root:[241,   950] training loss: 0.00088414
INFO:root:[241,  1000] training loss: 0.00001951
INFO:root:[241,  1050] training loss: 0.00001580
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch241
INFO:root:[242,    50] training loss: 0.00642790
INFO:root:[242,   100] training loss: 0.00829606
INFO:root:[242,   150] training loss: 0.00703098
INFO:root:[242,   200] training loss: 0.00606785
INFO:root:[242,   250] training loss: 0.00520023
INFO:root:[242,   300] training loss: 0.00672458
INFO:root:[242,   350] training loss: 0.00529789
INFO:root:[242,   400] training loss: 0.00000583
INFO:root:[242,   450] training loss: 0.00000558
INFO:root:[242,   500] training loss: 0.00001340
INFO:root:[242,   550] training loss: 0.00023253
INFO:root:[242,   600] training loss: 0.00014419
INFO:root:[242,   650] training loss: 0.00001110
INFO:root:[242,   700] training loss: 0.00001119
INFO:root:[242,   750] training loss: 0.00015944
INFO:root:[242,   800] training loss: 0.00015493
INFO:root:[242,   850] training loss: 0.00014810
INFO:root:[242,   900] training loss: 0.00254754
INFO:root:[242,   950] training loss: 0.00073515
INFO:root:[242,  1000] training loss: 0.00002050
INFO:root:[242,  1050] training loss: 0.00001605
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch242
INFO:root:[243,    50] training loss: 0.00719698
INFO:root:[243,   100] training loss: 0.00651381
INFO:root:[243,   150] training loss: 0.00724363
INFO:root:[243,   200] training loss: 0.00575698
INFO:root:[243,   250] training loss: 0.00611336
INFO:root:[243,   300] training loss: 0.00838598
INFO:root:[243,   350] training loss: 0.00513067
INFO:root:[243,   400] training loss: 0.00000691
INFO:root:[243,   450] training loss: 0.00000592
INFO:root:[243,   500] training loss: 0.00001433
INFO:root:[243,   550] training loss: 0.00025501
INFO:root:[243,   600] training loss: 0.00010612
INFO:root:[243,   650] training loss: 0.00001015
INFO:root:[243,   700] training loss: 0.00001109
INFO:root:[243,   750] training loss: 0.00011593
INFO:root:[243,   800] training loss: 0.00020695
INFO:root:[243,   850] training loss: 0.00022235
INFO:root:[243,   900] training loss: 0.00313377
INFO:root:[243,   950] training loss: 0.00079406
INFO:root:[243,  1000] training loss: 0.00002040
INFO:root:[243,  1050] training loss: 0.00001561
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch243
INFO:root:[244,    50] training loss: 0.00628275
INFO:root:[244,   100] training loss: 0.00607268
INFO:root:[244,   150] training loss: 0.00609047
INFO:root:[244,   200] training loss: 0.00621645
INFO:root:[244,   250] training loss: 0.00585277
INFO:root:[244,   300] training loss: 0.00658095
INFO:root:[244,   350] training loss: 0.00521223
INFO:root:[244,   400] training loss: 0.00000634
INFO:root:[244,   450] training loss: 0.00000697
INFO:root:[244,   500] training loss: 0.00001333
INFO:root:[244,   550] training loss: 0.00016963
INFO:root:[244,   600] training loss: 0.00008524
INFO:root:[244,   650] training loss: 0.00001320
INFO:root:[244,   700] training loss: 0.00001002
INFO:root:[244,   750] training loss: 0.00008621
INFO:root:[244,   800] training loss: 0.00016654
INFO:root:[244,   850] training loss: 0.00016561
INFO:root:[244,   900] training loss: 0.00273572
INFO:root:[244,   950] training loss: 0.00074870
INFO:root:[244,  1000] training loss: 0.00001956
INFO:root:[244,  1050] training loss: 0.00001562
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch244
INFO:root:[245,    50] training loss: 0.00690627
INFO:root:[245,   100] training loss: 0.00653511
INFO:root:[245,   150] training loss: 0.00671068
INFO:root:[245,   200] training loss: 0.00589993
INFO:root:[245,   250] training loss: 0.00570250
INFO:root:[245,   300] training loss: 0.00675550
INFO:root:[245,   350] training loss: 0.00552170
INFO:root:[245,   400] training loss: 0.00000655
INFO:root:[245,   450] training loss: 0.00000683
INFO:root:[245,   500] training loss: 0.00001312
INFO:root:[245,   550] training loss: 0.00022969
INFO:root:[245,   600] training loss: 0.00007601
INFO:root:[245,   650] training loss: 0.00001219
INFO:root:[245,   700] training loss: 0.00001080
INFO:root:[245,   750] training loss: 0.00012560
INFO:root:[245,   800] training loss: 0.00012044
INFO:root:[245,   850] training loss: 0.00014964
INFO:root:[245,   900] training loss: 0.00261787
INFO:root:[245,   950] training loss: 0.00193148
INFO:root:[245,  1000] training loss: 0.00001857
INFO:root:[245,  1050] training loss: 0.00001650
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch245
INFO:root:[246,    50] training loss: 0.00675761
INFO:root:[246,   100] training loss: 0.00794334
INFO:root:[246,   150] training loss: 0.00670211
INFO:root:[246,   200] training loss: 0.00622769
INFO:root:[246,   250] training loss: 0.00549237
INFO:root:[246,   300] training loss: 0.00685865
INFO:root:[246,   350] training loss: 0.00548988
INFO:root:[246,   400] training loss: 0.00000607
INFO:root:[246,   450] training loss: 0.00000658
INFO:root:[246,   500] training loss: 0.00001216
INFO:root:[246,   550] training loss: 0.00016503
INFO:root:[246,   600] training loss: 0.00010091
INFO:root:[246,   650] training loss: 0.00001518
INFO:root:[246,   700] training loss: 0.00001239
INFO:root:[246,   750] training loss: 0.00012720
INFO:root:[246,   800] training loss: 0.00016923
INFO:root:[246,   850] training loss: 0.00013576
INFO:root:[246,   900] training loss: 0.00309473
INFO:root:[246,   950] training loss: 0.00077605
INFO:root:[246,  1000] training loss: 0.00001964
INFO:root:[246,  1050] training loss: 0.00002084
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch246
INFO:root:[247,    50] training loss: 0.00665301
INFO:root:[247,   100] training loss: 0.00603062
INFO:root:[247,   150] training loss: 0.00641656
INFO:root:[247,   200] training loss: 0.00612593
INFO:root:[247,   250] training loss: 0.00587257
INFO:root:[247,   300] training loss: 0.00686003
INFO:root:[247,   350] training loss: 0.00494348
INFO:root:[247,   400] training loss: 0.00000540
INFO:root:[247,   450] training loss: 0.00000473
INFO:root:[247,   500] training loss: 0.00001304
INFO:root:[247,   550] training loss: 0.00018594
INFO:root:[247,   600] training loss: 0.00009623
INFO:root:[247,   650] training loss: 0.00001183
INFO:root:[247,   700] training loss: 0.00001053
INFO:root:[247,   750] training loss: 0.00010358
INFO:root:[247,   800] training loss: 0.00016342
INFO:root:[247,   850] training loss: 0.00019837
INFO:root:[247,   900] training loss: 0.00228310
INFO:root:[247,   950] training loss: 0.00077938
INFO:root:[247,  1000] training loss: 0.00002251
INFO:root:[247,  1050] training loss: 0.00001832
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch247
INFO:root:[248,    50] training loss: 0.00650226
INFO:root:[248,   100] training loss: 0.00620089
INFO:root:[248,   150] training loss: 0.00690941
INFO:root:[248,   200] training loss: 0.00627862
INFO:root:[248,   250] training loss: 0.00537694
INFO:root:[248,   300] training loss: 0.00683411
INFO:root:[248,   350] training loss: 0.00522432
INFO:root:[248,   400] training loss: 0.00000529
INFO:root:[248,   450] training loss: 0.00000586
INFO:root:[248,   500] training loss: 0.00001327
INFO:root:[248,   550] training loss: 0.00023584
INFO:root:[248,   600] training loss: 0.00007467
INFO:root:[248,   650] training loss: 0.00001146
INFO:root:[248,   700] training loss: 0.00001050
INFO:root:[248,   750] training loss: 0.00026404
INFO:root:[248,   800] training loss: 0.00018214
INFO:root:[248,   850] training loss: 0.00020576
INFO:root:[248,   900] training loss: 0.00306299
INFO:root:[248,   950] training loss: 0.00083435
INFO:root:[248,  1000] training loss: 0.00001976
INFO:root:[248,  1050] training loss: 0.00001773
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch248
INFO:root:[249,    50] training loss: 0.00713966
INFO:root:[249,   100] training loss: 0.00642881
INFO:root:[249,   150] training loss: 0.00738865
INFO:root:[249,   200] training loss: 0.00585836
INFO:root:[249,   250] training loss: 0.00526331
INFO:root:[249,   300] training loss: 0.00736206
INFO:root:[249,   350] training loss: 0.00506325
INFO:root:[249,   400] training loss: 0.00000657
INFO:root:[249,   450] training loss: 0.00000588
INFO:root:[249,   500] training loss: 0.00001510
INFO:root:[249,   550] training loss: 0.00019094
INFO:root:[249,   600] training loss: 0.00013124
INFO:root:[249,   650] training loss: 0.00001131
INFO:root:[249,   700] training loss: 0.00001148
INFO:root:[249,   750] training loss: 0.00008874
INFO:root:[249,   800] training loss: 0.00019263
INFO:root:[249,   850] training loss: 0.00018859
INFO:root:[249,   900] training loss: 0.00252196
INFO:root:[249,   950] training loss: 0.00072177
INFO:root:[249,  1000] training loss: 0.00001865
INFO:root:[249,  1050] training loss: 0.00001529
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch249
INFO:root:[250,    50] training loss: 0.00656847
INFO:root:[250,   100] training loss: 0.00823338
INFO:root:[250,   150] training loss: 0.00625263
INFO:root:[250,   200] training loss: 0.00694631
INFO:root:[250,   250] training loss: 0.00523499
INFO:root:[250,   300] training loss: 0.00675708
INFO:root:[250,   350] training loss: 0.00558589
INFO:root:[250,   400] training loss: 0.00000623
INFO:root:[250,   450] training loss: 0.00000620
INFO:root:[250,   500] training loss: 0.00001484
INFO:root:[250,   550] training loss: 0.00022070
INFO:root:[250,   600] training loss: 0.00009410
INFO:root:[250,   650] training loss: 0.00001301
INFO:root:[250,   700] training loss: 0.00000864
INFO:root:[250,   750] training loss: 0.00013782
INFO:root:[250,   800] training loss: 0.00019169
INFO:root:[250,   850] training loss: 0.00015078
INFO:root:[250,   900] training loss: 0.00297807
INFO:root:[250,   950] training loss: 0.00090279
INFO:root:[250,  1000] training loss: 0.00001924
INFO:root:[250,  1050] training loss: 0.00001623
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch250
INFO:root:[251,    50] training loss: 0.00712523
INFO:root:[251,   100] training loss: 0.00682776
INFO:root:[251,   150] training loss: 0.00630412
INFO:root:[251,   200] training loss: 0.00611093
INFO:root:[251,   250] training loss: 0.00523930
INFO:root:[251,   300] training loss: 0.00694143
INFO:root:[251,   350] training loss: 0.00572672
INFO:root:[251,   400] training loss: 0.00000685
INFO:root:[251,   450] training loss: 0.00000699
INFO:root:[251,   500] training loss: 0.00001060
INFO:root:[251,   550] training loss: 0.00016818
INFO:root:[251,   600] training loss: 0.00010591
INFO:root:[251,   650] training loss: 0.00001700
INFO:root:[251,   700] training loss: 0.00001643
INFO:root:[251,   750] training loss: 0.00010322
INFO:root:[251,   800] training loss: 0.00019507
INFO:root:[251,   850] training loss: 0.00024814
INFO:root:[251,   900] training loss: 0.00264042
INFO:root:[251,   950] training loss: 0.00136770
INFO:root:[251,  1000] training loss: 0.00001941
INFO:root:[251,  1050] training loss: 0.00001776
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch251
INFO:root:[252,    50] training loss: 0.00654568
INFO:root:[252,   100] training loss: 0.00656104
INFO:root:[252,   150] training loss: 0.00654380
INFO:root:[252,   200] training loss: 0.00595589
INFO:root:[252,   250] training loss: 0.00536602
INFO:root:[252,   300] training loss: 0.00643481
INFO:root:[252,   350] training loss: 0.00601798
INFO:root:[252,   400] training loss: 0.00000605
INFO:root:[252,   450] training loss: 0.00000845
INFO:root:[252,   500] training loss: 0.00000951
INFO:root:[252,   550] training loss: 0.00024202
INFO:root:[252,   600] training loss: 0.00011025
INFO:root:[252,   650] training loss: 0.00002053
INFO:root:[252,   700] training loss: 0.00000937
INFO:root:[252,   750] training loss: 0.00009102
INFO:root:[252,   800] training loss: 0.00018851
INFO:root:[252,   850] training loss: 0.00019091
INFO:root:[252,   900] training loss: 0.00254770
INFO:root:[252,   950] training loss: 0.00105947
INFO:root:[252,  1000] training loss: 0.00002288
INFO:root:[252,  1050] training loss: 0.00001565
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch252
INFO:root:[253,    50] training loss: 0.00651302
INFO:root:[253,   100] training loss: 0.00653286
INFO:root:[253,   150] training loss: 0.00674045
INFO:root:[253,   200] training loss: 0.00593235
INFO:root:[253,   250] training loss: 0.00590575
INFO:root:[253,   300] training loss: 0.00662820
INFO:root:[253,   350] training loss: 0.00537311
INFO:root:[253,   400] training loss: 0.00000716
INFO:root:[253,   450] training loss: 0.00000519
INFO:root:[253,   500] training loss: 0.00001969
INFO:root:[253,   550] training loss: 0.00017648
INFO:root:[253,   600] training loss: 0.00011887
INFO:root:[253,   650] training loss: 0.00001669
INFO:root:[253,   700] training loss: 0.00000813
INFO:root:[253,   750] training loss: 0.00009255
INFO:root:[253,   800] training loss: 0.00018169
INFO:root:[253,   850] training loss: 0.00016725
INFO:root:[253,   900] training loss: 0.00457014
INFO:root:[253,   950] training loss: 0.00090806
INFO:root:[253,  1000] training loss: 0.00002185
INFO:root:[253,  1050] training loss: 0.00001521
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch253
INFO:root:[254,    50] training loss: 0.00677478
INFO:root:[254,   100] training loss: 0.00868013
INFO:root:[254,   150] training loss: 0.00610945
INFO:root:[254,   200] training loss: 0.00640187
INFO:root:[254,   250] training loss: 0.00534189
INFO:root:[254,   300] training loss: 0.00752715
INFO:root:[254,   350] training loss: 0.00517245
INFO:root:[254,   400] training loss: 0.00000568
INFO:root:[254,   450] training loss: 0.00000502
INFO:root:[254,   500] training loss: 0.00001376
INFO:root:[254,   550] training loss: 0.00023662
INFO:root:[254,   600] training loss: 0.00011329
INFO:root:[254,   650] training loss: 0.00001118
INFO:root:[254,   700] training loss: 0.00000936
INFO:root:[254,   750] training loss: 0.00017978
INFO:root:[254,   800] training loss: 0.00017342
INFO:root:[254,   850] training loss: 0.00017017
INFO:root:[254,   900] training loss: 0.00278236
INFO:root:[254,   950] training loss: 0.00084237
INFO:root:[254,  1000] training loss: 0.00001979
INFO:root:[254,  1050] training loss: 0.00002244
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch254
INFO:root:[255,    50] training loss: 0.00671898
INFO:root:[255,   100] training loss: 0.00668429
INFO:root:[255,   150] training loss: 0.00681888
INFO:root:[255,   200] training loss: 0.00633745
INFO:root:[255,   250] training loss: 0.00554756
INFO:root:[255,   300] training loss: 0.00698787
INFO:root:[255,   350] training loss: 0.00541337
INFO:root:[255,   400] training loss: 0.00000720
INFO:root:[255,   450] training loss: 0.00000511
INFO:root:[255,   500] training loss: 0.00001524
INFO:root:[255,   550] training loss: 0.00023774
INFO:root:[255,   600] training loss: 0.00007832
INFO:root:[255,   650] training loss: 0.00001001
INFO:root:[255,   700] training loss: 0.00000829
INFO:root:[255,   750] training loss: 0.00010409
INFO:root:[255,   800] training loss: 0.00014915
INFO:root:[255,   850] training loss: 0.00013799
INFO:root:[255,   900] training loss: 0.00321160
INFO:root:[255,   950] training loss: 0.00079318
INFO:root:[255,  1000] training loss: 0.00001986
INFO:root:[255,  1050] training loss: 0.00001544
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch255
INFO:root:[256,    50] training loss: 0.00748179
INFO:root:[256,   100] training loss: 0.00676252
INFO:root:[256,   150] training loss: 0.00625801
INFO:root:[256,   200] training loss: 0.00634532
INFO:root:[256,   250] training loss: 0.00585315
INFO:root:[256,   300] training loss: 0.00664441
INFO:root:[256,   350] training loss: 0.00539928
INFO:root:[256,   400] training loss: 0.00000621
INFO:root:[256,   450] training loss: 0.00000662
INFO:root:[256,   500] training loss: 0.00001161
INFO:root:[256,   550] training loss: 0.00021000
INFO:root:[256,   600] training loss: 0.00009476
INFO:root:[256,   650] training loss: 0.00001293
INFO:root:[256,   700] training loss: 0.00001144
INFO:root:[256,   750] training loss: 0.00011837
INFO:root:[256,   800] training loss: 0.00018657
INFO:root:[256,   850] training loss: 0.00026348
INFO:root:[256,   900] training loss: 0.00293980
INFO:root:[256,   950] training loss: 0.00089370
INFO:root:[256,  1000] training loss: 0.00002121
INFO:root:[256,  1050] training loss: 0.00001530
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch256
INFO:root:[257,    50] training loss: 0.00616783
INFO:root:[257,   100] training loss: 0.00647806
INFO:root:[257,   150] training loss: 0.00641691
INFO:root:[257,   200] training loss: 0.00592597
INFO:root:[257,   250] training loss: 0.00553056
INFO:root:[257,   300] training loss: 0.00675207
INFO:root:[257,   350] training loss: 0.00535467
INFO:root:[257,   400] training loss: 0.00000633
INFO:root:[257,   450] training loss: 0.00000601
INFO:root:[257,   500] training loss: 0.00001210
INFO:root:[257,   550] training loss: 0.00016958
INFO:root:[257,   600] training loss: 0.00010552
INFO:root:[257,   650] training loss: 0.00001185
INFO:root:[257,   700] training loss: 0.00001194
INFO:root:[257,   750] training loss: 0.00011814
INFO:root:[257,   800] training loss: 0.00017782
INFO:root:[257,   850] training loss: 0.00014043
INFO:root:[257,   900] training loss: 0.00275097
INFO:root:[257,   950] training loss: 0.00086741
INFO:root:[257,  1000] training loss: 0.00002014
INFO:root:[257,  1050] training loss: 0.00004391
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch257
INFO:root:[258,    50] training loss: 0.00641072
INFO:root:[258,   100] training loss: 0.00613175
INFO:root:[258,   150] training loss: 0.00637657
INFO:root:[258,   200] training loss: 0.00620029
INFO:root:[258,   250] training loss: 0.00618820
INFO:root:[258,   300] training loss: 0.00721797
INFO:root:[258,   350] training loss: 0.00527119
INFO:root:[258,   400] training loss: 0.00002367
INFO:root:[258,   450] training loss: 0.00000458
INFO:root:[258,   500] training loss: 0.00001436
INFO:root:[258,   550] training loss: 0.00017451
INFO:root:[258,   600] training loss: 0.00011350
INFO:root:[258,   650] training loss: 0.00001390
INFO:root:[258,   700] training loss: 0.00001272
INFO:root:[258,   750] training loss: 0.00010871
INFO:root:[258,   800] training loss: 0.00016573
INFO:root:[258,   850] training loss: 0.00016161
INFO:root:[258,   900] training loss: 0.00292414
INFO:root:[258,   950] training loss: 0.00119587
INFO:root:[258,  1000] training loss: 0.00001976
INFO:root:[258,  1050] training loss: 0.00001653
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch258
INFO:root:[259,    50] training loss: 0.00697976
INFO:root:[259,   100] training loss: 0.00639879
INFO:root:[259,   150] training loss: 0.00636650
INFO:root:[259,   200] training loss: 0.00673939
INFO:root:[259,   250] training loss: 0.00555664
INFO:root:[259,   300] training loss: 0.00663658
INFO:root:[259,   350] training loss: 0.00535664
INFO:root:[259,   400] training loss: 0.00000583
INFO:root:[259,   450] training loss: 0.00000565
INFO:root:[259,   500] training loss: 0.00001115
INFO:root:[259,   550] training loss: 0.00017313
INFO:root:[259,   600] training loss: 0.00008963
INFO:root:[259,   650] training loss: 0.00001138
INFO:root:[259,   700] training loss: 0.00000834
INFO:root:[259,   750] training loss: 0.00011031
INFO:root:[259,   800] training loss: 0.00027630
INFO:root:[259,   850] training loss: 0.00023423
INFO:root:[259,   900] training loss: 0.00277938
INFO:root:[259,   950] training loss: 0.00116324
INFO:root:[259,  1000] training loss: 0.00002023
INFO:root:[259,  1050] training loss: 0.00001529
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch259
INFO:root:[260,    50] training loss: 0.00654256
INFO:root:[260,   100] training loss: 0.00608277
INFO:root:[260,   150] training loss: 0.00620545
INFO:root:[260,   200] training loss: 0.00609429
INFO:root:[260,   250] training loss: 0.00528983
INFO:root:[260,   300] training loss: 0.00664898
INFO:root:[260,   350] training loss: 0.00632854
INFO:root:[260,   400] training loss: 0.00000656
INFO:root:[260,   450] training loss: 0.00000832
INFO:root:[260,   500] training loss: 0.00001219
INFO:root:[260,   550] training loss: 0.00023840
INFO:root:[260,   600] training loss: 0.00011183
INFO:root:[260,   650] training loss: 0.00001057
INFO:root:[260,   700] training loss: 0.00000882
INFO:root:[260,   750] training loss: 0.00010044
INFO:root:[260,   800] training loss: 0.00017276
INFO:root:[260,   850] training loss: 0.00015176
INFO:root:[260,   900] training loss: 0.00311682
INFO:root:[260,   950] training loss: 0.00084076
INFO:root:[260,  1000] training loss: 0.00002249
INFO:root:[260,  1050] training loss: 0.00001684
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch260
INFO:root:[261,    50] training loss: 0.00691673
INFO:root:[261,   100] training loss: 0.00666039
INFO:root:[261,   150] training loss: 0.00610526
INFO:root:[261,   200] training loss: 0.00588731
INFO:root:[261,   250] training loss: 0.00544458
INFO:root:[261,   300] training loss: 0.00703088
INFO:root:[261,   350] training loss: 0.00532859
INFO:root:[261,   400] training loss: 0.00000888
INFO:root:[261,   450] training loss: 0.00000600
INFO:root:[261,   500] training loss: 0.00001240
INFO:root:[261,   550] training loss: 0.00019225
INFO:root:[261,   600] training loss: 0.00007435
INFO:root:[261,   650] training loss: 0.00001382
INFO:root:[261,   700] training loss: 0.00001176
INFO:root:[261,   750] training loss: 0.00012127
INFO:root:[261,   800] training loss: 0.00013525
INFO:root:[261,   850] training loss: 0.00015196
INFO:root:[261,   900] training loss: 0.00283738
INFO:root:[261,   950] training loss: 0.00070829
INFO:root:[261,  1000] training loss: 0.00002061
INFO:root:[261,  1050] training loss: 0.00003073
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch261
INFO:root:[262,    50] training loss: 0.00784874
INFO:root:[262,   100] training loss: 0.00720890
INFO:root:[262,   150] training loss: 0.00728995
INFO:root:[262,   200] training loss: 0.00617352
INFO:root:[262,   250] training loss: 0.00736588
INFO:root:[262,   300] training loss: 0.00696276
INFO:root:[262,   350] training loss: 0.00520137
INFO:root:[262,   400] training loss: 0.00000755
INFO:root:[262,   450] training loss: 0.00000734
INFO:root:[262,   500] training loss: 0.00001324
INFO:root:[262,   550] training loss: 0.00022284
INFO:root:[262,   600] training loss: 0.00007881
INFO:root:[262,   650] training loss: 0.00001793
INFO:root:[262,   700] training loss: 0.00001368
INFO:root:[262,   750] training loss: 0.00010027
INFO:root:[262,   800] training loss: 0.00020185
INFO:root:[262,   850] training loss: 0.00022324
INFO:root:[262,   900] training loss: 0.00308402
INFO:root:[262,   950] training loss: 0.00090897
INFO:root:[262,  1000] training loss: 0.00002560
INFO:root:[262,  1050] training loss: 0.00001628
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch262
INFO:root:[263,    50] training loss: 0.00639577
INFO:root:[263,   100] training loss: 0.00789779
INFO:root:[263,   150] training loss: 0.00740603
INFO:root:[263,   200] training loss: 0.00627532
INFO:root:[263,   250] training loss: 0.00538535
INFO:root:[263,   300] training loss: 0.00651406
INFO:root:[263,   350] training loss: 0.00572506
INFO:root:[263,   400] training loss: 0.00000688
INFO:root:[263,   450] training loss: 0.00000694
INFO:root:[263,   500] training loss: 0.00001255
INFO:root:[263,   550] training loss: 0.00020816
INFO:root:[263,   600] training loss: 0.00011066
INFO:root:[263,   650] training loss: 0.00001413
INFO:root:[263,   700] training loss: 0.00000903
INFO:root:[263,   750] training loss: 0.00010185
INFO:root:[263,   800] training loss: 0.00020130
INFO:root:[263,   850] training loss: 0.00015575
INFO:root:[263,   900] training loss: 0.00302805
INFO:root:[263,   950] training loss: 0.00077394
INFO:root:[263,  1000] training loss: 0.00002188
INFO:root:[263,  1050] training loss: 0.00001684
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch263
INFO:root:[264,    50] training loss: 0.00655451
INFO:root:[264,   100] training loss: 0.00618617
INFO:root:[264,   150] training loss: 0.00649332
INFO:root:[264,   200] training loss: 0.00584795
INFO:root:[264,   250] training loss: 0.00550028
INFO:root:[264,   300] training loss: 0.00662455
INFO:root:[264,   350] training loss: 0.00520510
INFO:root:[264,   400] training loss: 0.00000656
INFO:root:[264,   450] training loss: 0.00000637
INFO:root:[264,   500] training loss: 0.00001191
INFO:root:[264,   550] training loss: 0.00018615
INFO:root:[264,   600] training loss: 0.00009550
INFO:root:[264,   650] training loss: 0.00001888
INFO:root:[264,   700] training loss: 0.00001357
INFO:root:[264,   750] training loss: 0.00011187
INFO:root:[264,   800] training loss: 0.00016817
INFO:root:[264,   850] training loss: 0.00018264
INFO:root:[264,   900] training loss: 0.00289831
INFO:root:[264,   950] training loss: 0.00092974
INFO:root:[264,  1000] training loss: 0.00001789
INFO:root:[264,  1050] training loss: 0.00001589
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch264
INFO:root:[265,    50] training loss: 0.00655915
INFO:root:[265,   100] training loss: 0.00738768
INFO:root:[265,   150] training loss: 0.00615295
INFO:root:[265,   200] training loss: 0.00610592
INFO:root:[265,   250] training loss: 0.00536849
INFO:root:[265,   300] training loss: 0.00834486
INFO:root:[265,   350] training loss: 0.00541285
INFO:root:[265,   400] training loss: 0.00000473
INFO:root:[265,   450] training loss: 0.00000855
INFO:root:[265,   500] training loss: 0.00001475
INFO:root:[265,   550] training loss: 0.00017781
INFO:root:[265,   600] training loss: 0.00012983
INFO:root:[265,   650] training loss: 0.00001365
INFO:root:[265,   700] training loss: 0.00001060
INFO:root:[265,   750] training loss: 0.00011919
INFO:root:[265,   800] training loss: 0.00023061
INFO:root:[265,   850] training loss: 0.00017332
INFO:root:[265,   900] training loss: 0.00283421
INFO:root:[265,   950] training loss: 0.00093004
INFO:root:[265,  1000] training loss: 0.00002098
INFO:root:[265,  1050] training loss: 0.00001542
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch265
INFO:root:[266,    50] training loss: 0.00655532
INFO:root:[266,   100] training loss: 0.00603393
INFO:root:[266,   150] training loss: 0.00633585
INFO:root:[266,   200] training loss: 0.00651510
INFO:root:[266,   250] training loss: 0.00553277
INFO:root:[266,   300] training loss: 0.00704354
INFO:root:[266,   350] training loss: 0.00573737
INFO:root:[266,   400] training loss: 0.00000613
INFO:root:[266,   450] training loss: 0.00000690
INFO:root:[266,   500] training loss: 0.00001133
INFO:root:[266,   550] training loss: 0.00019518
INFO:root:[266,   600] training loss: 0.00010873
INFO:root:[266,   650] training loss: 0.00001350
INFO:root:[266,   700] training loss: 0.00000968
INFO:root:[266,   750] training loss: 0.00009119
INFO:root:[266,   800] training loss: 0.00014870
INFO:root:[266,   850] training loss: 0.00019882
INFO:root:[266,   900] training loss: 0.00295178
INFO:root:[266,   950] training loss: 0.00111318
INFO:root:[266,  1000] training loss: 0.00001781
INFO:root:[266,  1050] training loss: 0.00001685
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch266
INFO:root:[267,    50] training loss: 0.00636998
INFO:root:[267,   100] training loss: 0.00637212
INFO:root:[267,   150] training loss: 0.00602505
INFO:root:[267,   200] training loss: 0.00591964
INFO:root:[267,   250] training loss: 0.00608689
INFO:root:[267,   300] training loss: 0.00668884
INFO:root:[267,   350] training loss: 0.00592727
INFO:root:[267,   400] training loss: 0.00000858
INFO:root:[267,   450] training loss: 0.00001042
INFO:root:[267,   500] training loss: 0.00001347
INFO:root:[267,   550] training loss: 0.00019596
INFO:root:[267,   600] training loss: 0.00009930
INFO:root:[267,   650] training loss: 0.00001200
INFO:root:[267,   700] training loss: 0.00001005
INFO:root:[267,   750] training loss: 0.00013602
INFO:root:[267,   800] training loss: 0.00016740
INFO:root:[267,   850] training loss: 0.00016420
INFO:root:[267,   900] training loss: 0.00322122
INFO:root:[267,   950] training loss: 0.00074098
INFO:root:[267,  1000] training loss: 0.00002054
INFO:root:[267,  1050] training loss: 0.00001516
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch267
INFO:root:[268,    50] training loss: 0.00639073
INFO:root:[268,   100] training loss: 0.00697877
INFO:root:[268,   150] training loss: 0.00719339
INFO:root:[268,   200] training loss: 0.00590470
INFO:root:[268,   250] training loss: 0.00544066
INFO:root:[268,   300] training loss: 0.00678925
INFO:root:[268,   350] training loss: 0.00595772
INFO:root:[268,   400] training loss: 0.00001750
INFO:root:[268,   450] training loss: 0.00000556
INFO:root:[268,   500] training loss: 0.00001264
INFO:root:[268,   550] training loss: 0.00019656
INFO:root:[268,   600] training loss: 0.00010495
INFO:root:[268,   650] training loss: 0.00001186
INFO:root:[268,   700] training loss: 0.00000957
INFO:root:[268,   750] training loss: 0.00016558
INFO:root:[268,   800] training loss: 0.00014909
INFO:root:[268,   850] training loss: 0.00023072
INFO:root:[268,   900] training loss: 0.00329825
INFO:root:[268,   950] training loss: 0.00073004
INFO:root:[268,  1000] training loss: 0.00002476
INFO:root:[268,  1050] training loss: 0.00001567
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch268
INFO:root:[269,    50] training loss: 0.00673297
INFO:root:[269,   100] training loss: 0.00656652
INFO:root:[269,   150] training loss: 0.00603156
INFO:root:[269,   200] training loss: 0.00582034
INFO:root:[269,   250] training loss: 0.00584271
INFO:root:[269,   300] training loss: 0.00679746
INFO:root:[269,   350] training loss: 0.00545478
INFO:root:[269,   400] training loss: 0.00000520
INFO:root:[269,   450] training loss: 0.00001003
INFO:root:[269,   500] training loss: 0.00001169
INFO:root:[269,   550] training loss: 0.00021565
INFO:root:[269,   600] training loss: 0.00011564
INFO:root:[269,   650] training loss: 0.00001067
INFO:root:[269,   700] training loss: 0.00001003
INFO:root:[269,   750] training loss: 0.00010704
INFO:root:[269,   800] training loss: 0.00020148
INFO:root:[269,   850] training loss: 0.00016456
INFO:root:[269,   900] training loss: 0.00345842
INFO:root:[269,   950] training loss: 0.00081350
INFO:root:[269,  1000] training loss: 0.00001980
INFO:root:[269,  1050] training loss: 0.00002722
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch269
INFO:root:[270,    50] training loss: 0.00704699
INFO:root:[270,   100] training loss: 0.00665254
INFO:root:[270,   150] training loss: 0.00657720
INFO:root:[270,   200] training loss: 0.00607786
INFO:root:[270,   250] training loss: 0.00548670
INFO:root:[270,   300] training loss: 0.00659396
INFO:root:[270,   350] training loss: 0.00535287
INFO:root:[270,   400] training loss: 0.00000575
INFO:root:[270,   450] training loss: 0.00000680
INFO:root:[270,   500] training loss: 0.00001200
INFO:root:[270,   550] training loss: 0.00019520
INFO:root:[270,   600] training loss: 0.00010114
INFO:root:[270,   650] training loss: 0.00001083
INFO:root:[270,   700] training loss: 0.00001307
INFO:root:[270,   750] training loss: 0.00010874
INFO:root:[270,   800] training loss: 0.00019581
INFO:root:[270,   850] training loss: 0.00020153
INFO:root:[270,   900] training loss: 0.00297718
INFO:root:[270,   950] training loss: 0.00075544
INFO:root:[270,  1000] training loss: 0.00002241
INFO:root:[270,  1050] training loss: 0.00001915
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch270
INFO:root:[271,    50] training loss: 0.00667286
INFO:root:[271,   100] training loss: 0.00670783
INFO:root:[271,   150] training loss: 0.00645389
INFO:root:[271,   200] training loss: 0.00631826
INFO:root:[271,   250] training loss: 0.00532216
INFO:root:[271,   300] training loss: 0.00665354
INFO:root:[271,   350] training loss: 0.00599338
INFO:root:[271,   400] training loss: 0.00000520
INFO:root:[271,   450] training loss: 0.00000872
INFO:root:[271,   500] training loss: 0.00001309
INFO:root:[271,   550] training loss: 0.00018570
INFO:root:[271,   600] training loss: 0.00008284
INFO:root:[271,   650] training loss: 0.00002001
INFO:root:[271,   700] training loss: 0.00000996
INFO:root:[271,   750] training loss: 0.00012604
INFO:root:[271,   800] training loss: 0.00018595
INFO:root:[271,   850] training loss: 0.00021386
INFO:root:[271,   900] training loss: 0.00253330
INFO:root:[271,   950] training loss: 0.00080284
INFO:root:[271,  1000] training loss: 0.00002045
INFO:root:[271,  1050] training loss: 0.00001601
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch271
INFO:root:[272,    50] training loss: 0.00628185
INFO:root:[272,   100] training loss: 0.00682895
INFO:root:[272,   150] training loss: 0.00628763
INFO:root:[272,   200] training loss: 0.00604288
INFO:root:[272,   250] training loss: 0.00517723
INFO:root:[272,   300] training loss: 0.00667224
INFO:root:[272,   350] training loss: 0.00529972
INFO:root:[272,   400] training loss: 0.00000593
INFO:root:[272,   450] training loss: 0.00000906
INFO:root:[272,   500] training loss: 0.00001279
INFO:root:[272,   550] training loss: 0.00015126
INFO:root:[272,   600] training loss: 0.00009897
INFO:root:[272,   650] training loss: 0.00002139
INFO:root:[272,   700] training loss: 0.00001152
INFO:root:[272,   750] training loss: 0.00013947
INFO:root:[272,   800] training loss: 0.00019646
INFO:root:[272,   850] training loss: 0.00013772
INFO:root:[272,   900] training loss: 0.00291482
INFO:root:[272,   950] training loss: 0.00080851
INFO:root:[272,  1000] training loss: 0.00002345
INFO:root:[272,  1050] training loss: 0.00001700
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch272
INFO:root:[273,    50] training loss: 0.00638401
INFO:root:[273,   100] training loss: 0.00652505
INFO:root:[273,   150] training loss: 0.00632500
INFO:root:[273,   200] training loss: 0.00580531
INFO:root:[273,   250] training loss: 0.00561300
INFO:root:[273,   300] training loss: 0.00677041
INFO:root:[273,   350] training loss: 0.00551740
INFO:root:[273,   400] training loss: 0.00001939
INFO:root:[273,   450] training loss: 0.00000702
INFO:root:[273,   500] training loss: 0.00001156
INFO:root:[273,   550] training loss: 0.00029205
INFO:root:[273,   600] training loss: 0.00009201
INFO:root:[273,   650] training loss: 0.00001241
INFO:root:[273,   700] training loss: 0.00001486
INFO:root:[273,   750] training loss: 0.00013067
INFO:root:[273,   800] training loss: 0.00021129
INFO:root:[273,   850] training loss: 0.00016407
INFO:root:[273,   900] training loss: 0.00257034
INFO:root:[273,   950] training loss: 0.00107091
INFO:root:[273,  1000] training loss: 0.00002051
INFO:root:[273,  1050] training loss: 0.00001612
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch273
INFO:root:[274,    50] training loss: 0.00666070
INFO:root:[274,   100] training loss: 0.00668613
INFO:root:[274,   150] training loss: 0.00647710
INFO:root:[274,   200] training loss: 0.00684171
INFO:root:[274,   250] training loss: 0.00542043
INFO:root:[274,   300] training loss: 0.00664822
INFO:root:[274,   350] training loss: 0.00519040
INFO:root:[274,   400] training loss: 0.00000690
INFO:root:[274,   450] training loss: 0.00000983
INFO:root:[274,   500] training loss: 0.00001541
INFO:root:[274,   550] training loss: 0.00020137
INFO:root:[274,   600] training loss: 0.00011227
INFO:root:[274,   650] training loss: 0.00001169
INFO:root:[274,   700] training loss: 0.00001010
INFO:root:[274,   750] training loss: 0.00013981
INFO:root:[274,   800] training loss: 0.00014130
INFO:root:[274,   850] training loss: 0.00018286
INFO:root:[274,   900] training loss: 0.00342794
INFO:root:[274,   950] training loss: 0.00079298
INFO:root:[274,  1000] training loss: 0.00002216
INFO:root:[274,  1050] training loss: 0.00001769
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch274
INFO:root:[275,    50] training loss: 0.00665046
INFO:root:[275,   100] training loss: 0.00709615
INFO:root:[275,   150] training loss: 0.00702197
INFO:root:[275,   200] training loss: 0.00615248
INFO:root:[275,   250] training loss: 0.00560394
INFO:root:[275,   300] training loss: 0.00675744
INFO:root:[275,   350] training loss: 0.00524400
INFO:root:[275,   400] training loss: 0.00000744
INFO:root:[275,   450] training loss: 0.00000582
INFO:root:[275,   500] training loss: 0.00001459
INFO:root:[275,   550] training loss: 0.00022382
INFO:root:[275,   600] training loss: 0.00010793
INFO:root:[275,   650] training loss: 0.00001089
INFO:root:[275,   700] training loss: 0.00001184
INFO:root:[275,   750] training loss: 0.00012738
INFO:root:[275,   800] training loss: 0.00010889
INFO:root:[275,   850] training loss: 0.00016516
INFO:root:[275,   900] training loss: 0.00281375
INFO:root:[275,   950] training loss: 0.00069092
INFO:root:[275,  1000] training loss: 0.00002091
INFO:root:[275,  1050] training loss: 0.00002409
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch275
INFO:root:[276,    50] training loss: 0.00640393
INFO:root:[276,   100] training loss: 0.00695594
INFO:root:[276,   150] training loss: 0.00654248
INFO:root:[276,   200] training loss: 0.00592050
INFO:root:[276,   250] training loss: 0.00553956
INFO:root:[276,   300] training loss: 0.00666455
INFO:root:[276,   350] training loss: 0.00552768
INFO:root:[276,   400] training loss: 0.00000486
INFO:root:[276,   450] training loss: 0.00000561
INFO:root:[276,   500] training loss: 0.00001366
INFO:root:[276,   550] training loss: 0.00022294
INFO:root:[276,   600] training loss: 0.00011133
INFO:root:[276,   650] training loss: 0.00001681
INFO:root:[276,   700] training loss: 0.00000966
INFO:root:[276,   750] training loss: 0.00009853
INFO:root:[276,   800] training loss: 0.00017025
INFO:root:[276,   850] training loss: 0.00026947
INFO:root:[276,   900] training loss: 0.00296212
INFO:root:[276,   950] training loss: 0.00075659
INFO:root:[276,  1000] training loss: 0.00002742
INFO:root:[276,  1050] training loss: 0.00001602
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch276
INFO:root:[277,    50] training loss: 0.00660444
INFO:root:[277,   100] training loss: 0.00629785
INFO:root:[277,   150] training loss: 0.00644172
INFO:root:[277,   200] training loss: 0.00594449
INFO:root:[277,   250] training loss: 0.00552402
INFO:root:[277,   300] training loss: 0.00719805
INFO:root:[277,   350] training loss: 0.00519922
INFO:root:[277,   400] training loss: 0.00001360
INFO:root:[277,   450] training loss: 0.00000658
INFO:root:[277,   500] training loss: 0.00001188
INFO:root:[277,   550] training loss: 0.00015956
INFO:root:[277,   600] training loss: 0.00009361
INFO:root:[277,   650] training loss: 0.00001189
INFO:root:[277,   700] training loss: 0.00000920
INFO:root:[277,   750] training loss: 0.00009851
INFO:root:[277,   800] training loss: 0.00022567
INFO:root:[277,   850] training loss: 0.00015655
INFO:root:[277,   900] training loss: 0.00269974
INFO:root:[277,   950] training loss: 0.00131831
INFO:root:[277,  1000] training loss: 0.00002100
INFO:root:[277,  1050] training loss: 0.00001864
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch277
INFO:root:[278,    50] training loss: 0.00647652
INFO:root:[278,   100] training loss: 0.00613485
INFO:root:[278,   150] training loss: 0.00655707
INFO:root:[278,   200] training loss: 0.00629816
INFO:root:[278,   250] training loss: 0.00544748
INFO:root:[278,   300] training loss: 0.00705704
INFO:root:[278,   350] training loss: 0.00543630
INFO:root:[278,   400] training loss: 0.00000770
INFO:root:[278,   450] training loss: 0.00000670
INFO:root:[278,   500] training loss: 0.00001020
INFO:root:[278,   550] training loss: 0.00021878
INFO:root:[278,   600] training loss: 0.00008915
INFO:root:[278,   650] training loss: 0.00001558
INFO:root:[278,   700] training loss: 0.00001007
INFO:root:[278,   750] training loss: 0.00009165
INFO:root:[278,   800] training loss: 0.00017982
INFO:root:[278,   850] training loss: 0.00018285
INFO:root:[278,   900] training loss: 0.00288586
INFO:root:[278,   950] training loss: 0.00089292
INFO:root:[278,  1000] training loss: 0.00002207
INFO:root:[278,  1050] training loss: 0.00002609
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch278
INFO:root:[279,    50] training loss: 0.00641170
INFO:root:[279,   100] training loss: 0.00648645
INFO:root:[279,   150] training loss: 0.00842578
INFO:root:[279,   200] training loss: 0.00601921
INFO:root:[279,   250] training loss: 0.00559858
INFO:root:[279,   300] training loss: 0.00704640
INFO:root:[279,   350] training loss: 0.00581681
INFO:root:[279,   400] training loss: 0.00000603
INFO:root:[279,   450] training loss: 0.00000792
INFO:root:[279,   500] training loss: 0.00001056
INFO:root:[279,   550] training loss: 0.00020986
INFO:root:[279,   600] training loss: 0.00009926
INFO:root:[279,   650] training loss: 0.00001152
INFO:root:[279,   700] training loss: 0.00001097
INFO:root:[279,   750] training loss: 0.00011198
INFO:root:[279,   800] training loss: 0.00015793
INFO:root:[279,   850] training loss: 0.00019687
INFO:root:[279,   900] training loss: 0.00303639
INFO:root:[279,   950] training loss: 0.00097951
INFO:root:[279,  1000] training loss: 0.00002249
INFO:root:[279,  1050] training loss: 0.00001848
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch279
INFO:root:[280,    50] training loss: 0.00635194
INFO:root:[280,   100] training loss: 0.00806568
INFO:root:[280,   150] training loss: 0.00626539
INFO:root:[280,   200] training loss: 0.00609962
INFO:root:[280,   250] training loss: 0.00545766
INFO:root:[280,   300] training loss: 0.00669046
INFO:root:[280,   350] training loss: 0.00518577
INFO:root:[280,   400] training loss: 0.00000664
INFO:root:[280,   450] training loss: 0.00000531
INFO:root:[280,   500] training loss: 0.00001241
INFO:root:[280,   550] training loss: 0.00016933
INFO:root:[280,   600] training loss: 0.00008947
INFO:root:[280,   650] training loss: 0.00001357
INFO:root:[280,   700] training loss: 0.00000888
INFO:root:[280,   750] training loss: 0.00010192
INFO:root:[280,   800] training loss: 0.00015127
INFO:root:[280,   850] training loss: 0.00028270
INFO:root:[280,   900] training loss: 0.00269806
INFO:root:[280,   950] training loss: 0.00136836
INFO:root:[280,  1000] training loss: 0.00002096
INFO:root:[280,  1050] training loss: 0.00001590
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch280
INFO:root:[281,    50] training loss: 0.00679910
INFO:root:[281,   100] training loss: 0.00632183
INFO:root:[281,   150] training loss: 0.00627424
INFO:root:[281,   200] training loss: 0.00631021
INFO:root:[281,   250] training loss: 0.00609370
INFO:root:[281,   300] training loss: 0.00704091
INFO:root:[281,   350] training loss: 0.00528802
INFO:root:[281,   400] training loss: 0.00000962
INFO:root:[281,   450] training loss: 0.00000695
INFO:root:[281,   500] training loss: 0.00001112
INFO:root:[281,   550] training loss: 0.00019976
INFO:root:[281,   600] training loss: 0.00008768
INFO:root:[281,   650] training loss: 0.00001370
INFO:root:[281,   700] training loss: 0.00000840
INFO:root:[281,   750] training loss: 0.00010931
INFO:root:[281,   800] training loss: 0.00022141
INFO:root:[281,   850] training loss: 0.00012583
INFO:root:[281,   900] training loss: 0.00241100
INFO:root:[281,   950] training loss: 0.00080275
INFO:root:[281,  1000] training loss: 0.00002154
INFO:root:[281,  1050] training loss: 0.00001987
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch281
INFO:root:[282,    50] training loss: 0.00669123
INFO:root:[282,   100] training loss: 0.00675317
INFO:root:[282,   150] training loss: 0.00694065
INFO:root:[282,   200] training loss: 0.00628528
INFO:root:[282,   250] training loss: 0.00540660
INFO:root:[282,   300] training loss: 0.00732060
INFO:root:[282,   350] training loss: 0.00541944
INFO:root:[282,   400] training loss: 0.00000838
INFO:root:[282,   450] training loss: 0.00000746
INFO:root:[282,   500] training loss: 0.00001680
INFO:root:[282,   550] training loss: 0.00022637
INFO:root:[282,   600] training loss: 0.00008607
INFO:root:[282,   650] training loss: 0.00001250
INFO:root:[282,   700] training loss: 0.00000923
INFO:root:[282,   750] training loss: 0.00014411
INFO:root:[282,   800] training loss: 0.00015915
INFO:root:[282,   850] training loss: 0.00015991
INFO:root:[282,   900] training loss: 0.00262103
INFO:root:[282,   950] training loss: 0.00081759
INFO:root:[282,  1000] training loss: 0.00002021
INFO:root:[282,  1050] training loss: 0.00001874
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch282
INFO:root:[283,    50] training loss: 0.00683171
INFO:root:[283,   100] training loss: 0.00657838
INFO:root:[283,   150] training loss: 0.00746847
INFO:root:[283,   200] training loss: 0.00581728
INFO:root:[283,   250] training loss: 0.00544677
INFO:root:[283,   300] training loss: 0.00704626
INFO:root:[283,   350] training loss: 0.00560599
INFO:root:[283,   400] training loss: 0.00000557
INFO:root:[283,   450] training loss: 0.00000551
INFO:root:[283,   500] training loss: 0.00001181
INFO:root:[283,   550] training loss: 0.00022158
INFO:root:[283,   600] training loss: 0.00008570
INFO:root:[283,   650] training loss: 0.00001101
INFO:root:[283,   700] training loss: 0.00000871
INFO:root:[283,   750] training loss: 0.00010747
INFO:root:[283,   800] training loss: 0.00020517
INFO:root:[283,   850] training loss: 0.00016895
INFO:root:[283,   900] training loss: 0.00277456
INFO:root:[283,   950] training loss: 0.00076718
INFO:root:[283,  1000] training loss: 0.00002086
INFO:root:[283,  1050] training loss: 0.00001797
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch283
INFO:root:[284,    50] training loss: 0.00668464
INFO:root:[284,   100] training loss: 0.00613604
INFO:root:[284,   150] training loss: 0.00614236
INFO:root:[284,   200] training loss: 0.00592990
INFO:root:[284,   250] training loss: 0.00602957
INFO:root:[284,   300] training loss: 0.00694512
INFO:root:[284,   350] training loss: 0.00543884
INFO:root:[284,   400] training loss: 0.00000655
INFO:root:[284,   450] training loss: 0.00000571
INFO:root:[284,   500] training loss: 0.00001278
INFO:root:[284,   550] training loss: 0.00019158
INFO:root:[284,   600] training loss: 0.00009820
INFO:root:[284,   650] training loss: 0.00001684
INFO:root:[284,   700] training loss: 0.00000872
INFO:root:[284,   750] training loss: 0.00013713
INFO:root:[284,   800] training loss: 0.00018142
INFO:root:[284,   850] training loss: 0.00018693
INFO:root:[284,   900] training loss: 0.00315624
INFO:root:[284,   950] training loss: 0.00074703
INFO:root:[284,  1000] training loss: 0.00002119
INFO:root:[284,  1050] training loss: 0.00001771
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch284
INFO:root:[285,    50] training loss: 0.00699027
INFO:root:[285,   100] training loss: 0.00632799
INFO:root:[285,   150] training loss: 0.00660202
INFO:root:[285,   200] training loss: 0.00592138
INFO:root:[285,   250] training loss: 0.00549354
INFO:root:[285,   300] training loss: 0.00736342
INFO:root:[285,   350] training loss: 0.00529522
INFO:root:[285,   400] training loss: 0.00000697
INFO:root:[285,   450] training loss: 0.00000823
INFO:root:[285,   500] training loss: 0.00001225
INFO:root:[285,   550] training loss: 0.00018862
INFO:root:[285,   600] training loss: 0.00009861
INFO:root:[285,   650] training loss: 0.00001086
INFO:root:[285,   700] training loss: 0.00001041
INFO:root:[285,   750] training loss: 0.00014892
INFO:root:[285,   800] training loss: 0.00019403
INFO:root:[285,   850] training loss: 0.00027879
INFO:root:[285,   900] training loss: 0.00263230
INFO:root:[285,   950] training loss: 0.00079032
INFO:root:[285,  1000] training loss: 0.00002169
INFO:root:[285,  1050] training loss: 0.00003984
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch285
INFO:root:[286,    50] training loss: 0.00662291
INFO:root:[286,   100] training loss: 0.00622897
INFO:root:[286,   150] training loss: 0.00640904
INFO:root:[286,   200] training loss: 0.00597460
INFO:root:[286,   250] training loss: 0.00630603
INFO:root:[286,   300] training loss: 0.00680787
INFO:root:[286,   350] training loss: 0.00565667
INFO:root:[286,   400] training loss: 0.00000670
INFO:root:[286,   450] training loss: 0.00000729
INFO:root:[286,   500] training loss: 0.00001189
INFO:root:[286,   550] training loss: 0.00022935
INFO:root:[286,   600] training loss: 0.00011213
INFO:root:[286,   650] training loss: 0.00002206
INFO:root:[286,   700] training loss: 0.00001008
INFO:root:[286,   750] training loss: 0.00012665
INFO:root:[286,   800] training loss: 0.00016099
INFO:root:[286,   850] training loss: 0.00018409
INFO:root:[286,   900] training loss: 0.00273325
INFO:root:[286,   950] training loss: 0.00105462
INFO:root:[286,  1000] training loss: 0.00002095
INFO:root:[286,  1050] training loss: 0.00001929
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch286
INFO:root:[287,    50] training loss: 0.00647411
INFO:root:[287,   100] training loss: 0.00790424
INFO:root:[287,   150] training loss: 0.00624708
INFO:root:[287,   200] training loss: 0.00592222
INFO:root:[287,   250] training loss: 0.00509546
INFO:root:[287,   300] training loss: 0.00676443
INFO:root:[287,   350] training loss: 0.00536767
INFO:root:[287,   400] training loss: 0.00000982
INFO:root:[287,   450] training loss: 0.00000544
INFO:root:[287,   500] training loss: 0.00001555
INFO:root:[287,   550] training loss: 0.00018210
INFO:root:[287,   600] training loss: 0.00010322
INFO:root:[287,   650] training loss: 0.00001190
INFO:root:[287,   700] training loss: 0.00001009
INFO:root:[287,   750] training loss: 0.00010843
INFO:root:[287,   800] training loss: 0.00016314
INFO:root:[287,   850] training loss: 0.00017410
INFO:root:[287,   900] training loss: 0.00343628
INFO:root:[287,   950] training loss: 0.00069428
INFO:root:[287,  1000] training loss: 0.00001987
INFO:root:[287,  1050] training loss: 0.00001548
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch287
INFO:root:[288,    50] training loss: 0.00705349
INFO:root:[288,   100] training loss: 0.00638170
INFO:root:[288,   150] training loss: 0.00641263
INFO:root:[288,   200] training loss: 0.00600723
INFO:root:[288,   250] training loss: 0.00517026
INFO:root:[288,   300] training loss: 0.00650311
INFO:root:[288,   350] training loss: 0.00566523
INFO:root:[288,   400] training loss: 0.00007004
INFO:root:[288,   450] training loss: 0.00000799
INFO:root:[288,   500] training loss: 0.00001513
INFO:root:[288,   550] training loss: 0.00020360
INFO:root:[288,   600] training loss: 0.00009962
INFO:root:[288,   650] training loss: 0.00001366
INFO:root:[288,   700] training loss: 0.00001069
INFO:root:[288,   750] training loss: 0.00010724
INFO:root:[288,   800] training loss: 0.00021666
INFO:root:[288,   850] training loss: 0.00013027
INFO:root:[288,   900] training loss: 0.00288528
INFO:root:[288,   950] training loss: 0.00098378
INFO:root:[288,  1000] training loss: 0.00001939
INFO:root:[288,  1050] training loss: 0.00001478
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch288
INFO:root:[289,    50] training loss: 0.00664194
INFO:root:[289,   100] training loss: 0.00863221
INFO:root:[289,   150] training loss: 0.00699266
INFO:root:[289,   200] training loss: 0.00629228
INFO:root:[289,   250] training loss: 0.00564655
INFO:root:[289,   300] training loss: 0.00689688
INFO:root:[289,   350] training loss: 0.00542385
INFO:root:[289,   400] training loss: 0.00000541
INFO:root:[289,   450] training loss: 0.00000532
INFO:root:[289,   500] training loss: 0.00001215
INFO:root:[289,   550] training loss: 0.00023397
INFO:root:[289,   600] training loss: 0.00007936
INFO:root:[289,   650] training loss: 0.00001148
INFO:root:[289,   700] training loss: 0.00001308
INFO:root:[289,   750] training loss: 0.00016402
INFO:root:[289,   800] training loss: 0.00014818
INFO:root:[289,   850] training loss: 0.00019326
INFO:root:[289,   900] training loss: 0.00275278
INFO:root:[289,   950] training loss: 0.00078259
INFO:root:[289,  1000] training loss: 0.00002325
INFO:root:[289,  1050] training loss: 0.00001579
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch289
INFO:root:[290,    50] training loss: 0.00645972
INFO:root:[290,   100] training loss: 0.00606998
INFO:root:[290,   150] training loss: 0.00609054
INFO:root:[290,   200] training loss: 0.00592114
INFO:root:[290,   250] training loss: 0.00536727
INFO:root:[290,   300] training loss: 0.00687005
INFO:root:[290,   350] training loss: 0.00522044
INFO:root:[290,   400] training loss: 0.00000615
INFO:root:[290,   450] training loss: 0.00000514
INFO:root:[290,   500] training loss: 0.00017385
INFO:root:[290,   550] training loss: 0.00021456
INFO:root:[290,   600] training loss: 0.00012642
INFO:root:[290,   650] training loss: 0.00001487
INFO:root:[290,   700] training loss: 0.00000928
INFO:root:[290,   750] training loss: 0.00015228
INFO:root:[290,   800] training loss: 0.00015655
INFO:root:[290,   850] training loss: 0.00018190
INFO:root:[290,   900] training loss: 0.00281577
INFO:root:[290,   950] training loss: 0.00116343
INFO:root:[290,  1000] training loss: 0.00002128
INFO:root:[290,  1050] training loss: 0.00001599
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch290
INFO:root:[291,    50] training loss: 0.00627495
INFO:root:[291,   100] training loss: 0.00868146
INFO:root:[291,   150] training loss: 0.00657738
INFO:root:[291,   200] training loss: 0.00594658
INFO:root:[291,   250] training loss: 0.00618911
INFO:root:[291,   300] training loss: 0.00654622
INFO:root:[291,   350] training loss: 0.00545512
INFO:root:[291,   400] training loss: 0.00000627
INFO:root:[291,   450] training loss: 0.00000579
INFO:root:[291,   500] training loss: 0.00001587
INFO:root:[291,   550] training loss: 0.00021737
INFO:root:[291,   600] training loss: 0.00010100
INFO:root:[291,   650] training loss: 0.00001260
INFO:root:[291,   700] training loss: 0.00001126
INFO:root:[291,   750] training loss: 0.00023503
INFO:root:[291,   800] training loss: 0.00018346
INFO:root:[291,   850] training loss: 0.00018331
INFO:root:[291,   900] training loss: 0.00266758
INFO:root:[291,   950] training loss: 0.00079641
INFO:root:[291,  1000] training loss: 0.00002369
INFO:root:[291,  1050] training loss: 0.00001771
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch291
INFO:root:[292,    50] training loss: 0.00654073
INFO:root:[292,   100] training loss: 0.00644178
INFO:root:[292,   150] training loss: 0.00635399
INFO:root:[292,   200] training loss: 0.00605407
INFO:root:[292,   250] training loss: 0.00593752
INFO:root:[292,   300] training loss: 0.00691032
INFO:root:[292,   350] training loss: 0.00558290
INFO:root:[292,   400] training loss: 0.00000567
INFO:root:[292,   450] training loss: 0.00000748
INFO:root:[292,   500] training loss: 0.00001075
INFO:root:[292,   550] training loss: 0.00018703
INFO:root:[292,   600] training loss: 0.00008695
INFO:root:[292,   650] training loss: 0.00001432
INFO:root:[292,   700] training loss: 0.00000946
INFO:root:[292,   750] training loss: 0.00011990
INFO:root:[292,   800] training loss: 0.00014432
INFO:root:[292,   850] training loss: 0.00018943
INFO:root:[292,   900] training loss: 0.00286270
INFO:root:[292,   950] training loss: 0.00079722
INFO:root:[292,  1000] training loss: 0.00002076
INFO:root:[292,  1050] training loss: 0.00001846
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch292
INFO:root:[293,    50] training loss: 0.00632740
INFO:root:[293,   100] training loss: 0.00675909
INFO:root:[293,   150] training loss: 0.00661237
INFO:root:[293,   200] training loss: 0.00594524
INFO:root:[293,   250] training loss: 0.00564758
INFO:root:[293,   300] training loss: 0.00659946
INFO:root:[293,   350] training loss: 0.00526987
INFO:root:[293,   400] training loss: 0.00000524
INFO:root:[293,   450] training loss: 0.00000701
INFO:root:[293,   500] training loss: 0.00001359
INFO:root:[293,   550] training loss: 0.00020523
INFO:root:[293,   600] training loss: 0.00012989
INFO:root:[293,   650] training loss: 0.00001135
INFO:root:[293,   700] training loss: 0.00000924
INFO:root:[293,   750] training loss: 0.00017378
INFO:root:[293,   800] training loss: 0.00016328
INFO:root:[293,   850] training loss: 0.00018990
INFO:root:[293,   900] training loss: 0.00254953
INFO:root:[293,   950] training loss: 0.00074558
INFO:root:[293,  1000] training loss: 0.00003156
INFO:root:[293,  1050] training loss: 0.00002147
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch293
INFO:root:[294,    50] training loss: 0.00644913
INFO:root:[294,   100] training loss: 0.00769602
INFO:root:[294,   150] training loss: 0.00748888
INFO:root:[294,   200] training loss: 0.00599703
INFO:root:[294,   250] training loss: 0.00604312
INFO:root:[294,   300] training loss: 0.00650277
INFO:root:[294,   350] training loss: 0.00528080
INFO:root:[294,   400] training loss: 0.00000641
INFO:root:[294,   450] training loss: 0.00000744
INFO:root:[294,   500] training loss: 0.00001399
INFO:root:[294,   550] training loss: 0.00019105
INFO:root:[294,   600] training loss: 0.00015007
INFO:root:[294,   650] training loss: 0.00001310
INFO:root:[294,   700] training loss: 0.00000903
INFO:root:[294,   750] training loss: 0.00012357
INFO:root:[294,   800] training loss: 0.00016547
INFO:root:[294,   850] training loss: 0.00022943
INFO:root:[294,   900] training loss: 0.00246671
INFO:root:[294,   950] training loss: 0.00092273
INFO:root:[294,  1000] training loss: 0.00002013
INFO:root:[294,  1050] training loss: 0.00001593
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch294
INFO:root:[295,    50] training loss: 0.00639245
INFO:root:[295,   100] training loss: 0.00728661
INFO:root:[295,   150] training loss: 0.00625387
INFO:root:[295,   200] training loss: 0.00595376
INFO:root:[295,   250] training loss: 0.00593926
INFO:root:[295,   300] training loss: 0.00667675
INFO:root:[295,   350] training loss: 0.00538230
INFO:root:[295,   400] training loss: 0.00000694
INFO:root:[295,   450] training loss: 0.00000642
INFO:root:[295,   500] training loss: 0.00001141
INFO:root:[295,   550] training loss: 0.00019648
INFO:root:[295,   600] training loss: 0.00012917
INFO:root:[295,   650] training loss: 0.00001000
INFO:root:[295,   700] training loss: 0.00001064
INFO:root:[295,   750] training loss: 0.00012135
INFO:root:[295,   800] training loss: 0.00021663
INFO:root:[295,   850] training loss: 0.00022036
INFO:root:[295,   900] training loss: 0.00278432
INFO:root:[295,   950] training loss: 0.00087553
INFO:root:[295,  1000] training loss: 0.00002233
INFO:root:[295,  1050] training loss: 0.00001549
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch295
INFO:root:[296,    50] training loss: 0.00662023
INFO:root:[296,   100] training loss: 0.00817775
INFO:root:[296,   150] training loss: 0.00612600
INFO:root:[296,   200] training loss: 0.00572448
INFO:root:[296,   250] training loss: 0.00617748
INFO:root:[296,   300] training loss: 0.00662390
INFO:root:[296,   350] training loss: 0.00517716
INFO:root:[296,   400] training loss: 0.00001339
INFO:root:[296,   450] training loss: 0.00000641
INFO:root:[296,   500] training loss: 0.00001079
INFO:root:[296,   550] training loss: 0.00021827
INFO:root:[296,   600] training loss: 0.00009911
INFO:root:[296,   650] training loss: 0.00001242
INFO:root:[296,   700] training loss: 0.00002157
INFO:root:[296,   750] training loss: 0.00017630
INFO:root:[296,   800] training loss: 0.00018816
INFO:root:[296,   850] training loss: 0.00018532
INFO:root:[296,   900] training loss: 0.00269249
INFO:root:[296,   950] training loss: 0.00082738
INFO:root:[296,  1000] training loss: 0.00002176
INFO:root:[296,  1050] training loss: 0.00001757
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch296
INFO:root:[297,    50] training loss: 0.00678605
INFO:root:[297,   100] training loss: 0.00651310
INFO:root:[297,   150] training loss: 0.00624563
INFO:root:[297,   200] training loss: 0.00607008
INFO:root:[297,   250] training loss: 0.00536607
INFO:root:[297,   300] training loss: 0.00675132
INFO:root:[297,   350] training loss: 0.00602227
INFO:root:[297,   400] training loss: 0.00001630
INFO:root:[297,   450] training loss: 0.00000798
INFO:root:[297,   500] training loss: 0.00001212
INFO:root:[297,   550] training loss: 0.00018392
INFO:root:[297,   600] training loss: 0.00008996
INFO:root:[297,   650] training loss: 0.00001192
INFO:root:[297,   700] training loss: 0.00000956
INFO:root:[297,   750] training loss: 0.00013013
INFO:root:[297,   800] training loss: 0.00014202
INFO:root:[297,   850] training loss: 0.00013981
INFO:root:[297,   900] training loss: 0.00315527
INFO:root:[297,   950] training loss: 0.00079387
INFO:root:[297,  1000] training loss: 0.00002469
INFO:root:[297,  1050] training loss: 0.00002020
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch297
INFO:root:[298,    50] training loss: 0.00663054
INFO:root:[298,   100] training loss: 0.00633112
INFO:root:[298,   150] training loss: 0.00606004
INFO:root:[298,   200] training loss: 0.00602653
INFO:root:[298,   250] training loss: 0.00587549
INFO:root:[298,   300] training loss: 0.00689268
INFO:root:[298,   350] training loss: 0.00524061
INFO:root:[298,   400] training loss: 0.00000491
INFO:root:[298,   450] training loss: 0.00000579
INFO:root:[298,   500] training loss: 0.00001379
INFO:root:[298,   550] training loss: 0.00019091
INFO:root:[298,   600] training loss: 0.00010802
INFO:root:[298,   650] training loss: 0.00001114
INFO:root:[298,   700] training loss: 0.00001111
INFO:root:[298,   750] training loss: 0.00011831
INFO:root:[298,   800] training loss: 0.00020403
INFO:root:[298,   850] training loss: 0.00015722
INFO:root:[298,   900] training loss: 0.00256344
INFO:root:[298,   950] training loss: 0.00071081
INFO:root:[298,  1000] training loss: 0.00002279
INFO:root:[298,  1050] training loss: 0.00001672
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch298
INFO:root:[299,    50] training loss: 0.00652935
INFO:root:[299,   100] training loss: 0.00721182
INFO:root:[299,   150] training loss: 0.00726118
INFO:root:[299,   200] training loss: 0.00604580
INFO:root:[299,   250] training loss: 0.00527521
INFO:root:[299,   300] training loss: 0.00855349
INFO:root:[299,   350] training loss: 0.00588366
INFO:root:[299,   400] training loss: 0.00000626
INFO:root:[299,   450] training loss: 0.00000739
INFO:root:[299,   500] training loss: 0.00001422
INFO:root:[299,   550] training loss: 0.00024627
INFO:root:[299,   600] training loss: 0.00009847
INFO:root:[299,   650] training loss: 0.00000959
INFO:root:[299,   700] training loss: 0.00001106
INFO:root:[299,   750] training loss: 0.00012203
INFO:root:[299,   800] training loss: 0.00017678
INFO:root:[299,   850] training loss: 0.00015909
INFO:root:[299,   900] training loss: 0.00262482
INFO:root:[299,   950] training loss: 0.00092142
INFO:root:[299,  1000] training loss: 0.00001953
INFO:root:[299,  1050] training loss: 0.00001736
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:epoch299
INFO:root:[300,    50] training loss: 0.00823154
INFO:root:[300,   100] training loss: 0.00679248
INFO:root:[300,   150] training loss: 0.00687489
INFO:root:[300,   200] training loss: 0.00654507
INFO:root:[300,   250] training loss: 0.00521948
INFO:root:[300,   300] training loss: 0.00695869
INFO:root:[300,   350] training loss: 0.00595508
INFO:root:[300,   400] training loss: 0.00000606
INFO:root:[300,   450] training loss: 0.00000678
INFO:root:[300,   500] training loss: 0.00001346
INFO:root:[300,   550] training loss: 0.00021573
INFO:root:[300,   600] training loss: 0.00009656
INFO:root:[300,   650] training loss: 0.00001138
INFO:root:[300,   700] training loss: 0.00000847
INFO:root:[300,   750] training loss: 0.00016046
INFO:root:[300,   800] training loss: 0.00020535
INFO:root:[300,   850] training loss: 0.00020786
INFO:root:[300,   900] training loss: 0.00290523
INFO:root:[300,   950] training loss: 0.00089267
INFO:root:[300,  1000] training loss: 0.00001957
INFO:root:[300,  1050] training loss: 0.00001835
INFO:root:              precision    recall  f1-score   support

          G2     0.7500    1.0000    0.8571         3
           S     0.9237    0.8159    0.8665      1722
    Prophase     0.8033    0.8527    0.8273      1039
    Anaphase     0.7273    0.8000    0.7619        10
          G1     0.4906    0.7027    0.5778        74
   Metaphase     0.6304    0.6935    0.6604      1018
   Telophase     0.8571    1.0000    0.9231         6

    accuracy                         0.7918      3872
   macro avg     0.7403    0.8378    0.7820      3872
weighted avg     0.8053    0.7918    0.7961      3872

INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6454 test images: 87 %
