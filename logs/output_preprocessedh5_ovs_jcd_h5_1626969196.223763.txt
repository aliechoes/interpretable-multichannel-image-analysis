INFO:root:the deviced being used is cuda:0
INFO:root:statistics used: {'mean': tensor([0.3083, 0.0614, 0.1431]), 'std': tensor([0.1654, 0.1624, 0.1261])}
INFO:root:test_indx used: 8237, 3382, 2361, 23748, 26770, 23949, 26217, 2657, 12907, 12557, 9901, 28753, 26388, 18315, 9953, 1279, 483, 29762, 10375, 15787, 19051, 2360, 15455, 3587, 18292, 15161, 23610, 3242, 14355, 6639, 26804, 8859, 23863, 8840, 14873, 7827, 20791, 4885, 26928, 3376, 13621, 29907, 25215, 22651, 14878, 23525, 26595, 3688, 17113, 26824, 25818, 5062, 30105, 24690, 3709, 28211, 21721, 2321, 25599, 29410, 21245, 31452, 4282, 29187, 24573, 9878, 8000, 3446, 644, 22650, 1335, 13040, 19767, 26185, 16244, 31665, 21214, 13237, 3835, 11641, 4531, 24304, 184, 32152, 2830, 3798, 2016, 5724, 14967, 23031, 4668, 19491, 8718, 24518, 18181, 24615, 24372, 6955, 25187, 19658, 14280, 11520, 7434, 23161, 18419, 9989, 7944, 1516, 13962, 25255, 494, 26071, 15251, 26837, 28784, 9727, 17183, 9126, 18414, 4571, 17614, 3525, 28278, 25052, 1204, 18733, 4201, 22394, 18005, 19869, 4714, 374, 10896, 13991, 20660, 22152, 18343, 5290, 16743, 8031, 16729, 3130, 821, 19211, 17013, 21072, 7150, 7269, 31594, 26072, 21791, 27380, 28244, 3787, 19620, 9722, 14850, 27137, 13317, 5825, 31670, 2478, 12442, 12667, 26485, 605, 15491, 28725, 10831, 22164, 4042, 31518, 20299, 10842, 20721, 23386, 24865, 29958, 6284, 16696, 7578, 7340, 15998, 3155, 281, 2502, 11972, 25068, 18715, 17557, 4092, 27294, 29398, 8727, 24955, 23843, 19948, 25833, 11, 7339, 6752, 26351, 7581, 28703, 22672, 16917, 23766, 10170, 21968, 14618, 23652, 30304, 19502, 6349, 9981, 6105, 29284, 31383, 11484, 723, 17148, 22157, 20793, 30586, 26887, 30567, 16194, 9103, 22575, 31661, 8018, 17678, 31432, 12527, 3405, 17967, 23714, 15446, 11855, 27814, 3005, 29645, 15198, 20116, 13329, 31738, 5200, 14352, 28782, 26009, 32106, 23850, 31657, 21179, 1524, 8133, 8033, 6404, 21969, 21166, 9264, 7751, 17431, 20037, 4184, 27590, 1097, 10211, 18290, 19859, 4984, 18502, 31241, 18262, 9229, 27825, 27387, 7263, 4955, 870, 21805, 15786, 30979, 20794, 639, 26601, 5265, 26069, 2545, 21116, 21041, 25149, 5844, 17633, 31103, 3494, 28466, 27043, 23506, 28407, 17139, 24015, 7542, 10597, 18466, 22357, 2967, 25492, 18219, 27109, 10483, 11859, 14654, 17022, 13340, 31428, 27430, 25244, 5593, 28534, 7121, 4296, 15091, 16119, 3550, 8274, 26397, 11655, 1666, 31688, 18596, 2524, 2381, 719, 26498, 9079, 9534, 28754, 22026, 16144, 11773, 17925, 24116, 1760, 25519, 12858, 29525, 8790, 2742, 19270, 8262, 6767, 2069, 15253, 22758, 11963, 19194, 24101, 1978, 3412, 15526, 14121, 7466, 31993, 16457, 26057, 28962, 20946, 27213, 137, 17838, 12030, 28502, 2225, 1183, 14440, 17182, 19985, 30974, 11158, 16765, 1567, 400, 25547, 7360, 14374, 5812, 2914, 21588, 16245, 12471, 4755, 6323, 2215, 7327, 29560, 17720, 16609, 19360, 11207, 5681, 20109, 5744, 140, 11868, 26510, 27553, 7433, 16271, 2243, 26212, 10574, 22537, 26935, 3485, 18131, 21135, 25744, 3613, 26242, 26295, 31987, 10943, 25884, 22205, 18910, 29941, 28069, 15292, 8037, 26146, 23384, 31834, 20731, 24913, 23944, 5049, 9159, 19341, 23958, 9661, 16330, 26324, 9548, 31580, 12232, 30522, 18473, 17061, 3048, 16233, 28617, 7647, 28866, 10111, 10229, 2701, 18109, 21319, 30064, 12051, 20733, 23915, 22478, 4290, 28408, 21310, 31275, 21329, 13159, 949, 8121, 29711, 10956, 9731, 19449, 12187, 27901, 28046, 3592, 3398, 29417, 956, 29536, 13459, 16834, 30927, 31161, 7440, 27701, 921, 10698, 9701, 8993, 20956, 6401, 30697, 30630, 24897, 11140, 25766, 25677, 18729, 677, 21785, 13228, 5650, 28779, 7529, 19577, 23857, 20048, 9699, 30863, 27881, 7276, 14153, 3045, 21274, 9030, 13789, 8633, 249, 30386, 29530, 4514, 13983, 5277, 29137, 25569, 22050, 25111, 4339, 20419, 2300, 671, 18565, 25020, 23791, 7428, 25437, 15064, 24799, 17860, 24662, 2986, 18849, 27083, 26632, 1184, 28469, 24777, 2955, 22692, 22281, 867, 9221, 9632, 12412, 30733, 14329, 21675, 20750, 8844, 11846, 7220, 30377, 24577, 10747, 23580, 27095, 12135, 27930, 24611, 28134, 22044, 20504, 17339, 12550, 28666, 10789, 25877, 31154, 19596, 6028, 3673, 9118, 23556, 14409, 26784, 24588, 1220, 28837, 10825, 29245, 10296, 9582, 8637, 31477, 9086, 28078, 31469, 2155, 4761, 18816, 28522, 30278, 18710, 975, 30354, 31948, 9149, 5293, 4789, 23950, 4827, 24961, 19281, 30465, 20730, 13300, 2601, 7379, 6820, 10741, 20136, 20663, 9184, 16769, 26328, 13772, 21302, 17365, 1342, 787, 29776, 27359, 31698, 7566, 4660, 4143, 19973, 17155, 6005, 24427, 16474, 1120, 764, 8480, 11008, 30742, 27745, 25212, 29715, 7783, 23804, 4864, 25556, 31339, 208, 21824, 9427, 4309, 521, 20044, 11974, 25074, 1339, 31437, 18033, 30773, 26059, 10655, 447, 18990, 9821, 28070, 28906, 29330, 24968, 7414, 9370, 31829, 25936, 6731, 14539, 13844, 9339, 27196, 16191, 19071, 10363, 10476, 19754, 15283, 14337, 10766, 339, 21477, 21173, 2448, 18339, 11244, 308, 15766, 765, 29901, 28077, 3535, 15338, 22828, 15578, 4500, 819, 18276, 3019, 19510, 26974, 7424, 19996, 19450, 672, 43, 2737, 13853, 13794, 31351, 9165, 9501, 20557, 18040, 1084, 2471, 19255, 22919, 23694, 13367, 8028, 3433, 30088, 26780, 15956, 21210, 21690, 12875, 31730, 10692, 5842, 9594, 12091, 25118, 18204, 28857, 30861, 21221, 11149, 26854, 21416, 28891, 25001, 31601, 31468, 25887, 26342, 25473, 6595, 7842, 3034, 26959, 17427, 8593, 11847, 19756, 24988, 4284, 16031, 8924, 17569, 31732, 10249, 14877, 29777, 17200, 31147, 22507, 15475, 9526, 24957, 3564, 26544, 31188, 12189, 21055, 2085, 14158, 7153, 22049, 6400, 15041, 4344, 6385, 16910, 25777, 25726, 890, 17541, 6539, 30984, 20756, 10749, 9016, 11511, 6930, 2572, 1062, 20790, 1246, 29106, 24920, 15457, 19599, 8776, 31516, 11902, 2124, 445, 8252, 4793, 16407, 32115, 15101, 7224, 14800, 2912, 14378, 25256, 11796, 31839, 3286, 29651, 26856, 21578, 20385, 11181, 14950, 23877, 18848, 17325, 11587, 27987, 17948, 12919, 9040, 21627, 1331, 10316, 11463, 1125, 8886, 10965, 29236, 16358, 26629, 12859, 30481, 26951, 20352, 7585, 23876, 6920, 29743, 7889, 28882, 14080, 6220, 2439, 24463, 30356, 13277, 24309, 23426, 11199, 19674, 21593, 1509, 10940, 19872, 14853, 16858, 7138, 11500, 11470, 26023, 18250, 18548, 18159, 25402, 14600, 29494, 22365, 12826, 12340, 29976, 14112, 19002, 31907, 16441, 23639, 1213, 12305, 7, 20588, 18855, 25038, 15940, 26752, 12096, 15716, 31376, 1648, 6844, 14495, 593, 15031, 12624, 264, 11085, 6709, 10009, 9651, 9122, 6317, 9643, 5309, 2858, 15148, 17961, 14855, 25309, 26257, 31964, 9074, 15236, 11635, 20994, 25121, 25448, 21684, 8631, 32221, 11789, 24459, 1405, 942, 12807, 31060, 16963, 26043, 30780, 30213, 25060, 1186, 15430, 1685, 29815, 5695, 27595, 4028, 11905, 27846, 16273, 26769, 9000, 14923, 5110, 17501, 4614, 30559, 5982, 17597, 10867, 28260, 15895, 26628, 26588, 15006, 4432, 280, 22748, 23038, 16655, 19538, 31861, 4118, 21664, 1455, 19048, 25137, 24894, 17897, 14266, 8191, 6086, 12903, 6366, 27914, 13223, 17081, 32233, 907, 28094, 2581, 15816, 22680, 22372, 10644, 24371, 876, 1529, 8873, 30242, 2077, 19889, 1259, 14545, 5499, 22765, 25642, 15702, 14433, 24972, 31041, 23218, 25082, 3506, 18869, 15137, 20966, 24675, 21209, 54, 18568, 25465, 8431, 6802, 10470, 15193, 30211, 25036, 2630, 7222, 12861, 5763, 9703, 25208, 1274, 882, 3324, 13022, 17172, 7164, 17128, 12687, 30678, 20257, 20881, 15808, 25580, 13952, 7331, 20182, 24107, 1114, 20957, 23824, 14647, 13805, 6109, 12976, 22501, 19992, 30887, 4037, 1140, 1308, 22894, 12148, 16016, 7574, 30023, 21162, 17184, 16651, 15051, 31527, 14714, 27288, 14784, 9589, 16579, 25911, 20414, 30367, 21013, 2740, 7501, 5371, 16490, 5440, 21841, 27838, 26325, 5250, 327, 6413, 7090, 29058, 24303, 3609, 14001, 21340, 32265, 25370, 19466, 24180, 29851, 4406, 6837, 24986, 10973, 16978, 20206, 14854, 13087, 20183, 4751, 15713, 20685, 22504, 13162, 226, 2262, 4404, 7296, 18388, 16644, 7936, 31369, 9573, 2627, 14429, 6344, 26129, 26656, 26038, 22103, 24625, 4508, 10138, 19991, 19402, 24815, 13118, 3760, 14096, 30541, 30017, 888, 2101, 31710, 23882, 32005, 5785, 22821, 29946, 884, 6883, 13857, 2506, 16062, 7375, 24457, 3674, 20917, 30384, 11830, 10693, 26688, 18089, 31439, 10742, 14649, 18612, 13615, 5319, 23928, 24438, 25451, 11072, 24395, 29881, 1719, 16738, 12439, 11049, 29027, 19633, 10953, 16930, 11423, 23983, 8719, 30056, 3336, 1245, 8452, 6535, 17655, 12229, 12584, 22363, 21265, 3911, 5624, 16276, 28825, 8444, 12566, 2008, 8657, 197, 21843, 17639, 23679, 17358, 9207, 6970, 8984, 10804, 1287, 3069, 1494, 15431, 29493, 18481, 22342, 12587, 22341, 11792, 12245, 22291, 14679, 7877, 6341, 4157, 1268, 19277, 31529, 6775, 19657, 19989, 30880, 27847, 1263, 4684, 7354, 15520, 23816, 19803, 22654, 25950, 13597, 10502, 3151, 31989, 6097, 4491, 14972, 11981, 3900, 20458, 7839, 18335, 21827, 11039, 2417, 15752, 15289, 15428, 2172, 26019, 9022, 16209, 29981, 23711, 29471, 32196, 7003, 4357, 23878, 9629, 28272, 23110, 14331, 9687, 23226, 8964, 6586, 16589, 11033, 2213, 4695, 22708, 16692, 15021, 16602, 10411, 11857, 1008, 20482, 169, 4079, 17645, 11130, 22697, 10936, 15233, 17438, 29473, 19417, 15931, 6974, 10270, 2982, 1452, 9474, 2276, 23343, 3620, 25913, 25819, 26457, 5980, 28201, 11376, 9174, 17464, 23196, 31930, 22732, 11209, 4504, 31209, 2120, 23137, 9900, 10208, 22448, 22881, 9347, 18141, 15473, 19815, 13992, 27280, 9434, 16952, 10186, 7870, 13734, 17849, 4393, 21718, 32133, 31471, 20122, 11840, 15648, 22845, 30126, 1537, 12603, 17770, 9664, 20736, 20275, 6650, 2419, 30285, 29721, 15103, 30123, 15965, 15901, 26934, 14777, 13780, 7111, 4685, 15708, 15217, 8130, 21409, 14081, 5345, 28328, 28307, 8740, 22611, 19490, 10365, 305, 18645, 19679, 1278, 8559, 17670, 31783, 23591, 2455, 17802, 27609, 17637, 14265, 13347, 2096, 12298, 19501, 15652, 1054, 13099, 21366, 9626, 27611, 14049, 27500, 24592, 23054, 5334, 27247, 29616, 7069, 1736, 8223, 11255, 14821, 23939, 27517, 6303, 17079, 10271, 8538, 10877, 30461, 27376, 15144, 9862, 20173, 16161, 28308, 26625, 27061, 20523, 18628, 21301, 16976, 23817, 30051, 4166, 23124, 2499, 899, 8503, 26910, 14024, 1089, 5075, 11050, 16518, 15516, 1823, 24143, 17932, 23311, 19800, 12682, 16059, 17271, 6636, 29948, 8685, 29865, 11147, 6507, 18340, 8699, 27835, 3191, 28654, 24357, 29565, 29729, 32173, 26381, 4259, 859, 21410, 4099, 27666, 4008, 8640, 6438, 17987, 19616, 25823, 26298, 1497, 27134, 5936, 18460, 10954, 10460, 5818, 8507, 1726, 32066, 3676, 7490, 8188, 20311, 25115, 31049, 22149, 21529, 7278, 14496, 3645, 1686, 14341, 31828, 5352, 31123, 22588, 1320, 28323, 1700, 9817, 18075, 30782, 24622, 20478, 22780, 31666, 21983, 21206, 3519, 28888, 12729, 25419, 19977, 28277, 29546, 26539, 5356, 23285, 4346, 15392, 16827, 20008, 19126, 26925, 511, 9223, 1348, 11251, 3213, 999, 31089, 25459, 10408, 18149, 8598, 6926, 6651, 16691, 16224, 15098, 20397, 4197, 21960, 18237, 27002, 12373, 12537, 23756, 20188, 6672, 28671, 10453, 29824, 23155, 10472, 5539, 10533, 16403, 2573, 20989, 18479, 1875, 6420, 31838, 5940, 14893, 6403, 4093, 15821, 13207, 18859, 8902, 21138, 25938, 11178, 6122, 13320, 31567, 3219, 18974, 12586, 29304, 15179, 12672, 10769, 22840, 30225, 15026, 30776, 31944, 21513, 29327, 30163, 21596, 10473, 17761, 29773, 26953, 14187, 16114, 18240, 2659, 14567, 15932, 18078, 23515, 26036, 2280, 19670, 20518, 31108, 11668, 23029, 12706, 22295, 31322, 216, 21984, 8330, 2888, 15948, 12509, 29692, 6626, 5688, 22477, 15048, 12287, 22570, 5311, 17419, 11022, 31733, 23337, 1495, 13333, 15227, 14446, 23961, 2458, 8627, 13895, 29778, 23096, 27863, 18238, 17455, 11913, 18213, 31668, 6255, 3729, 22045, 18947, 9237, 25326, 74, 16735, 30467, 32147, 12502, 29900, 27250, 630, 4526, 2513, 15728, 10521, 5141, 9255, 13619, 25487, 18592, 9317, 14479, 570, 31227, 22239, 11617, 11229, 23920, 17587, 17654, 13883, 12786, 19779, 30961, 7240, 10711, 27574, 3892, 3375, 17319, 7972, 11414, 28404, 7037, 31232, 26134, 13975, 7910, 14792, 3970, 29375, 18403, 18377, 15631, 9536, 16344, 26710, 15657, 15499, 15010, 8717, 3093, 27776, 22585, 21090, 12061, 19858, 28165, 21580, 23034, 4276, 10146, 959, 8187, 16082, 15335, 52, 29420, 26015, 30615, 20082, 1007, 24331, 10740, 3995, 27084, 30700, 31765, 25724, 6218, 14881, 18360, 19865, 17240, 30856, 28342, 778, 3431, 18563, 28532, 18202, 9825, 8663, 26386, 28836, 13463, 31412, 28509, 20761, 23237, 6083, 10761, 28817, 4240, 6503, 21736, 5943, 21819, 15037, 7230, 8417, 1908, 13707, 15605, 29001, 23051, 21204, 15426, 8609, 29975, 28313, 15921, 529, 28647, 17248, 25706, 31346, 26054, 21818, 23326, 30632, 27608, 16616, 12261, 3834, 21811, 3499, 30068, 1749, 7767, 798, 31750, 3419, 19110, 23405, 6804, 19293, 11097, 15421, 22126, 13033, 8275, 29730, 16645, 20374, 16552, 7097, 19615, 19956, 18172, 19480, 15820, 28928, 3921, 3393, 12169, 28974, 13750, 25701, 4196, 27099, 28935, 3300, 23310, 26265, 6197, 4502, 27026, 17680, 15237, 4999, 2870, 24077, 6684, 21018, 31082, 20462, 4341, 25312, 25509, 17432, 31484, 11738, 18654, 22921, 12464, 25254, 12474, 9697, 15302, 12841, 12408, 9719, 7306, 6725, 8085, 550, 27644, 10350, 25219, 15372, 9013, 8005, 28748, 3094, 2151, 3972, 8548, 24153, 1459, 8635, 21532, 18690, 13535, 16865, 16256, 27893, 10959, 27045, 10593, 19980, 14196, 23830, 5958, 27642, 3157, 24734, 15724, 5872, 22658, 26139, 15171, 18092, 30082, 5007, 20833, 15680, 27174, 18080, 27675, 7322, 27929, 27576, 18731, 30790, 11896, 24486, 6302, 5879, 24651, 19953, 18173, 343, 142, 27055, 3651, 18662, 10925, 15662, 17762, 23242, 19376, 26878, 9425, 6575, 22897, 18585, 858, 6965, 20555, 15813, 21283, 12777, 10200, 23754, 8746, 7163, 22936, 24339, 14097, 18137, 10580, 6673, 14680, 30237, 13020, 11524, 25644, 20824, 20466, 21577, 15867, 10094, 4046, 6838, 30542, 12529, 2979, 30346, 17605, 7503, 22211, 1340, 15564, 9782, 26300, 615, 646, 13927, 27248, 26542, 5231, 19418, 6191, 32071, 22717, 830, 918, 24052, 26533, 31873, 2854, 22767, 15399, 2777, 148, 18318, 14753, 2780, 12467, 21950, 30715, 22310, 13762, 23450, 14559, 13310, 1271, 20239, 12146, 4174, 25835, 12762, 750, 29217, 8378, 7682, 25695, 12226, 194, 19845, 31924, 21854, 1166, 6299, 357, 10116, 18933, 20914, 28757, 5283, 13722, 5279, 4004, 17449, 10503, 7038, 19287, 13471, 19247, 5153, 19649, 3531, 23581, 5511, 10003, 6616, 8695, 20408, 30628, 9974, 13997, 15740, 12863, 21999, 1221, 5662, 396, 1804, 2765, 24728, 26705, 3206, 15401, 29225, 19410, 26446, 13731, 8329, 27799, 12775, 20403, 5991, 10439, 8132, 21202, 9295, 22313, 10191, 27472, 24824, 16981, 748, 12748, 11875, 14973, 28439, 23333, 20106, 30335, 17276, 29850, 19612, 17429, 4712, 19748, 28246, 26560, 16685, 8875, 30477, 25563, 16639, 6777, 18441, 29594, 25144, 27433, 9604, 15109, 5995, 21348, 25453, 22012, 11481, 7730, 22877, 6409, 22700, 15733, 27076, 32042, 23803, 3714, 19580, 15477, 16225, 11082, 12739, 10485, 199, 16372, 29335, 28301, 11248, 14817, 9571, 23062, 13130, 10517, 8603, 31449, 3772, 190, 769, 30720, 17719, 30592, 31228, 32215, 14750, 3560, 5015, 31127, 11735, 23647, 3032, 7869, 27904, 2135, 18070, 11746, 25117, 26763, 20603, 4723, 1267, 10809, 31448, 12497, 27398, 30, 9084, 6139, 4264, 16318, 8567, 10760, 2984, 13760, 7216, 15699, 20906, 29866, 6776, 28058, 20715, 923, 8166, 15944, 6226, 4401, 7351, 12120, 26402, 40, 16666, 6842, 27710, 6206, 7016, 28724, 29596, 23348, 16763, 5180, 24, 118, 14452, 19877, 2148, 21153, 25817, 13149, 25456, 31669, 8457, 1751, 3041, 3091, 31012, 20795, 26106, 20317, 18935, 25405, 20604, 24784, 27983, 9806, 24912, 11518, 1446, 5391, 5273, 12158, 31336, 4939, 2815, 1565, 17090, 13242, 210, 23924, 26942, 16681, 19663, 27792, 21097, 17250, 20704, 20406, 19597, 1202, 12145, 2309, 2037, 594, 18437, 4302, 9218, 27241, 30124, 27848, 20404, 4692, 24991, 23838, 26263, 25838, 7248, 22246, 12734, 1261, 20639, 24355, 9653, 22679, 4878, 22252, 1444, 26756, 31910, 14305, 31796, 24204, 29591, 22506, 1761, 13925, 29099, 10241, 7894, 2767, 19757, 3180, 6015, 25653, 13044, 16730, 19512, 14652, 31584, 2980, 17865, 14309, 19622, 18490, 7557, 31005, 2074, 3708, 3218, 30328, 16453, 19613, 27720, 13945, 7820, 31848, 25645, 18815, 23345, 2704, 17471, 146, 2523, 28075, 18424, 17750, 17234, 24629, 16961, 25942, 16551, 7718, 21630, 26440, 15674, 6612, 24149, 18411, 30624, 9702, 23859, 16385, 21401, 25789, 13522, 16207, 19061, 28495, 13108, 30330, 31068, 14, 23043, 10122, 19887, 2269, 21461, 12248, 5145, 14050, 11562, 5292, 26535, 25989, 6452, 9847, 28065, 17843, 11865, 22104, 17370, 11151, 8094, 21010, 7745, 27916, 13143, 5834, 3566, 25890, 11477, 9923, 7076, 17992, 14614, 22480, 1750, 27667, 14333, 28412, 30310, 21536, 15106, 11897, 12493, 27053, 1240, 21350, 16949, 29441, 23757, 12462, 11739, 18409, 19155, 13811, 31318, 2103, 5126, 9794, 26151, 11314, 7811, 8721, 15968, 2597, 30371, 1779, 2526, 7661, 23484, 29402, 17033, 31895, 6863, 18436, 16915, 26897, 29713, 713, 1596, 12855, 3472, 185, 29397, 1309, 16994, 13995, 1995, 19310, 1661, 1924, 13376, 5165, 611, 6262, 31920, 1559, 11984, 3529, 13600, 15057, 2709, 5142, 12791, 12278, 1086, 6319, 11123, 15452, 31712, 12447, 3297, 6223, 29138, 2395, 30638, 14682, 9770, 29325, 31289, 29653, 1892, 4863, 26860, 32199, 16847, 16140, 7382, 30301, 1807, 50, 2610, 20889, 18902, 1463, 20595, 25691, 2019, 14262, 14686, 21043, 9954, 9251, 14302, 21782, 2390, 5326, 21894, 9959, 17623, 24800, 4176, 23126, 22125, 7497, 28011, 7632, 23462, 23774, 30654, 18429, 4926, 20574, 31559, 15329, 28222, 15258, 8492, 27682, 9105, 29745, 27290, 8095, 701, 5888, 23200, 19, 2050, 20270, 28911, 247, 29323, 15938, 24742, 8159, 365, 10595, 26020, 10345, 25012, 19018, 23283, 23763, 15332, 3000, 3655, 4209, 32256, 17157, 25486, 3424, 7132, 8110, 26290, 9637, 19550, 27686, 9739, 16132, 29884, 4142, 22178, 20249, 7071, 3008, 19204, 27344, 20203, 30291, 26492, 6482, 19629, 2466, 9771, 14515, 23175, 27781, 22961, 15991, 11825, 21189, 4888, 18919, 1448, 30999, 9222, 6331, 15519, 3953, 29443, 13049, 24082, 4877, 26987, 20307, 19294, 16436, 26364, 17629, 8244, 21831, 6058, 31966, 9217, 25601, 30871, 30257, 7707, 13054, 8212, 9160, 25986, 2757, 252, 24346, 17447, 31957, 20240, 3040, 24731, 11117, 19714, 5118, 5014, 5981, 31176, 7615, 27601, 19695, 31430, 16154, 5000, 20990, 26166, 15243, 11783, 2643, 28321, 8885, 28862, 31453, 30169, 26818, 22208, 2073, 4615, 7875, 22714, 14628, 20411, 12468, 31680, 23159, 3362, 21885, 7741, 10524, 8338, 24410, 1713, 5930, 17789, 1927, 11387, 11499, 18918, 22251, 29472, 14588, 27200, 7044, 24378, 8733, 4770, 3912, 10462, 3347, 26611, 20378, 11297, 19132, 26733, 22716, 29780, 2931, 5904, 26808, 20103, 30032, 30594, 23122, 7425, 11539, 9757, 31248, 26279, 8210, 14536, 13095, 32012, 16761, 11716, 985, 30004, 29332, 19879, 30235, 17586, 28961, 14691, 19994, 6877, 29016, 3601, 17377, 12705, 14052, 21272, 5649, 29334, 31748, 29404, 9422, 21865, 13953, 11582, 24112, 19130, 27797, 11372, 16954, 297, 14413, 16020, 8354, 15515, 6081, 23179, 13873, 27393, 28056, 4254, 678, 13127, 16373, 30719, 8176, 11586, 19473, 23374, 23017, 18800, 4333, 22218, 1354, 6908, 23511, 25314, 11765, 25893, 22174, 17894, 3195, 7668, 24758, 3497, 14580, 1464, 5295, 19906, 17044, 6379, 872, 9483, 29654, 17170, 26280, 2287, 26040, 9864, 3580, 31496, 27158, 8517, 5252, 10141, 15447, 11447, 6129, 9288, 16007, 19974, 16678, 7732, 23006, 8910, 26981, 11242, 27680, 11740, 15339, 24637, 4701, 19979, 2154, 6760, 16890, 5177, 2751, 18155, 18829, 5146, 30143, 20228, 11517, 22999, 23614, 24283, 30040, 2948, 364, 26427, 26035, 10451, 128, 27034, 8554, 5312, 31658, 7109, 22321, 12974, 22820, 20107, 28375, 21311, 22349, 14312, 7300, 22655, 13003, 9206, 25697, 6959, 26074, 8209, 17565, 25367, 24587, 4648, 28645, 7198, 27157, 4168, 25325, 30793, 28596, 30881, 5436, 25424, 19978, 25418, 19229, 1413, 18586, 28670, 24607, 10325, 21849, 17353, 21762, 10768, 27336, 17580, 28976, 10185, 13344, 19687, 24335, 26392, 23111, 31967, 22, 3307, 15984, 24943, 27022, 2181, 26285, 19691, 10103, 31380, 6417, 24614, 3367, 8907, 26990, 26975, 29867, 20015, 31314, 10275, 29366, 2182, 2505, 14250, 17300, 28304, 8380, 15538, 9982, 4441, 13684, 2747, 26453, 4239, 2987, 29078, 24760, 13080, 16236, 528, 9097, 13666, 17209, 6020, 22934, 6451, 29788, 12180, 20108, 6098, 17579, 4433, 13512, 17123, 8951, 602, 6562, 29083, 3552, 8313, 18527, 13343, 7711, 17064, 4158, 30976, 16795, 7527, 30593, 117, 29456, 9131, 19721, 28677, 350, 3853, 22324, 30308, 2748, 19241, 29025, 14075, 27480, 24061, 31931, 8622, 13039, 31479, 10628, 7962, 20431, 2336, 13468, 29833, 6254, 4833, 16054, 24092, 23297, 1178, 16878, 8039, 2079, 15493, 28903, 17813, 31233, 21386, 16267, 2916, 10305, 6954, 28942, 20931, 28319, 3397, 6696, 25343, 22933, 32183, 13433, 20297, 22707, 4036, 31189, 22860, 24964, 21954, 16266, 7495, 27015, 5975, 1013, 11161, 4835, 9373, 9278, 4687, 25286, 138, 17165, 9990, 11460, 24464, 14918, 10690, 1384, 6305, 30455, 29967, 14288, 20569, 11844, 20325, 26545, 22460, 29313, 21109, 24382, 30402, 18027, 25662, 8160, 4644, 352, 13700, 12813, 24044, 11941, 7564, 391, 13328, 24638, 27259, 16135, 9249, 5832, 19645, 21769, 26832, 10713, 27232, 3220, 29483, 16001, 20762, 8677, 23491, 22811, 28390, 1959, 19045, 1791, 25799, 13917, 17690, 6770, 1638, 7526, 4679, 6405, 27509, 14834, 29983, 12466, 727, 4855, 31778, 25288, 3198, 12286, 14945, 15803, 427, 28166, 30573, 7856, 21691, 14743, 26494, 2618, 16780, 27180, 18246, 5769, 13234, 2245, 13435, 22240, 2720, 4551, 15634, 18461, 27181, 19013, 17908, 15141, 21938, 25875, 30406, 17194, 1821, 10882, 12519, 8596, 23825, 24099, 23898, 4476, 15700, 31673, 5680, 1050, 6394, 27672, 29468, 4330, 15444, 30246, 7422, 24316, 9939, 22207, 26169, 14314, 18892, 6960, 30309, 21917, 3503, 10256, 18108, 30274, 19514, 5799, 20702, 4804, 8174, 6702, 28496, 30576, 17029, 18885, 29705, 13012, 11476, 22487, 11263, 24795, 7334, 21545, 24809, 31531, 14386, 15923, 28180, 16354, 29376, 8242, 25796, 5555, 1083, 10725, 8799, 248, 30067, 29545, 21962, 12312, 17945, 16987, 17242, 25004, 17672, 30820, 18726, 30202, 21468, 25031, 8773, 21563, 25661, 9835, 24858, 8250, 29899, 13291, 16049, 22074, 13813, 1314, 13161, 4492, 3150, 618, 17168, 11269, 1351, 19527, 4485, 8236, 7452, 21346, 14047, 27491, 26076, 31031, 21738, 17793, 23451, 10235, 18750, 11292, 27626, 14538, 17189, 15800, 7313, 26529, 27593, 31292, 31744, 28380, 31813, 10552, 22041, 4894, 31208, 26830, 26802, 23202, 932, 29119, 25017, 27841, 14172, 3701, 21369, 13674, 9958, 4415, 10137, 7544, 1952, 15594, 1866, 7486, 27834, 13928, 7131, 17917, 19311, 21944, 19626, 30434, 28437, 24903, 10053, 15412, 13658, 2032, 9018, 12653, 30933, 10049, 14092, 28786, 26606, 14846, 27912, 797, 5884, 4389, 26371, 17514, 20321, 9543, 19397, 22947, 9542, 1972, 15448, 17530, 12902, 29289, 11985, 32065, 9996, 26210, 28706, 26586, 21485, 7207, 4851, 2450, 26884, 29672, 18410, 26754, 21081, 106, 1987, 13282, 4620, 6145, 23373, 17944, 4010, 31976, 26228, 25257, 30326, 18307, 10223, 16703, 6367, 10636, 30440, 7963, 9488, 18773, 18201, 8710, 12581, 2446, 25462, 23976, 27448, 25227, 11249, 9468, 1090, 3748, 25120, 21082, 5464, 12941, 1311, 4821, 17485, 5488, 12443, 24697, 32021, 11658, 25452, 31092, 9520, 23759, 561, 31061, 20265, 19102, 25925, 14353, 2494, 22325, 30109, 18334, 15112, 3906, 3299, 31804, 14976, 180, 30862, 20722, 31406, 30053, 24019, 3342, 10516, 14400, 17369, 3885, 29156, 742, 5191, 3211, 31849, 13914, 30336, 1321, 8731, 1702, 9885, 20255, 20631, 16025, 20130, 29682, 6995, 29746, 10013, 29014, 6568, 8061, 28150, 10952, 21543, 430, 25204, 11872, 13202, 31358, 12461, 13213, 25279, 3224, 20101, 16237, 5589, 3186, 27124, 10088, 30468, 18907, 3699, 13372, 10433, 7113, 10264, 29748, 7627, 12664, 19261, 11722, 1805, 5109, 19267, 30315, 13402, 7367, 18299, 17930, 4831, 22599, 5028, 9714, 2921, 24882, 13969, 13057, 22112, 5073, 11619, 15545, 5272, 32254, 13240, 18044, 24627, 11510, 2641, 23888, 20613, 9054, 5508, 11924, 3126, 22215, 10617, 9749, 7595, 19609, 30251, 27277, 4808, 13103, 15717, 21759, 8564, 13356, 31356, 6119, 21882, 7396, 7474, 8126, 3851, 5781, 23776, 22411, 3828, 31144, 24177, 2408, 3166, 3073, 22959, 2706, 7699, 16122, 17034, 10132, 23014, 24792, 25171, 1168, 27419, 24055, 30232, 26489, 30572, 23627, 26999, 2415, 589, 22514, 9133, 25959, 28627, 10957, 28865, 8493, 20217, 11923, 5032, 16536, 6412, 361, 17884, 12909, 14593, 20312, 1879, 6088, 22525, 21498, 31674, 22533, 15424, 7350, 12434, 17445, 17403, 8170, 21644, 31, 18009, 12241, 18936, 23956, 19430, 10860, 15574, 16657, 31528, 15296, 12621, 10482, 20692, 24978, 22139, 29244, 10571, 31950, 14678, 20820, 22607, 6806, 28875, 2220, 30292, 6596, 20515, 26180, 27573, 6131, 26943, 22344, 23271, 10856, 5406, 28995, 14206, 21900, 7782, 18677, 23078, 14695, 18753, 16105, 14454, 8700, 1096, 11404, 9734, 27252, 7750, 15513, 9050, 15877, 25974, 3929, 18646, 18597, 29181, 25043, 12972, 30819, 14328, 1667, 10064, 10057, 29868, 4934, 30807, 7932, 25987, 29065, 23726, 27251, 11824, 8514, 4581, 28626, 16611, 19356, 23589, 685, 17293, 23113, 28858, 16659, 27994, 14327, 5382, 3833, 16781, 6705, 24141, 24334, 9401, 21361, 28760, 30777, 25643, 22786, 3830, 21168, 29864, 12479, 4289, 920, 23678, 29566, 25745, 27347, 298, 19181, 29671, 5122, 5735, 3979, 14162, 6011, 20195, 2461, 25100, 18618, 13503, 24387, 19854, 21328, 17159, 9308, 28577, 3824, 24640, 24804, 21773, 2029, 18963, 28127, 31971, 3141, 6689, 9212, 7187, 29998, 7120, 909, 24379, 8053, 23583, 24084, 1670, 17450, 22572, 24572, 27765, 16922, 4451, 28462, 7778, 30297, 3083, 15961, 10010, 29128, 11646, 30483, 4958, 23885, 28363, 9435, 4170, 22376, 9353, 6386, 5204, 3182, 31617, 22114, 9331, 30850, 23305, 9557, 30084, 20649, 22461, 22932, 3724, 13270, 288, 4018, 18637, 27309, 699, 7029, 23762, 3156, 19162, 20640, 31763, 29714, 16199, 18041, 16439, 4464, 9685, 2750, 14557, 30534, 6578, 7508, 27390, 10377, 7543, 722, 17740, 1945, 3735, 7622, 28861, 18650, 26343, 5517, 9480, 30012, 25590, 29753, 30229, 23986, 26089, 26883, 16313, 25855, 12926, 5615, 9121, 19098, 16622, 17251, 24170, 151, 4587, 23742, 9117, 13078, 6246, 5784, 12472, 22806, 24008, 16252, 14042, 1815, 253, 29147, 20204, 7249, 25006, 26487, 3337, 2184, 20087, 18175, 25594, 18926, 3444, 21859, 832, 11427, 27895, 681, 21459, 14948, 12917, 663, 14069, 23430, 17538, 5780, 18996, 19732, 1800, 21165, 17018, 26996, 3994, 5894, 10513, 7223, 29673, 7621, 19342, 9950, 25587, 29790, 21199, 15946, 7705, 5474, 9521, 25973, 12427, 24472, 12441, 9859, 14664, 15909, 15122, 23605, 7127, 22248, 16286, 9290, 9929, 31386, 2758, 9495, 24540, 31025, 14926, 14359, 24660, 737, 25104, 20461, 15206, 31579, 32239, 16523, 2375, 13038, 1944, 5765, 21457, 19063, 7516, 26709, 17267, 9227, 913, 13551, 18115, 29979, 27193, 24451, 27059, 29652, 21923, 20671, 13867, 18471, 310, 32181, 26437, 16939, 6056, 17266, 22419, 29219, 16635, 27255, 31328, 6280, 9019, 12561, 24698, 428, 26924, 31421, 7801, 30387, 14787, 31589, 4686, 28710, 15090, 24058, 1426, 10653, 6907, 28153, 20643, 15027, 21467, 11458, 6583, 22415, 13239, 26661, 22193, 14029, 17299, 4997, 24212, 29797, 15633, 5490, 2723, 19160, 5716, 23340, 7966, 7245, 5776, 26058, 30277, 9020, 30920, 17904, 6012, 8182, 881, 1167, 18688, 27207, 17259, 30028, 30395, 8777, 23145, 32052, 18701, 17619, 28275, 1264, 16073, 23076, 5881, 9779, 11706, 10144, 9781, 18779, 16264, 28483, 13275, 24095, 18737, 2250, 9726, 1617, 10303, 5910, 5690, 2296, 5577, 32014, 17069, 17487, 20158, 30250, 23241, 6318, 24041, 13821, 23981, 25122, 17292, 9992, 10511, 70, 18971, 17533, 14879, 10855, 26992, 21198, 10888, 14362, 12752, 22953, 16193, 31224, 29183, 30935, 5048, 31047, 7210, 22347, 26775, 3899, 3275, 17411, 19035, 17156, 12839, 8438, 26892, 6092, 27844, 16188, 17546, 10723, 20153, 8158, 15197, 8302, 16755, 19448, 27940, 738, 20904, 9355, 14046, 11975, 16770, 237, 19921, 17389, 6360, 7982, 19839, 6667, 30173, 16149, 11669, 32143, 17878, 26357, 4017, 15104, 4574, 25514, 21466, 1017, 18129, 17382, 30735, 20575, 17140, 22323, 31048, 601, 20180, 23633, 12632, 3496, 1272, 9554, 28735, 18474, 3395, 10629, 12115, 11012, 27883, 11250, 22849, 14865, 22261, 28081, 31002, 14176, 18493, 929, 5696, 27031, 28564, 18320, 21288, 16229, 3043, 30646, 9350, 31629, 10002, 6296, 15407, 15649, 3082, 18874, 19809, 23190, 20837, 7573, 23630, 26707, 14236, 7832, 22909, 3335, 27041, 1619, 1401, 22459, 31401, 31566, 14736, 3309, 3632, 6112, 21549, 17428, 11682, 6570, 6831, 17110, 21780, 18642, 5987, 21481, 11697, 21528, 17400, 30763, 8605, 20991, 6906, 13010, 26984, 1442, 695, 377, 28880, 31459, 7630, 4801, 18986, 344, 9475, 25758, 23854, 30831, 19008, 12449, 1211, 16901, 25670, 25152, 16841, 15727, 30110, 31856, 2308, 13461, 19208, 22184, 16838, 6969, 12834, 7404, 17863, 17824, 9127, 31623, 18054, 31694, 15586, 11845, 7380, 14875, 8485, 18349, 19966, 15751, 6046, 2968, 24851, 3481, 11622, 28358, 24677, 614, 14505, 15154, 31979, 29478, 28410, 5117, 7421, 2698, 19675, 6748, 30986, 9352, 20911, 8062, 5717, 6301, 13279, 3484, 21540, 20006, 31628, 19576, 22022, 28149, 26655, 12831, 20254, 27977, 18842, 7618, 4826, 14181, 17795, 24591, 17065, 20608, 21734, 4430, 9066, 23760, 24215, 24887, 4288, 25376, 7292, 16120, 7032, 1171, 1996, 28557, 15868, 15152, 19955, 37, 19056, 26360, 25461, 11432, 11395, 9587, 25180, 16178, 24460, 6304, 29287, 14561, 768, 2212, 14499, 9240, 23475, 20202, 30648, 26571, 5294, 25605, 27246, 20806, 12332, 19268, 16548, 23463, 29427, 10603, 19886, 19218, 13411, 30494, 29229, 1163, 10567, 31575, 17942, 3586, 6533, 9431, 7841, 19773, 17175, 3072, 18722, 9828, 31393, 14009, 16324, 9389, 941, 9665, 31447, 10632, 17352, 11116, 13863, 23811, 4930, 30527, 23716, 24152, 7110, 6740, 4151, 13395, 24120, 27004, 30463, 7341, 29980, 18176, 18598, 21429, 16029, 6259, 24632, 7301, 15298, 25336, 13807, 31199, 11333, 9819, 17379, 27542, 5360, 3761, 3183, 13197, 22756, 31833, 6864, 19265, 11373, 10355, 11171, 23066, 866, 20250, 31781, 24441, 2293, 19777, 22854, 2894, 28512, 25272, 2823, 2444, 4465, 2413, 28173, 29015, 23007, 12680, 17361, 25525, 31495, 23082, 19452, 11170, 1031, 5552, 9635, 13439, 31784, 8874, 4117, 10913, 15238, 23441, 457, 10828, 12253, 11350, 16338, 2267, 12531, 26562, 28337, 17499, 24779, 21874, 25185, 19870, 5854, 14577, 12401, 15190, 9116, 1349, 27733, 14783, 15834, 7887, 2826, 12065, 7169, 14003, 29112, 7174, 26969, 28112, 28573, 22766, 21713, 29584, 12657, 15914, 267, 2509, 26762, 4119, 29667, 15380, 18128, 23046, 4906, 30992, 24220, 25975, 19572, 3855, 13560, 12768, 13860, 1292, 31505, 6983, 11174, 10336, 30374, 28292, 4599, 10962, 15216, 928, 23980, 5493, 19107, 16418, 23385, 7882, 25982, 1106, 5002, 15370, 10691, 23146, 10181, 26012, 6320, 8327, 14525, 17286, 27104, 8857, 19436, 23442, 28230, 21196, 20581, 31297, 22918, 13491, 17435, 15825, 13602, 7643, 24926, 2566, 10790, 5595, 10125, 7748, 22308, 19444, 444, 468, 19070, 5495, 4849, 26753, 26363, 11336, 1855, 10210, 11909, 19363, 31259, 11799, 4769, 18943, 29926, 15123, 24258, 7664, 5563, 20209, 31253, 11726, 14859, 26296, 30418, 13757, 2303, 13383, 19788, 1087, 12223, 25879, 19925, 13168, 16411, 28711, 23787, 29135, 20176, 21904, 19394, 16581, 163, 25495, 22545, 29863, 7083, 10066, 26284, 1818, 21685, 24293, 14026, 5813, 1889, 18209, 4011, 8064, 28797, 30816, 31212, 5811, 3867, 30604, 17684, 12897, 21208, 4102, 19292, 15052, 14142, 5603, 26461, 11537, 13632, 4469, 19710, 20454, 27917, 25686, 12796, 26734, 13790, 12543, 13730, 4647, 30128, 2562, 2481, 13993, 4242, 14653, 26486, 25347, 5210, 29947, 8989, 8919, 16103, 25978, 17949, 30248, 24879, 26760, 23820, 9192, 8977, 4278, 24930, 9439, 22887, 19587, 23395, 16640, 19787, 8458, 1137, 27417, 5380, 31506, 11176, 23818, 24620, 31943, 1668, 7441, 2171, 26954, 12303, 29522, 19290, 27164, 30394, 20848, 23460, 19062, 1641, 1902, 29069, 27817, 5974, 20095, 31544, 8760, 18229, 28020, 19148, 5237, 9767, 24578, 9497, 883, 359, 2936, 8055, 6519, 15415, 28448, 6966, 9196, 21660, 3417, 12738, 6726, 22510, 10990, 17952, 30692, 6228, 30946, 10328, 22879, 6265, 28082, 25360, 31715, 25910, 3540, 19536, 6384, 8811, 27239, 18730, 6410, 24568, 18983, 2290, 21778, 631, 12579, 460, 13636, 23534, 17295, 31801, 3416, 5909, 26569, 19548, 15280, 11127, 19761, 11139, 29874, 25390, 6008, 8778, 20669, 16786, 2787, 8192, 11299, 12556, 2756, 3690, 25366, 4876, 20737, 25408, 6486, 24593, 19273, 15366, 11541, 25480, 149, 15753, 1485, 289, 26173, 2080, 18802, 13180, 20635, 22624, 28804, 22200, 31338, 566, 6830, 31585, 25848, 10801, 24096, 11230, 31095, 14527, 26553, 19214, 31532, 501, 31882, 29943, 6240, 14712, 11346, 18486, 20134, 20559, 30317, 24506, 1605, 12425, 23622, 5164, 17668, 21876, 588, 9300, 28803, 12302, 27299, 4842, 17994, 31357, 15270, 22542, 8526, 8035, 16960, 22058, 10569, 8269, 28982, 2683, 8201, 26055, 17270, 3287, 14439, 23847, 27702, 1657, 24716, 9924, 21977, 22623, 20624, 9418, 29241, 27110, 14734, 12246, 10490, 13884, 2402, 8802, 25947, 4138, 12414, 14519, 16746, 23093, 15769, 2398, 8001, 14668, 28416, 13827, 22356, 9948, 22117, 8550, 21356, 12948, 22132, 5381, 24308, 6961, 18060, 7218, 25625, 9944, 1318, 11498, 23540, 21303, 20785, 10889, 26132, 19355, 648, 5664, 22118, 624, 27341, 18804, 19813, 12023, 32023, 11774, 30305, 25960, 22973, 12103, 15772, 25246, 662, 28472, 13753, 5941, 27395, 31880, 5206, 13536, 2663, 9195, 31347, 10243, 20666, 24059, 30561, 10370, 20524, 5379, 4207, 13838, 5129, 17899, 1992, 4813, 20086, 16873, 27700, 20873, 8151, 23538, 879, 4204, 19655, 707, 22982, 491, 13269, 10351, 12370, 1711, 31274, 7915, 23042, 21394, 2382, 27742, 24270, 14531, 14267, 19221, 3501, 7052, 16412, 96, 11514, 8309, 26786, 11588, 15312, 18647, 27685, 8585, 30482, 5375, 24719, 13770, 27622, 10849, 11561, 3117, 22036, 15139, 13109, 1843, 17424, 20369, 9691, 3516, 11560, 11379, 13448, 32013, 29008, 13392, 4816, 4413, 30484, 25687, 26765, 8996, 1862, 22329, 15517, 3067, 1510, 27, 11624, 25968, 24458, 19993, 23987, 21435, 12124, 19935, 15175, 30115, 8664, 15709, 17716, 15667, 5667, 13591, 22105, 19715, 25920, 14586, 18587, 28377, 8077, 14115, 334, 22466, 29460, 27128, 2385, 12052, 22923, 21943, 12186, 14154, 25700, 7589, 3598, 18718, 22336, 27541, 28868, 15533, 14644, 4165, 4140, 27971, 20912, 5332, 4582, 24598, 29606, 30750, 28575, 24938, 2703, 6251, 8083, 11733, 23797, 2734, 18552, 16914, 965, 763, 14045, 12942, 31955, 5358, 16826, 7749, 25737, 3223, 15295, 10557, 27507, 7462, 30853, 25565, 5051, 22285, 7409, 21987, 30814, 2118, 26604, 8319, 23129, 23219, 17344, 2273, 23624, 24407, 762, 11576, 3443, 3910, 9659, 5330, 24271, 29755, 14136, 4275, 30410, 9676, 15262, 15892, 5476, 21628, 12382, 13738, 29760, 12674, 24069, 19133, 25631, 24822, 10914, 28896, 15646, 29122, 8195, 5133, 1573, 9822, 28831, 3504, 9262, 32184, 10104, 3860, 16998, 24706, 21681, 13091, 8097, 10119, 12155, 4973, 20525, 16406, 8181, 5713, 32041, 10732, 22014, 16129, 19517, 7143, 23319, 10018, 10069, 1965, 18015, 3746, 30408, 22454, 4610, 3449, 20489, 7549, 17198, 32208, 10835, 26407, 10177, 10432, 14285, 258, 4050, 12335, 28747, 20207, 31899, 17691, 11106, 9809, 12829, 1393, 15458, 4054, 31745, 14958, 30579, 21729, 16311, 22703, 29759, 10233, 14183, 24848, 8821, 29836, 6504, 157, 26243, 21661, 31888, 28199, 1857, 22928, 25430, 19307, 895, 9934, 13158, 21538, 27332, 4794, 30789, 24453, 12121, 25280, 5183, 23366, 25161, 27757, 3838, 16700, 21637, 26472, 20434, 23084, 21584, 7376, 4328, 5185, 19780, 3554, 8042, 14448, 14729, 676, 32046, 25648, 19096, 24650, 1193, 3770, 15880, 3455, 15941, 27898, 23777, 16821, 20971, 30382, 26739, 30228, 10906, 24242, 32171, 18387, 27878, 16819, 27176, 7597, 25772, 21052, 29050, 24740, 23120, 32085, 15675, 25218, 10645, 16417, 5486, 27423, 790, 25490, 32182, 10570, 24225, 28236, 23892, 26376, 8027, 22762, 12967, 25813, 13121, 4449, 20169, 27729, 15780, 19336, 6327, 30754, 15718, 27038, 3744, 9673, 14964, 8108, 24664, 2699, 17488, 11926, 586, 4322, 5613, 5264, 1936, 18337, 10285, 25449, 13251, 16159, 22950, 18475, 23413, 25166, 20822, 9253, 21453, 966, 30378, 8381, 17326, 9881, 10919, 31709, 1652, 24429, 19141, 28785, 14321, 15933, 5885, 17525, 2965, 1358, 3047, 27442, 26757, 21044, 17796, 22130, 20144, 22150, 10545, 26989, 24137, 5587, 25650, 21503, 27127, 7570, 13387, 25533, 22362, 14210, 23500, 16542, 17951, 11501, 28270, 25566, 13217, 597, 28682, 32192, 2903, 17378, 2928, 23004, 4747, 22052, 11718, 4499, 2517, 14177, 31084, 9695, 17588, 15855, 3357, 25088, 26896, 20360, 26662, 28460, 28039, 7010, 11185, 13345, 4774, 20335, 21752, 13990, 7975, 11218, 6560, 31215, 22315, 4584, 27525, 1390, 26862, 10705, 25189, 20802, 24768, 24935, 19933, 8148, 11986, 28353, 4218, 26509, 31954, 24494, 29570, 23590, 1997, 21003, 15024, 14737, 28543, 3464, 3366, 24693, 5621, 398, 29929, 20805, 21505, 8891, 17839, 31906, 25196, 12505, 19406, 10007, 8498, 8399, 9837, 24227, 657, 22098, 2864, 24793, 10497, 20626, 3064, 7817, 30025, 32130, 29044, 917, 5441, 536, 6823, 9999, 12508, 21598, 7273, 11689, 13355, 22954, 29655, 690, 1983, 31055, 14048, 30967, 628, 9152, 29796, 29696, 10084, 15182, 25832, 21881, 4267, 11584, 24884, 28676, 17608, 5905, 14962, 20251, 2790, 28241, 20609, 23547, 23058, 6611, 30182, 16512, 27042, 29728, 5105, 18959, 32158, 25553, 26633, 10777, 9873, 3795, 3298, 11843, 18381, 11569, 20975, 6925, 18435, 22747, 5638, 32150, 3949, 9951, 13782, 28352, 29712, 12220, 45, 12191, 6230, 28106, 8128, 10033, 24016, 22647, 15859, 8652, 10886, 31787, 29438, 1301, 27713, 4152, 28826, 1074, 29352, 20544, 18755, 17071, 11928, 21525, 21297, 12750, 20054, 22617, 27936, 7196, 8193, 4634, 4429, 22875, 19656, 21989, 98, 14789, 6461, 14776, 13635, 30922, 13803, 16003, 5162, 562, 4867, 26609, 25714, 1562, 6890, 24148, 1752, 5341, 30205, 32072, 9890, 28877, 15848, 4076, 21974, 11124, 19567, 599, 6138, 21755, 13146, 20825, 9226, 28133, 11445, 18594, 326, 4722, 15543, 2849, 21421, 4719, 7639, 31125, 984, 2774, 18077, 8051, 3296, 20792, 29170, 25069, 8946, 20030, 1504, 1696, 28183, 19726, 7183, 1132, 25688, 32026, 16606, 31009, 7498, 13923, 6337, 10858, 5796, 18803, 9426, 7017, 3101, 25902, 20668, 5654, 28434, 25548, 15007, 20442, 11079, 1198, 1000, 6248, 16283, 3819, 2072, 5308, 21100, 31142, 31729, 22653, 18783, 21758, 25593, 15436, 7935, 25399, 7530, 27274, 11644, 29639, 6252, 27671, 29567, 29450, 5895, 8826, 4366, 17766, 26493, 25895, 7033, 16628, 27559, 23490, 17304, 3898, 12692, 15593, 6094, 20542, 9937, 12874, 29415, 3753, 31863, 4380, 32209, 16158, 28668, 160, 7787, 13225, 19919, 13986, 16549, 20538, 2109, 19275, 14093, 3579, 28400, 19659, 3757, 17681, 2829, 22530, 30536, 23569, 4498, 2291, 20426, 18660, 2128, 10684, 6756, 17815, 30266, 7694, 7823, 11755, 11482, 29139, 4298, 10863, 18862, 19816, 13578, 4410, 11800, 16235, 3328, 5278, 24319, 7999, 27177, 23702, 18636, 21756, 552, 5507, 18311, 23831, 29210, 3608, 6497, 31652, 30027, 9575, 1477, 19740, 29484, 16437, 11115, 25972, 30127, 24240, 13786, 23585, 31592, 3517, 18453, 3279, 23520, 6075, 17152, 9988, 6921, 13764, 10201, 5157, 2933, 3341, 1070, 5261, 18255, 31130, 29917, 1682, 5396, 19497, 2012, 22551, 15916, 23260, 19124, 20861, 28498, 21309, 12727, 27968, 27085, 17205, 24417, 17166, 5839, 12279, 28811, 27997, 1853, 752, 14809, 31222, 12235, 9539, 15334, 13870, 25195, 15263, 23422, 9750, 15528, 30841, 29341, 436, 28594, 30141, 25611, 20127, 24526, 20928, 3505, 11167, 5100, 23660, 10859, 8772, 27087, 11475, 18941, 807, 3985, 21533, 25516, 12592, 14209, 2233, 10802, 12406, 26706, 4044, 1871, 21883, 11093, 28840, 12345, 17036, 29457, 8583, 9360, 11208, 30198, 12243, 30118, 5652, 7202, 2539, 16450, 12569, 1704, 13052, 3777, 11066, 11389, 25203, 15364, 29605, 14277, 13101, 16647, 28912, 5316, 24269, 5131, 14888, 6778, 24688, 26473, 19828, 3687, 16911, 31097, 1633, 9909, 26903, 20184, 29703, 9993, 23165, 4125, 17511, 12255, 29508, 2812, 2275, 18121, 25667, 3192, 30385, 4890, 30283, 12997, 5531, 9713, 29075, 29153, 12904, 7104, 18830, 21820, 29904, 26644, 27708, 21338, 32113, 13783, 8986, 7776, 6607, 29196, 11444, 18656, 14788, 28461, 16408, 30740, 25222, 4974, 20913, 14592, 23698, 10252, 18426, 28357, 4287, 19431, 23725, 1904, 31021, 4680, 2312, 18839, 12608, 12119, 16138, 23554, 31513, 23769, 10306, 20278, 27372, 26067, 21215, 27483, 31014, 17804, 18438, 28834, 19135, 3775, 17147, 28129, 1755, 31757, 23699, 3380, 10945, 30422, 23177, 15187, 16303, 2902, 18814, 3318, 18305, 9320, 17974, 13272, 19428, 18448, 27204, 19999, 6336, 7681, 13714, 7478, 28714, 9547, 5791, 731, 29102, 12164, 30039, 27627, 3401, 8716, 9949, 11595, 12289, 21585, 11851, 24261, 11035, 17869, 27907, 20422, 2196, 25928, 3802, 13112, 8339, 8476, 23187, 18399, 22675, 14796, 22696, 1473, 4935, 30575, 4929, 22151, 22038, 3893, 9428, 1860, 14021, 31017, 6593, 31502, 10989, 1101, 2534, 11061, 8847, 8292, 21760, 6841, 6511, 6932, 26985, 26895, 13820, 13364, 27122, 5021, 21548, 26598, 27149, 8871, 22316, 10352, 17832, 8117, 17063, 26374, 12534, 29333, 23116, 29593, 17638, 16707, 6860, 8533, 28656, 2149, 29216, 130, 6018, 14913, 22018, 25205, 28331, 31422, 17882, 12512, 31051, 3200, 188, 8078, 25575, 23052, 24366, 4104, 8036, 5597, 4583, 1778, 11764, 6828, 27357, 31295, 27638, 13832, 24554, 18522, 28642, 20485, 5339, 30666, 7606, 8131, 5962, 10048, 15573, 8179, 10327, 728, 13836, 7092, 10910, 1337, 21959, 7080, 27656, 9569, 4596, 22902, 31633, 17364, 15945, 3468, 16147, 23609, 10131, 22757, 30414, 2728, 24344, 4368, 10566, 25234, 17706, 21669, 27240, 24105, 20073, 5263, 28655, 15629, 3142, 6426, 18324, 6660, 5887, 23304, 9356, 23369, 28174, 22690, 11192, 4177, 8087, 20860, 12352, 30710, 25084, 26528, 12317, 5648, 12767, 31603, 3882, 31510, 5590, 16213, 5232, 30370, 31679, 20242, 6669, 13431, 29528, 28540, 13705, 23248, 6679, 4897, 26001, 15614, 18252, 25676, 6997, 2647, 26639, 17323, 31283, 3750, 28743, 11613, 8010, 7483, 17947, 19874, 1746, 14322, 16799, 22986, 17127, 13307, 23474, 7106, 28403, 31099, 11947, 15569, 16725, 27762, 9443, 6620, 25431, 12965, 5860, 20145, 8745, 2658, 20779, 14356, 25116, 12843, 892, 24192, 11162, 2482, 30340, 13763, 24599, 7330, 24739, 9151, 24583, 17188, 12132, 19057, 17554, 22274, 20071, 28314, 20695, 19981, 25849, 12862, 7461, 25298, 323, 8098, 12946, 22189, 6724, 15665, 29130, 31000, 18203, 25534, 14916, 11999, 1542, 25864, 31146, 1774, 3977, 28368, 30452, 19082, 1695, 26480, 18178, 29658, 9507, 24221, 8704, 19837, 21175, 17493, 20528, 28640, 15864, 10871, 15146, 12981, 1942, 6832, 11519, 4481, 21414, 3295, 7938, 15130, 32243, 2614, 5090, 3559, 16301, 25146, 19631, 22269, 23930, 4916, 10112, 7743, 20799, 13793, 5644, 15987, 1060, 10826, 13267, 14102, 22338, 18732, 17143, 31706, 13389, 31631, 10744, 14782, 27046, 28055, 31303, 27718, 31362, 28338, 28111, 875, 13745, 26144, 1637, 20113, 29744, 16594, 31904, 9700, 15611, 8521, 16282, 12179, 7518, 181, 9274, 10619, 21570, 9636, 14368, 26045, 17214, 20985, 1540, 9161, 27160, 23485, 29809, 11534, 20932, 23593, 7231, 15588, 7275, 23010, 15271, 17497, 27340, 31264, 16041, 23592, 20698, 25374, 30311, 14406, 3311, 19798, 14319, 26482, 9032, 25892, 16111, 2584, 38, 25432, 14036, 14896, 3658, 9065, 15632, 3968, 7493, 12964, 15729, 12994, 3400, 29115, 27633, 15854, 25540, 27445, 20959, 20843, 11473, 13524, 8122, 14523, 29060, 15554, 4486, 20857, 19316, 1439, 3482, 9324, 23640, 26267, 17898, 6277, 24222, 8189, 28429, 21112, 28958, 8205, 6526, 16015, 14665, 14524, 16985, 3021, 8681, 2131, 22158, 27691, 30872, 19603, 18029, 15488, 22608, 27897, 20798, 4746
INFO:root:train dataset: 68117, validation dataset: 3872, test dataset: 6454
INFO:root:used only channels: []; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.04743137
INFO:root:[1,   100] training loss: 0.04196076
INFO:root:[1,   150] training loss: 0.03753050
INFO:root:[1,   200] training loss: 0.03563863
INFO:root:[1,   250] training loss: 0.02878251
INFO:root:[1,   300] training loss: 0.02703376
INFO:root:[1,   350] training loss: 0.03054245
INFO:root:[1,   400] training loss: 0.00065683
INFO:root:[1,   450] training loss: 0.00016571
INFO:root:[1,   500] training loss: 0.00775081
INFO:root:[1,   550] training loss: 0.00564918
INFO:root:[1,   600] training loss: 0.03156077
INFO:root:[1,   650] training loss: 0.00002628
INFO:root:[1,   700] training loss: 0.00001604
INFO:root:[1,   750] training loss: 0.00001369
INFO:root:[1,   800] training loss: 0.00000922
INFO:root:[1,   850] training loss: 0.00000834
INFO:root:[1,   900] training loss: 0.08835854
INFO:root:[1,   950] training loss: 0.01664639
INFO:root:[1,  1000] training loss: 0.00010632
INFO:root:[1,  1050] training loss: 0.00005602
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.0000    0.0000    0.0000         8
    Prophase     0.0000    0.0000    0.0000        73
   Metaphase     0.2670    1.0000    0.4215      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2670      3872
   macro avg     0.0381    0.1429    0.0602      3872
weighted avg     0.0713    0.2670    0.1126      3872

INFO:root:Accuracy of the network on the 3872 validation images: 26 %
INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.07978058
INFO:root:[2,   100] training loss: 0.04483589
INFO:root:[2,   150] training loss: 0.03159730
INFO:root:[2,   200] training loss: 0.02446432
INFO:root:[2,   250] training loss: 0.02745994
INFO:root:[2,   300] training loss: 0.02282546
INFO:root:[2,   350] training loss: 0.02630702
INFO:root:[2,   400] training loss: 0.00145246
INFO:root:[2,   450] training loss: 0.00020748
INFO:root:[2,   500] training loss: 0.00775928
INFO:root:[2,   550] training loss: 0.00737038
INFO:root:[2,   600] training loss: 0.03031297
INFO:root:[2,   650] training loss: 0.00002382
INFO:root:[2,   700] training loss: 0.00002045
INFO:root:[2,   750] training loss: 0.00002154
INFO:root:[2,   800] training loss: 0.00001774
INFO:root:[2,   850] training loss: 0.00001759
INFO:root:[2,   900] training loss: 0.06775432
INFO:root:[2,   950] training loss: 0.01866050
INFO:root:[2,  1000] training loss: 0.00025665
INFO:root:[2,  1050] training loss: 0.00015623
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.0000    0.0000    0.0000         8
    Prophase     0.0000    0.0000    0.0000        73
   Metaphase     0.2670    1.0000    0.4215      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2670      3872
   macro avg     0.0381    0.1429    0.0602      3872
weighted avg     0.0713    0.2670    0.1126      3872

INFO:root:Accuracy of the network on the 3872 validation images: 26 %
INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.06623482
INFO:root:[3,   100] training loss: 0.02887017
INFO:root:[3,   150] training loss: 0.02856767
INFO:root:[3,   200] training loss: 0.02612084
INFO:root:[3,   250] training loss: 0.02540523
INFO:root:[3,   300] training loss: 0.02373669
INFO:root:[3,   350] training loss: 0.02886441
INFO:root:[3,   400] training loss: 0.00372483
INFO:root:[3,   450] training loss: 0.00030611
INFO:root:[3,   500] training loss: 0.00626219
INFO:root:[3,   550] training loss: 0.00753406
INFO:root:[3,   600] training loss: 0.02794990
INFO:root:[3,   650] training loss: 0.00010983
INFO:root:[3,   700] training loss: 0.00008694
INFO:root:[3,   750] training loss: 0.00007415
INFO:root:[3,   800] training loss: 0.00006309
INFO:root:[3,   850] training loss: 0.00006113
INFO:root:[3,   900] training loss: 0.06026821
INFO:root:[3,   950] training loss: 0.01936922
INFO:root:[3,  1000] training loss: 0.00048031
INFO:root:[3,  1050] training loss: 0.00020455
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.0000    0.0000    0.0000         8
    Prophase     0.0000    0.0000    0.0000        73
   Metaphase     0.2670    1.0000    0.4215      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2670      3872
   macro avg     0.0381    0.1429    0.0602      3872
weighted avg     0.0713    0.2670    0.1126      3872

INFO:root:Accuracy of the network on the 3872 validation images: 26 %
INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.05679539
INFO:root:[4,   100] training loss: 0.02373635
INFO:root:[4,   150] training loss: 0.02227947
INFO:root:[4,   200] training loss: 0.02020535
INFO:root:[4,   250] training loss: 0.02106244
INFO:root:[4,   300] training loss: 0.01921255
INFO:root:[4,   350] training loss: 0.02270054
INFO:root:[4,   400] training loss: 0.00091135
INFO:root:[4,   450] training loss: 0.00013400
INFO:root:[4,   500] training loss: 0.00644337
INFO:root:[4,   550] training loss: 0.00573594
INFO:root:[4,   600] training loss: 0.02893940
INFO:root:[4,   650] training loss: 0.00012871
INFO:root:[4,   700] training loss: 0.00009483
INFO:root:[4,   750] training loss: 0.00008371
INFO:root:[4,   800] training loss: 0.00007038
INFO:root:[4,   850] training loss: 0.00006430
INFO:root:[4,   900] training loss: 0.06306923
INFO:root:[4,   950] training loss: 0.01886496
INFO:root:[4,  1000] training loss: 0.00004815
INFO:root:[4,  1050] training loss: 0.00002939
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.0000    0.0000    0.0000         8
    Prophase     0.0000    0.0000    0.0000        73
   Metaphase     0.2670    1.0000    0.4215      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2670      3872
   macro avg     0.0381    0.1429    0.0602      3872
weighted avg     0.0713    0.2670    0.1126      3872

INFO:root:Accuracy of the network on the 3872 validation images: 26 %
INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.06002540
INFO:root:[5,   100] training loss: 0.02984695
INFO:root:[5,   150] training loss: 0.02445525
INFO:root:[5,   200] training loss: 0.02553540
INFO:root:[5,   250] training loss: 0.02498152
INFO:root:[5,   300] training loss: 0.02284319
INFO:root:[5,   350] training loss: 0.02355029
INFO:root:[5,   400] training loss: 0.00096200
INFO:root:[5,   450] training loss: 0.00018550
INFO:root:[5,   500] training loss: 0.00719079
INFO:root:[5,   550] training loss: 0.00626034
INFO:root:[5,   600] training loss: 0.02650102
INFO:root:[5,   650] training loss: 0.00009243
INFO:root:[5,   700] training loss: 0.00007196
INFO:root:[5,   750] training loss: 0.00006819
INFO:root:[5,   800] training loss: 0.00005711
INFO:root:[5,   850] training loss: 0.00005136
INFO:root:[5,   900] training loss: 0.05951482
INFO:root:[5,   950] training loss: 0.01909571
INFO:root:[5,  1000] training loss: 0.00036806
INFO:root:[5,  1050] training loss: 0.00019684
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.0000    0.0000    0.0000         8
    Prophase     1.0000    0.0274    0.0533        73
   Metaphase     0.2673    1.0000    0.4218      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2676      3872
   macro avg     0.1810    0.1468    0.0679      3872
weighted avg     0.0902    0.2676    0.1136      3872

INFO:root:Accuracy of the network on the 3872 validation images: 26 %
INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.05725416
INFO:root:[6,   100] training loss: 0.02413752
INFO:root:[6,   150] training loss: 0.02355784
INFO:root:[6,   200] training loss: 0.02054176
INFO:root:[6,   250] training loss: 0.02418957
INFO:root:[6,   300] training loss: 0.02801988
INFO:root:[6,   350] training loss: 0.02100242
INFO:root:[6,   400] training loss: 0.00081357
INFO:root:[6,   450] training loss: 0.00012314
INFO:root:[6,   500] training loss: 0.00614308
INFO:root:[6,   550] training loss: 0.00681030
INFO:root:[6,   600] training loss: 0.02579849
INFO:root:[6,   650] training loss: 0.00011654
INFO:root:[6,   700] training loss: 0.00009136
INFO:root:[6,   750] training loss: 0.00007809
INFO:root:[6,   800] training loss: 0.00006897
INFO:root:[6,   850] training loss: 0.00005889
INFO:root:[6,   900] training loss: 0.04505337
INFO:root:[6,   950] training loss: 0.01921136
INFO:root:[6,  1000] training loss: 0.00041263
INFO:root:[6,  1050] training loss: 0.00023031
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.0000    0.0000    0.0000         8
    Prophase     1.0000    0.0137    0.0270        73
   Metaphase     0.2672    1.0000    0.4217      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2673      3872
   macro avg     0.1810    0.1448    0.0641      3872
weighted avg     0.0902    0.2673    0.1131      3872

INFO:root:Accuracy of the network on the 3872 validation images: 26 %
INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.05044960
INFO:root:[7,   100] training loss: 0.02140607
INFO:root:[7,   150] training loss: 0.02328151
INFO:root:[7,   200] training loss: 0.01926756
INFO:root:[7,   250] training loss: 0.02367806
INFO:root:[7,   300] training loss: 0.02952293
INFO:root:[7,   350] training loss: 0.02299085
INFO:root:[7,   400] training loss: 0.00052295
INFO:root:[7,   450] training loss: 0.00023221
INFO:root:[7,   500] training loss: 0.00559958
INFO:root:[7,   550] training loss: 0.00714338
INFO:root:[7,   600] training loss: 0.02616552
INFO:root:[7,   650] training loss: 0.00010962
INFO:root:[7,   700] training loss: 0.00008740
INFO:root:[7,   750] training loss: 0.00007663
INFO:root:[7,   800] training loss: 0.00006787
INFO:root:[7,   850] training loss: 0.00005988
INFO:root:[7,   900] training loss: 0.04883591
INFO:root:[7,   950] training loss: 0.01907930
INFO:root:[7,  1000] training loss: 0.00049254
INFO:root:[7,  1050] training loss: 0.00023212
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.4000    0.2500    0.3077         8
    Prophase     0.8750    0.0959    0.1728        73
   Metaphase     0.2680    1.0000    0.4227      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2694      3872
   macro avg     0.2204    0.1923    0.1290      3872
weighted avg     0.0889    0.2694    0.1168      3872

INFO:root:Accuracy of the network on the 3872 validation images: 26 %
INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.04881345
INFO:root:[8,   100] training loss: 0.01937733
INFO:root:[8,   150] training loss: 0.02133008
INFO:root:[8,   200] training loss: 0.01810878
INFO:root:[8,   250] training loss: 0.02337650
INFO:root:[8,   300] training loss: 0.02534713
INFO:root:[8,   350] training loss: 0.01824327
INFO:root:[8,   400] training loss: 0.00088428
INFO:root:[8,   450] training loss: 0.00018259
INFO:root:[8,   500] training loss: 0.00527108
INFO:root:[8,   550] training loss: 0.00793169
INFO:root:[8,   600] training loss: 0.02793770
INFO:root:[8,   650] training loss: 0.00010713
INFO:root:[8,   700] training loss: 0.00008592
INFO:root:[8,   750] training loss: 0.00008378
INFO:root:[8,   800] training loss: 0.00007086
INFO:root:[8,   850] training loss: 0.00006095
INFO:root:[8,   900] training loss: 0.04374487
INFO:root:[8,   950] training loss: 0.01663480
INFO:root:[8,  1000] training loss: 0.00045516
INFO:root:[8,  1050] training loss: 0.00024152
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.3333    0.2500    0.2857         8
    Prophase     0.7500    0.2466    0.3711        73
   Metaphase     0.2693    1.0000    0.4244      1034
    Anaphase     1.0000    0.3333    0.5000         3

    accuracy                         0.2725      3872
   macro avg     0.3361    0.2614    0.2259      3872
weighted avg     0.0875    0.2725    0.1213      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.04579801
INFO:root:[9,   100] training loss: 0.01806823
INFO:root:[9,   150] training loss: 0.01903947
INFO:root:[9,   200] training loss: 0.01686156
INFO:root:[9,   250] training loss: 0.01839513
INFO:root:[9,   300] training loss: 0.01990429
INFO:root:[9,   350] training loss: 0.02066425
INFO:root:[9,   400] training loss: 0.00029920
INFO:root:[9,   450] training loss: 0.00013768
INFO:root:[9,   500] training loss: 0.00558399
INFO:root:[9,   550] training loss: 0.00739772
INFO:root:[9,   600] training loss: 0.02903954
INFO:root:[9,   650] training loss: 0.00013041
INFO:root:[9,   700] training loss: 0.00010663
INFO:root:[9,   750] training loss: 0.00010789
INFO:root:[9,   800] training loss: 0.00009462
INFO:root:[9,   850] training loss: 0.00007829
INFO:root:[9,   900] training loss: 0.04157923
INFO:root:[9,   950] training loss: 0.01717063
INFO:root:[9,  1000] training loss: 0.00044546
INFO:root:[9,  1050] training loss: 0.00020888
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.1905    0.5000    0.2759         8
    Prophase     0.4412    0.2055    0.2804        73
   Metaphase     0.2705    0.9981    0.4257      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2714      3872
   macro avg     0.1289    0.2434    0.1403      3872
weighted avg     0.0809    0.2714    0.1195      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.04911336
INFO:root:[10,   100] training loss: 0.01830884
INFO:root:[10,   150] training loss: 0.01880976
INFO:root:[10,   200] training loss: 0.01662542
INFO:root:[10,   250] training loss: 0.02145073
INFO:root:[10,   300] training loss: 0.02380020
INFO:root:[10,   350] training loss: 0.01601037
INFO:root:[10,   400] training loss: 0.00053054
INFO:root:[10,   450] training loss: 0.00016224
INFO:root:[10,   500] training loss: 0.00510126
INFO:root:[10,   550] training loss: 0.00773712
INFO:root:[10,   600] training loss: 0.02822281
INFO:root:[10,   650] training loss: 0.00014242
INFO:root:[10,   700] training loss: 0.00011621
INFO:root:[10,   750] training loss: 0.00013304
INFO:root:[10,   800] training loss: 0.00010246
INFO:root:[10,   850] training loss: 0.00007934
INFO:root:[10,   900] training loss: 0.04267193
INFO:root:[10,   950] training loss: 0.01743665
INFO:root:[10,  1000] training loss: 0.00056329
INFO:root:[10,  1050] training loss: 0.00028081
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.3158    0.7500    0.4444         8
    Prophase     0.6000    0.2466    0.3495        73
   Metaphase     0.2703    0.9990    0.4255      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2730      3872
   macro avg     0.1694    0.2851    0.1742      3872
weighted avg     0.0842    0.2730    0.1211      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.04404159
INFO:root:[11,   100] training loss: 0.01882548
INFO:root:[11,   150] training loss: 0.02357242
INFO:root:[11,   200] training loss: 0.01912712
INFO:root:[11,   250] training loss: 0.01935666
INFO:root:[11,   300] training loss: 0.02198851
INFO:root:[11,   350] training loss: 0.01784219
INFO:root:[11,   400] training loss: 0.00071856
INFO:root:[11,   450] training loss: 0.00018907
INFO:root:[11,   500] training loss: 0.00464275
INFO:root:[11,   550] training loss: 0.00786300
INFO:root:[11,   600] training loss: 0.02709879
INFO:root:[11,   650] training loss: 0.00016720
INFO:root:[11,   700] training loss: 0.00013066
INFO:root:[11,   750] training loss: 0.00012613
INFO:root:[11,   800] training loss: 0.00010601
INFO:root:[11,   850] training loss: 0.00008742
INFO:root:[11,   900] training loss: 0.04194929
INFO:root:[11,   950] training loss: 0.01690984
INFO:root:[11,  1000] training loss: 0.00039969
INFO:root:[11,  1050] training loss: 0.00023659
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2500    0.7500    0.3750         8
    Prophase     0.4684    0.5068    0.4868        73
   Metaphase     0.2733    0.9952    0.4288      1034
    Anaphase     1.0000    0.6667    0.8000         3

    accuracy                         0.2774      3872
   macro avg     0.2845    0.4170    0.2987      3872
weighted avg     0.0831    0.2774    0.1251      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.04484012
INFO:root:[12,   100] training loss: 0.01794738
INFO:root:[12,   150] training loss: 0.01925390
INFO:root:[12,   200] training loss: 0.01725330
INFO:root:[12,   250] training loss: 0.01837992
INFO:root:[12,   300] training loss: 0.02036065
INFO:root:[12,   350] training loss: 0.01671683
INFO:root:[12,   400] training loss: 0.00051029
INFO:root:[12,   450] training loss: 0.00017759
INFO:root:[12,   500] training loss: 0.00457090
INFO:root:[12,   550] training loss: 0.00809344
INFO:root:[12,   600] training loss: 0.02677142
INFO:root:[12,   650] training loss: 0.00020626
INFO:root:[12,   700] training loss: 0.00015451
INFO:root:[12,   750] training loss: 0.00014126
INFO:root:[12,   800] training loss: 0.00011315
INFO:root:[12,   850] training loss: 0.00009873
INFO:root:[12,   900] training loss: 0.04097254
INFO:root:[12,   950] training loss: 0.01810067
INFO:root:[12,  1000] training loss: 0.00043917
INFO:root:[12,  1050] training loss: 0.00023574
INFO:root:              precision    recall  f1-score   support

           S     0.3333    0.5000    0.4000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2121    0.8750    0.3415         8
    Prophase     0.4800    0.3288    0.3902        73
   Metaphase     0.2722    0.9961    0.4276      1034
    Anaphase     1.0000    0.6667    0.8000         3

    accuracy                         0.2748      3872
   macro avg     0.3282    0.4809    0.3370      3872
weighted avg     0.0831    0.2748    0.1231      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.04184316
INFO:root:[13,   100] training loss: 0.01664040
INFO:root:[13,   150] training loss: 0.01843997
INFO:root:[13,   200] training loss: 0.01777766
INFO:root:[13,   250] training loss: 0.01913293
INFO:root:[13,   300] training loss: 0.02043356
INFO:root:[13,   350] training loss: 0.01541887
INFO:root:[13,   400] training loss: 0.00042142
INFO:root:[13,   450] training loss: 0.00020592
INFO:root:[13,   500] training loss: 0.00457598
INFO:root:[13,   550] training loss: 0.00796123
INFO:root:[13,   600] training loss: 0.02678492
INFO:root:[13,   650] training loss: 0.00017982
INFO:root:[13,   700] training loss: 0.00013685
INFO:root:[13,   750] training loss: 0.00012664
INFO:root:[13,   800] training loss: 0.00010671
INFO:root:[13,   850] training loss: 0.00009139
INFO:root:[13,   900] training loss: 0.03667581
INFO:root:[13,   950] training loss: 0.01637060
INFO:root:[13,  1000] training loss: 0.00040251
INFO:root:[13,  1050] training loss: 0.00024728
INFO:root:              precision    recall  f1-score   support

           S     0.3333    0.5000    0.4000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2308    0.7500    0.3529         8
    Prophase     0.4057    0.5890    0.4804        73
   Metaphase     0.2755    0.9952    0.4315      1034
    Anaphase     1.0000    0.6667    0.8000         3

    accuracy                         0.2792      3872
   macro avg     0.3208    0.5001    0.3521      3872
weighted avg     0.0826    0.2792    0.1259      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.04122267
INFO:root:[14,   100] training loss: 0.01550372
INFO:root:[14,   150] training loss: 0.01796370
INFO:root:[14,   200] training loss: 0.01649862
INFO:root:[14,   250] training loss: 0.01707003
INFO:root:[14,   300] training loss: 0.01982913
INFO:root:[14,   350] training loss: 0.01566746
INFO:root:[14,   400] training loss: 0.00057316
INFO:root:[14,   450] training loss: 0.00022883
INFO:root:[14,   500] training loss: 0.00454198
INFO:root:[14,   550] training loss: 0.00790486
INFO:root:[14,   600] training loss: 0.02670312
INFO:root:[14,   650] training loss: 0.00021201
INFO:root:[14,   700] training loss: 0.00014790
INFO:root:[14,   750] training loss: 0.00012263
INFO:root:[14,   800] training loss: 0.00010375
INFO:root:[14,   850] training loss: 0.00008822
INFO:root:[14,   900] training loss: 0.03876573
INFO:root:[14,   950] training loss: 0.01599951
INFO:root:[14,  1000] training loss: 0.00041575
INFO:root:[14,  1050] training loss: 0.00025808
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.5000    0.6250    0.5556         8
    Prophase     0.4653    0.6438    0.5402        73
   Metaphase     0.2745    0.9971    0.4305      1034
    Anaphase     1.0000    0.3333    0.5000         3

    accuracy                         0.2805      3872
   macro avg     0.3914    0.5142    0.3847      3872
weighted avg     0.0841    0.2805    0.1270      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.03860678
INFO:root:[15,   100] training loss: 0.01549214
INFO:root:[15,   150] training loss: 0.01839084
INFO:root:[15,   200] training loss: 0.01527458
INFO:root:[15,   250] training loss: 0.01714653
INFO:root:[15,   300] training loss: 0.01657693
INFO:root:[15,   350] training loss: 0.01458070
INFO:root:[15,   400] training loss: 0.00042506
INFO:root:[15,   450] training loss: 0.00013217
INFO:root:[15,   500] training loss: 0.00442881
INFO:root:[15,   550] training loss: 0.00782989
INFO:root:[15,   600] training loss: 0.02746575
INFO:root:[15,   650] training loss: 0.00021605
INFO:root:[15,   700] training loss: 0.00015726
INFO:root:[15,   750] training loss: 0.00014209
INFO:root:[15,   800] training loss: 0.00013365
INFO:root:[15,   850] training loss: 0.00009342
INFO:root:[15,   900] training loss: 0.03354991
INFO:root:[15,   950] training loss: 0.01572770
INFO:root:[15,  1000] training loss: 0.00039382
INFO:root:[15,  1050] training loss: 0.00022782
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.4167    0.6250    0.5000         8
    Prophase     0.4653    0.6438    0.5402        73
   Metaphase     0.2738    0.9952    0.4295      1034
    Anaphase     0.0000    0.0000    0.0000         3

    accuracy                         0.2792      3872
   macro avg     0.1651    0.3234    0.2100      3872
weighted avg     0.0828    0.2792    0.1259      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.03850099
INFO:root:[16,   100] training loss: 0.01619330
INFO:root:[16,   150] training loss: 0.01605681
INFO:root:[16,   200] training loss: 0.01697636
INFO:root:[16,   250] training loss: 0.01397657
INFO:root:[16,   300] training loss: 0.01760418
INFO:root:[16,   350] training loss: 0.01454984
INFO:root:[16,   400] training loss: 0.00034076
INFO:root:[16,   450] training loss: 0.00016190
INFO:root:[16,   500] training loss: 0.00438174
INFO:root:[16,   550] training loss: 0.00799593
INFO:root:[16,   600] training loss: 0.02793339
INFO:root:[16,   650] training loss: 0.00030003
INFO:root:[16,   700] training loss: 0.00020469
INFO:root:[16,   750] training loss: 0.00018865
INFO:root:[16,   800] training loss: 0.00013746
INFO:root:[16,   850] training loss: 0.00011730
INFO:root:[16,   900] training loss: 0.03159432
INFO:root:[16,   950] training loss: 0.01596561
INFO:root:[16,  1000] training loss: 0.00027910
INFO:root:[16,  1050] training loss: 0.00016937
INFO:root:              precision    recall  f1-score   support

           S     0.5000    0.5000    0.5000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.3846    0.6250    0.4762         8
    Prophase     0.3876    0.6849    0.4950        73
   Metaphase     0.2766    0.9971    0.4331      1034
    Anaphase     1.0000    0.3333    0.5000         3

    accuracy                         0.2810      3872
   macro avg     0.3641    0.4486    0.3435      3872
weighted avg     0.0830    0.2810    0.1266      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.03907026
INFO:root:[17,   100] training loss: 0.02085625
INFO:root:[17,   150] training loss: 0.02230867
INFO:root:[17,   200] training loss: 0.01954367
INFO:root:[17,   250] training loss: 0.01942262
INFO:root:[17,   300] training loss: 0.02042856
INFO:root:[17,   350] training loss: 0.01684540
INFO:root:[17,   400] training loss: 0.00017796
INFO:root:[17,   450] training loss: 0.00029056
INFO:root:[17,   500] training loss: 0.00457580
INFO:root:[17,   550] training loss: 0.00798534
INFO:root:[17,   600] training loss: 0.02876214
INFO:root:[17,   650] training loss: 0.00029304
INFO:root:[17,   700] training loss: 0.00020628
INFO:root:[17,   750] training loss: 0.00016893
INFO:root:[17,   800] training loss: 0.00013958
INFO:root:[17,   850] training loss: 0.00011759
INFO:root:[17,   900] training loss: 0.03678444
INFO:root:[17,   950] training loss: 0.01603472
INFO:root:[17,  1000] training loss: 0.00029659
INFO:root:[17,  1050] training loss: 0.00018925
INFO:root:              precision    recall  f1-score   support

           S     0.2500    0.5000    0.3333         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2308    0.7500    0.3529         8
    Prophase     0.2747    0.6849    0.3922        73
   Metaphase     0.2812    0.9952    0.4385      1034
    Anaphase     1.0000    0.3333    0.5000         3

    accuracy                         0.2807      3872
   macro avg     0.2910    0.4662    0.2881      3872
weighted avg     0.0817    0.2807    0.1258      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.03978939
INFO:root:[18,   100] training loss: 0.01552592
INFO:root:[18,   150] training loss: 0.01879006
INFO:root:[18,   200] training loss: 0.01656197
INFO:root:[18,   250] training loss: 0.01559445
INFO:root:[18,   300] training loss: 0.01786201
INFO:root:[18,   350] training loss: 0.01769954
INFO:root:[18,   400] training loss: 0.00017295
INFO:root:[18,   450] training loss: 0.00009943
INFO:root:[18,   500] training loss: 0.00459466
INFO:root:[18,   550] training loss: 0.00792420
INFO:root:[18,   600] training loss: 0.02948218
INFO:root:[18,   650] training loss: 0.00028305
INFO:root:[18,   700] training loss: 0.00021296
INFO:root:[18,   750] training loss: 0.00017950
INFO:root:[18,   800] training loss: 0.00014493
INFO:root:[18,   850] training loss: 0.00012167
INFO:root:[18,   900] training loss: 0.03736866
INFO:root:[18,   950] training loss: 0.01661237
INFO:root:[18,  1000] training loss: 0.00023377
INFO:root:[18,  1050] training loss: 0.00016288
INFO:root:              precision    recall  f1-score   support

           S     0.3333    1.0000    0.5000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2059    0.8750    0.3333         8
    Prophase     0.2283    0.6849    0.3425        73
   Metaphase     0.2831    0.9884    0.4401      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2800      3872
   macro avg     0.2929    0.6498    0.3737      3872
weighted avg     0.0813    0.2800    0.1257      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.03829046
INFO:root:[19,   100] training loss: 0.01565305
INFO:root:[19,   150] training loss: 0.01637318
INFO:root:[19,   200] training loss: 0.01418610
INFO:root:[19,   250] training loss: 0.01396396
INFO:root:[19,   300] training loss: 0.01639138
INFO:root:[19,   350] training loss: 0.01536759
INFO:root:[19,   400] training loss: 0.00016597
INFO:root:[19,   450] training loss: 0.00012548
INFO:root:[19,   500] training loss: 0.00453068
INFO:root:[19,   550] training loss: 0.00829892
INFO:root:[19,   600] training loss: 0.02978737
INFO:root:[19,   650] training loss: 0.00033621
INFO:root:[19,   700] training loss: 0.00023196
INFO:root:[19,   750] training loss: 0.00019423
INFO:root:[19,   800] training loss: 0.00015755
INFO:root:[19,   850] training loss: 0.00013214
INFO:root:[19,   900] training loss: 0.03687299
INFO:root:[19,   950] training loss: 0.01743708
INFO:root:[19,  1000] training loss: 0.00033109
INFO:root:[19,  1050] training loss: 0.00020888
INFO:root:              precision    recall  f1-score   support

           S     0.3333    0.5000    0.4000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.3333    0.5000    0.4000         8
    Prophase     0.3256    0.7671    0.4571        73
   Metaphase     0.2791    0.9942    0.4359      1034
    Anaphase     1.0000    0.6667    0.8000         3

    accuracy                         0.2818      3872
   macro avg     0.3245    0.4897    0.3561      3872
weighted avg     0.0823    0.2818    0.1267      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.03757936
INFO:root:[20,   100] training loss: 0.01548992
INFO:root:[20,   150] training loss: 0.01730622
INFO:root:[20,   200] training loss: 0.01437265
INFO:root:[20,   250] training loss: 0.01503109
INFO:root:[20,   300] training loss: 0.01752196
INFO:root:[20,   350] training loss: 0.01388714
INFO:root:[20,   400] training loss: 0.00026619
INFO:root:[20,   450] training loss: 0.00008058
INFO:root:[20,   500] training loss: 0.00457662
INFO:root:[20,   550] training loss: 0.00726811
INFO:root:[20,   600] training loss: 0.02783209
INFO:root:[20,   650] training loss: 0.00039683
INFO:root:[20,   700] training loss: 0.00026694
INFO:root:[20,   750] training loss: 0.00025158
INFO:root:[20,   800] training loss: 0.00018234
INFO:root:[20,   850] training loss: 0.00014526
INFO:root:[20,   900] training loss: 0.03319191
INFO:root:[20,   950] training loss: 0.01720754
INFO:root:[20,  1000] training loss: 0.00034275
INFO:root:[20,  1050] training loss: 0.00021614
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2609    0.7500    0.3871         8
    Prophase     0.3562    0.7123    0.4749        73
   Metaphase     0.2778    0.9942    0.4343      1034
    Anaphase     1.0000    0.6667    0.8000         3

    accuracy                         0.2810      3872
   macro avg     0.2707    0.4462    0.2995      3872
weighted avg     0.0822    0.2810    0.1264      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.04000963
INFO:root:[21,   100] training loss: 0.01694309
INFO:root:[21,   150] training loss: 0.01556090
INFO:root:[21,   200] training loss: 0.01523350
INFO:root:[21,   250] training loss: 0.01624733
INFO:root:[21,   300] training loss: 0.01922587
INFO:root:[21,   350] training loss: 0.01596614
INFO:root:[21,   400] training loss: 0.00024783
INFO:root:[21,   450] training loss: 0.00013796
INFO:root:[21,   500] training loss: 0.00469722
INFO:root:[21,   550] training loss: 0.00782017
INFO:root:[21,   600] training loss: 0.02912698
INFO:root:[21,   650] training loss: 0.00040332
INFO:root:[21,   700] training loss: 0.00027792
INFO:root:[21,   750] training loss: 0.00023939
INFO:root:[21,   800] training loss: 0.00018676
INFO:root:[21,   850] training loss: 0.00015241
INFO:root:[21,   900] training loss: 0.03252276
INFO:root:[21,   950] training loss: 0.01719727
INFO:root:[21,  1000] training loss: 0.00024430
INFO:root:[21,  1050] training loss: 0.00016856
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2400    0.7500    0.3636         8
    Prophase     0.2692    0.6712    0.3843        73
   Metaphase     0.2801    0.9913    0.4368      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2802      3872
   macro avg     0.3509    0.6304    0.4264      3872
weighted avg     0.0815    0.2802    0.1258      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.03594008
INFO:root:[22,   100] training loss: 0.01602305
INFO:root:[22,   150] training loss: 0.01687670
INFO:root:[22,   200] training loss: 0.01448584
INFO:root:[22,   250] training loss: 0.01509457
INFO:root:[22,   300] training loss: 0.01737100
INFO:root:[22,   350] training loss: 0.01478045
INFO:root:[22,   400] training loss: 0.00012783
INFO:root:[22,   450] training loss: 0.00014421
INFO:root:[22,   500] training loss: 0.00456965
INFO:root:[22,   550] training loss: 0.00734624
INFO:root:[22,   600] training loss: 0.02826172
INFO:root:[22,   650] training loss: 0.00045663
INFO:root:[22,   700] training loss: 0.00029562
INFO:root:[22,   750] training loss: 0.00023756
INFO:root:[22,   800] training loss: 0.00018689
INFO:root:[22,   850] training loss: 0.00015950
INFO:root:[22,   900] training loss: 0.03202073
INFO:root:[22,   950] training loss: 0.01773684
INFO:root:[22,  1000] training loss: 0.00019084
INFO:root:[22,  1050] training loss: 0.00013145
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2800    0.8750    0.4242         8
    Prophase     0.2203    0.7123    0.3366        73
   Metaphase     0.2829    0.9865    0.4397      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2800      3872
   macro avg     0.3500    0.6534    0.4287      3872
weighted avg     0.0814    0.2800    0.1258      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.03439836
INFO:root:[23,   100] training loss: 0.01566426
INFO:root:[23,   150] training loss: 0.01523697
INFO:root:[23,   200] training loss: 0.01381506
INFO:root:[23,   250] training loss: 0.01560207
INFO:root:[23,   300] training loss: 0.02063651
INFO:root:[23,   350] training loss: 0.01393668
INFO:root:[23,   400] training loss: 0.00017537
INFO:root:[23,   450] training loss: 0.00012308
INFO:root:[23,   500] training loss: 0.00416217
INFO:root:[23,   550] training loss: 0.00724339
INFO:root:[23,   600] training loss: 0.02732982
INFO:root:[23,   650] training loss: 0.00045903
INFO:root:[23,   700] training loss: 0.00029533
INFO:root:[23,   750] training loss: 0.00023447
INFO:root:[23,   800] training loss: 0.00019152
INFO:root:[23,   850] training loss: 0.00016784
INFO:root:[23,   900] training loss: 0.02952751
INFO:root:[23,   950] training loss: 0.01559740
INFO:root:[23,  1000] training loss: 0.00012173
INFO:root:[23,  1050] training loss: 0.00006916
INFO:root:              precision    recall  f1-score   support

           S     0.4000    1.0000    0.5714         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2069    0.7500    0.3243         8
    Prophase     0.2407    0.7123    0.3599        73
   Metaphase     0.2805    0.9816    0.4364      1034
    Anaphase     0.7500    1.0000    0.8571         3

    accuracy                         0.2784      3872
   macro avg     0.2683    0.6349    0.3642      3872
weighted avg     0.0807    0.2784    0.1249      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.03390185
INFO:root:[24,   100] training loss: 0.01436772
INFO:root:[24,   150] training loss: 0.01603976
INFO:root:[24,   200] training loss: 0.01572541
INFO:root:[24,   250] training loss: 0.01578671
INFO:root:[24,   300] training loss: 0.01583625
INFO:root:[24,   350] training loss: 0.01319508
INFO:root:[24,   400] training loss: 0.00025300
INFO:root:[24,   450] training loss: 0.00009907
INFO:root:[24,   500] training loss: 0.00408146
INFO:root:[24,   550] training loss: 0.00675659
INFO:root:[24,   600] training loss: 0.02676778
INFO:root:[24,   650] training loss: 0.00049008
INFO:root:[24,   700] training loss: 0.00032324
INFO:root:[24,   750] training loss: 0.00026688
INFO:root:[24,   800] training loss: 0.00021003
INFO:root:[24,   850] training loss: 0.00016746
INFO:root:[24,   900] training loss: 0.02934122
INFO:root:[24,   950] training loss: 0.01679688
INFO:root:[24,  1000] training loss: 0.00019866
INFO:root:[24,  1050] training loss: 0.00015180
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2333    0.8750    0.3684         8
    Prophase     0.2233    0.6575    0.3333        73
   Metaphase     0.2820    0.9874    0.4387      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2792      3872
   macro avg     0.3436    0.6457    0.4201      3872
weighted avg     0.0811    0.2792    0.1254      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.03491282
INFO:root:[25,   100] training loss: 0.01492929
INFO:root:[25,   150] training loss: 0.01495994
INFO:root:[25,   200] training loss: 0.01450508
INFO:root:[25,   250] training loss: 0.01476383
INFO:root:[25,   300] training loss: 0.01763769
INFO:root:[25,   350] training loss: 0.01398480
INFO:root:[25,   400] training loss: 0.00020048
INFO:root:[25,   450] training loss: 0.00005305
INFO:root:[25,   500] training loss: 0.00407681
INFO:root:[25,   550] training loss: 0.00638108
INFO:root:[25,   600] training loss: 0.02561115
INFO:root:[25,   650] training loss: 0.00047820
INFO:root:[25,   700] training loss: 0.00031473
INFO:root:[25,   750] training loss: 0.00026492
INFO:root:[25,   800] training loss: 0.00020339
INFO:root:[25,   850] training loss: 0.00016197
INFO:root:[25,   900] training loss: 0.02933481
INFO:root:[25,   950] training loss: 0.01526393
INFO:root:[25,  1000] training loss: 0.00018592
INFO:root:[25,  1050] training loss: 0.00012999
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.3158    0.7500    0.4444         8
    Prophase     0.2573    0.7260    0.3799        73
   Metaphase     0.2803    0.9865    0.4366      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2800      3872
   macro avg     0.3362    0.6375    0.4182      3872
weighted avg     0.0814    0.2800    0.1258      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.03124728
INFO:root:[26,   100] training loss: 0.01442345
INFO:root:[26,   150] training loss: 0.01454863
INFO:root:[26,   200] training loss: 0.01423925
INFO:root:[26,   250] training loss: 0.01797946
INFO:root:[26,   300] training loss: 0.01806076
INFO:root:[26,   350] training loss: 0.01376382
INFO:root:[26,   400] training loss: 0.00027186
INFO:root:[26,   450] training loss: 0.00006604
INFO:root:[26,   500] training loss: 0.00416914
INFO:root:[26,   550] training loss: 0.00645817
INFO:root:[26,   600] training loss: 0.02449157
INFO:root:[26,   650] training loss: 0.00052476
INFO:root:[26,   700] training loss: 0.00032366
INFO:root:[26,   750] training loss: 0.00028188
INFO:root:[26,   800] training loss: 0.00021029
INFO:root:[26,   850] training loss: 0.00016803
INFO:root:[26,   900] training loss: 0.02750671
INFO:root:[26,   950] training loss: 0.01325517
INFO:root:[26,  1000] training loss: 0.00017742
INFO:root:[26,  1050] training loss: 0.00012692
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.2414    0.8750    0.3784         8
    Prophase     0.2513    0.6712    0.3657        73
   Metaphase     0.2782    0.9797    0.4334      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2774      3872
   macro avg     0.3244    0.6466    0.4063      3872
weighted avg     0.0806    0.2774    0.1245      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.03005756
INFO:root:[27,   100] training loss: 0.01467608
INFO:root:[27,   150] training loss: 0.01438084
INFO:root:[27,   200] training loss: 0.01362513
INFO:root:[27,   250] training loss: 0.01388083
INFO:root:[27,   300] training loss: 0.01688614
INFO:root:[27,   350] training loss: 0.01310872
INFO:root:[27,   400] training loss: 0.00027417
INFO:root:[27,   450] training loss: 0.00024386
INFO:root:[27,   500] training loss: 0.00387795
INFO:root:[27,   550] training loss: 0.00500660
INFO:root:[27,   600] training loss: 0.02108894
INFO:root:[27,   650] training loss: 0.00045249
INFO:root:[27,   700] training loss: 0.00028723
INFO:root:[27,   750] training loss: 0.00025084
INFO:root:[27,   800] training loss: 0.00019065
INFO:root:[27,   850] training loss: 0.00014621
INFO:root:[27,   900] training loss: 0.02524422
INFO:root:[27,   950] training loss: 0.01213708
INFO:root:[27,  1000] training loss: 0.00015920
INFO:root:[27,  1050] training loss: 0.00011020
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.5000    0.0010    0.0019      1032
          G1     0.1400    0.8750    0.2414         8
    Prophase     0.2195    0.6164    0.3237        73
   Metaphase     0.2822    0.9845    0.4387      1034
    Anaphase     0.7500    1.0000    0.8571         3

    accuracy                         0.2779      3872
   macro avg     0.3417    0.6396    0.3614      3872
weighted avg     0.2139    0.2779    0.1253      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.02732084
INFO:root:[28,   100] training loss: 0.01430095
INFO:root:[28,   150] training loss: 0.01424835
INFO:root:[28,   200] training loss: 0.01273074
INFO:root:[28,   250] training loss: 0.01376082
INFO:root:[28,   300] training loss: 0.01474027
INFO:root:[28,   350] training loss: 0.01347233
INFO:root:[28,   400] training loss: 0.00005665
INFO:root:[28,   450] training loss: 0.00010608
INFO:root:[28,   500] training loss: 0.00352622
INFO:root:[28,   550] training loss: 0.00469488
INFO:root:[28,   600] training loss: 0.01848745
INFO:root:[28,   650] training loss: 0.00036529
INFO:root:[28,   700] training loss: 0.00021104
INFO:root:[28,   750] training loss: 0.00018102
INFO:root:[28,   800] training loss: 0.00012112
INFO:root:[28,   850] training loss: 0.00007336
INFO:root:[28,   900] training loss: 0.02478267
INFO:root:[28,   950] training loss: 0.00996352
INFO:root:[28,  1000] training loss: 0.00014386
INFO:root:[28,  1050] training loss: 0.00008944
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.7500    0.0029    0.0058      1032
          G1     0.2632    0.6250    0.3704         8
    Prophase     0.2849    0.7260    0.4093        73
   Metaphase     0.2784    0.9845    0.4341      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2800      3872
   macro avg     0.4395    0.6198    0.4123      3872
weighted avg     0.2812    0.2800    0.1271      3872

INFO:root:Accuracy of the network on the 3872 validation images: 27 %
INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.02566189
INFO:root:[29,   100] training loss: 0.01399690
INFO:root:[29,   150] training loss: 0.01508668
INFO:root:[29,   200] training loss: 0.01282924
INFO:root:[29,   250] training loss: 0.01331168
INFO:root:[29,   300] training loss: 0.01557520
INFO:root:[29,   350] training loss: 0.01266633
INFO:root:[29,   400] training loss: 0.00010129
INFO:root:[29,   450] training loss: 0.00010372
INFO:root:[29,   500] training loss: 0.00374305
INFO:root:[29,   550] training loss: 0.00456244
INFO:root:[29,   600] training loss: 0.01748306
INFO:root:[29,   650] training loss: 0.00029769
INFO:root:[29,   700] training loss: 0.00017211
INFO:root:[29,   750] training loss: 0.00017940
INFO:root:[29,   800] training loss: 0.00011342
INFO:root:[29,   850] training loss: 0.00009178
INFO:root:[29,   900] training loss: 0.02224116
INFO:root:[29,   950] training loss: 0.01360847
INFO:root:[29,  1000] training loss: 0.00022697
INFO:root:[29,  1050] training loss: 0.00014836
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.0000    0.0000    0.0000      1032
          G1     0.4167    0.6250    0.5000         8
    Prophase     0.4161    0.7808    0.5429        73
   Metaphase     0.2773    0.9961    0.4338      1034
    Anaphase     0.7500    1.0000    0.8571         3

    accuracy                         0.2833      3872
   macro avg     0.3609    0.6289    0.4477      3872
weighted avg     0.0837    0.2833    0.1282      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.02554564
INFO:root:[30,   100] training loss: 0.01399948
INFO:root:[30,   150] training loss: 0.01531380
INFO:root:[30,   200] training loss: 0.01236594
INFO:root:[30,   250] training loss: 0.01390027
INFO:root:[30,   300] training loss: 0.01478618
INFO:root:[30,   350] training loss: 0.01233086
INFO:root:[30,   400] training loss: 0.00010701
INFO:root:[30,   450] training loss: 0.00006605
INFO:root:[30,   500] training loss: 0.00322669
INFO:root:[30,   550] training loss: 0.00347013
INFO:root:[30,   600] training loss: 0.01556379
INFO:root:[30,   650] training loss: 0.00031201
INFO:root:[30,   700] training loss: 0.00017762
INFO:root:[30,   750] training loss: 0.00017634
INFO:root:[30,   800] training loss: 0.00011202
INFO:root:[30,   850] training loss: 0.00007351
INFO:root:[30,   900] training loss: 0.02281798
INFO:root:[30,   950] training loss: 0.01096643
INFO:root:[30,  1000] training loss: 0.00019218
INFO:root:[30,  1050] training loss: 0.00011907
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.7143    0.0048    0.0096      1032
          G1     0.2778    0.6250    0.3846         8
    Prophase     0.2827    0.7397    0.4091        73
   Metaphase     0.2799    0.9874    0.4361      1034
    Anaphase     0.6000    1.0000    0.7500         3

    accuracy                         0.2815      3872
   macro avg     0.4030    0.6224    0.3985      3872
weighted avg     0.2718    0.2815    0.1285      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.02345073
INFO:root:[31,   100] training loss: 0.01412533
INFO:root:[31,   150] training loss: 0.01373643
INFO:root:[31,   200] training loss: 0.01319565
INFO:root:[31,   250] training loss: 0.01533527
INFO:root:[31,   300] training loss: 0.01955105
INFO:root:[31,   350] training loss: 0.01279102
INFO:root:[31,   400] training loss: 0.00013063
INFO:root:[31,   450] training loss: 0.00006741
INFO:root:[31,   500] training loss: 0.00311848
INFO:root:[31,   550] training loss: 0.00348853
INFO:root:[31,   600] training loss: 0.01489716
INFO:root:[31,   650] training loss: 0.00043995
INFO:root:[31,   700] training loss: 0.00025372
INFO:root:[31,   750] training loss: 0.00023137
INFO:root:[31,   800] training loss: 0.00017543
INFO:root:[31,   850] training loss: 0.00014149
INFO:root:[31,   900] training loss: 0.02236746
INFO:root:[31,   950] training loss: 0.00975845
INFO:root:[31,  1000] training loss: 0.00013480
INFO:root:[31,  1050] training loss: 0.00008157
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.0000    0.0000    0.0000      1720
   Telophase     0.8667    0.0252    0.0490      1032
          G1     0.3333    0.8750    0.4828         8
    Prophase     0.1853    0.7260    0.2953        73
   Metaphase     0.2876    0.9807    0.4447      1034
    Anaphase     0.6000    1.0000    0.7500         3

    accuracy                         0.2854      3872
   macro avg     0.3961    0.6581    0.3841      3872
weighted avg     0.3127    0.2854    0.1393      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.02314278
INFO:root:[32,   100] training loss: 0.01574227
INFO:root:[32,   150] training loss: 0.01382113
INFO:root:[32,   200] training loss: 0.01223297
INFO:root:[32,   250] training loss: 0.01357879
INFO:root:[32,   300] training loss: 0.01618510
INFO:root:[32,   350] training loss: 0.01255696
INFO:root:[32,   400] training loss: 0.00025108
INFO:root:[32,   450] training loss: 0.00008279
INFO:root:[32,   500] training loss: 0.00352575
INFO:root:[32,   550] training loss: 0.00245995
INFO:root:[32,   600] training loss: 0.01282608
INFO:root:[32,   650] training loss: 0.00038249
INFO:root:[32,   700] training loss: 0.00023036
INFO:root:[32,   750] training loss: 0.00020261
INFO:root:[32,   800] training loss: 0.00014549
INFO:root:[32,   850] training loss: 0.00011181
INFO:root:[32,   900] training loss: 0.02252897
INFO:root:[32,   950] training loss: 0.00993286
INFO:root:[32,  1000] training loss: 0.00012745
INFO:root:[32,  1050] training loss: 0.00006262
INFO:root:              precision    recall  f1-score   support

           S     0.3333    1.0000    0.5000         2
          G2     1.0000    0.0035    0.0070      1720
   Telophase     0.9000    0.0174    0.0342      1032
          G1     0.2581    1.0000    0.4103         8
    Prophase     0.2016    0.6986    0.3129        73
   Metaphase     0.2841    0.9768    0.4402      1034
    Anaphase     1.0000    0.3333    0.5000         3

    accuracy                         0.2831      3872
   macro avg     0.5682    0.5757    0.3149      3872
weighted avg     0.7652    0.2831    0.1372      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.02637692
INFO:root:[33,   100] training loss: 0.01453106
INFO:root:[33,   150] training loss: 0.01418041
INFO:root:[33,   200] training loss: 0.01364470
INFO:root:[33,   250] training loss: 0.01335058
INFO:root:[33,   300] training loss: 0.01649878
INFO:root:[33,   350] training loss: 0.01372863
INFO:root:[33,   400] training loss: 0.00015450
INFO:root:[33,   450] training loss: 0.00007468
INFO:root:[33,   500] training loss: 0.00245815
INFO:root:[33,   550] training loss: 0.00232626
INFO:root:[33,   600] training loss: 0.01129959
INFO:root:[33,   650] training loss: 0.00032293
INFO:root:[33,   700] training loss: 0.00020296
INFO:root:[33,   750] training loss: 0.00019682
INFO:root:[33,   800] training loss: 0.00014478
INFO:root:[33,   850] training loss: 0.00011384
INFO:root:[33,   900] training loss: 0.01988688
INFO:root:[33,   950] training loss: 0.00819084
INFO:root:[33,  1000] training loss: 0.00014727
INFO:root:[33,  1050] training loss: 0.00007368
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     1.0000    0.0023    0.0046      1720
   Telophase     0.8421    0.0465    0.0882      1032
          G1     0.2069    0.7500    0.3243         8
    Prophase     0.1244    0.7397    0.2130        73
   Metaphase     0.2903    0.9381    0.4434      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2807      3872
   macro avg     0.5663    0.6395    0.3915      3872
weighted avg     0.7500    0.2807    0.1498      3872

INFO:root:Accuracy of the network on the 3872 validation images: 28 %
INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.02129588
INFO:root:[34,   100] training loss: 0.01420548
INFO:root:[34,   150] training loss: 0.01454112
INFO:root:[34,   200] training loss: 0.01273311
INFO:root:[34,   250] training loss: 0.01282002
INFO:root:[34,   300] training loss: 0.01455398
INFO:root:[34,   350] training loss: 0.01318162
INFO:root:[34,   400] training loss: 0.00016506
INFO:root:[34,   450] training loss: 0.00007623
INFO:root:[34,   500] training loss: 0.00174890
INFO:root:[34,   550] training loss: 0.00165214
INFO:root:[34,   600] training loss: 0.00903518
INFO:root:[34,   650] training loss: 0.00031454
INFO:root:[34,   700] training loss: 0.00020337
INFO:root:[34,   750] training loss: 0.00023738
INFO:root:[34,   800] training loss: 0.00015919
INFO:root:[34,   850] training loss: 0.00014717
INFO:root:[34,   900] training loss: 0.01916791
INFO:root:[34,   950] training loss: 0.00765065
INFO:root:[34,  1000] training loss: 0.00011735
INFO:root:[34,  1050] training loss: 0.00005248
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     1.0000    0.0023    0.0046      1720
   Telophase     0.8529    0.1686    0.2816      1032
          G1     0.2333    0.8750    0.3684         8
    Prophase     0.0945    0.7808    0.1686        73
   Metaphase     0.2982    0.8723    0.4444      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2967      3872
   macro avg     0.5922    0.6713    0.4382      3872
weighted avg     0.7546    0.2967    0.2009      3872

INFO:root:Accuracy of the network on the 3872 validation images: 29 %
INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.01967227
INFO:root:[35,   100] training loss: 0.01422442
INFO:root:[35,   150] training loss: 0.01271765
INFO:root:[35,   200] training loss: 0.01214789
INFO:root:[35,   250] training loss: 0.01230993
INFO:root:[35,   300] training loss: 0.01452375
INFO:root:[35,   350] training loss: 0.01129064
INFO:root:[35,   400] training loss: 0.00030290
INFO:root:[35,   450] training loss: 0.00006498
INFO:root:[35,   500] training loss: 0.00174124
INFO:root:[35,   550] training loss: 0.00144945
INFO:root:[35,   600] training loss: 0.00725250
INFO:root:[35,   650] training loss: 0.00032305
INFO:root:[35,   700] training loss: 0.00019987
INFO:root:[35,   750] training loss: 0.00024288
INFO:root:[35,   800] training loss: 0.00016432
INFO:root:[35,   850] training loss: 0.00011765
INFO:root:[35,   900] training loss: 0.01866338
INFO:root:[35,   950] training loss: 0.00736539
INFO:root:[35,  1000] training loss: 0.00011413
INFO:root:[35,  1050] training loss: 0.00006121
INFO:root:              precision    recall  f1-score   support

           S     0.0000    0.0000    0.0000         2
          G2     1.0000    0.0006    0.0012      1720
   Telophase     0.7860    0.2064    0.3269      1032
          G1     0.1481    1.0000    0.2581         8
    Prophase     0.0823    0.6575    0.1463        73
   Metaphase     0.2980    0.8530    0.4417      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.2983      3872
   macro avg     0.4735    0.5311    0.3106      3872
weighted avg     0.7359    0.2983    0.2097      3872

INFO:root:Accuracy of the network on the 3872 validation images: 29 %
INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.02093295
INFO:root:[36,   100] training loss: 0.01350090
INFO:root:[36,   150] training loss: 0.01278970
INFO:root:[36,   200] training loss: 0.01208793
INFO:root:[36,   250] training loss: 0.01236180
INFO:root:[36,   300] training loss: 0.01501077
INFO:root:[36,   350] training loss: 0.01315385
INFO:root:[36,   400] training loss: 0.00006348
INFO:root:[36,   450] training loss: 0.00007401
INFO:root:[36,   500] training loss: 0.00133892
INFO:root:[36,   550] training loss: 0.00122523
INFO:root:[36,   600] training loss: 0.00718916
INFO:root:[36,   650] training loss: 0.00030608
INFO:root:[36,   700] training loss: 0.00018646
INFO:root:[36,   750] training loss: 0.00024803
INFO:root:[36,   800] training loss: 0.00016917
INFO:root:[36,   850] training loss: 0.00010935
INFO:root:[36,   900] training loss: 0.02117635
INFO:root:[36,   950] training loss: 0.00721817
INFO:root:[36,  1000] training loss: 0.00012517
INFO:root:[36,  1050] training loss: 0.00004292
INFO:root:              precision    recall  f1-score   support

           S     0.5000    0.5000    0.5000         2
          G2     1.0000    0.0052    0.0104      1720
   Telophase     0.8553    0.1890    0.3095      1032
          G1     0.2941    0.6250    0.4000         8
    Prophase     0.1057    0.8904    0.1890        73
   Metaphase     0.3009    0.8723    0.4474      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.3048      3872
   macro avg     0.5794    0.5831    0.4080      3872
weighted avg     0.7561    0.3048    0.2120      3872

INFO:root:Accuracy of the network on the 3872 validation images: 30 %
INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.01902340
INFO:root:[37,   100] training loss: 0.01289540
INFO:root:[37,   150] training loss: 0.01278727
INFO:root:[37,   200] training loss: 0.01175247
INFO:root:[37,   250] training loss: 0.01337824
INFO:root:[37,   300] training loss: 0.01302703
INFO:root:[37,   350] training loss: 0.01171119
INFO:root:[37,   400] training loss: 0.00011659
INFO:root:[37,   450] training loss: 0.00007930
INFO:root:[37,   500] training loss: 0.00110621
INFO:root:[37,   550] training loss: 0.00099029
INFO:root:[37,   600] training loss: 0.00655683
INFO:root:[37,   650] training loss: 0.00025399
INFO:root:[37,   700] training loss: 0.00016784
INFO:root:[37,   750] training loss: 0.00025539
INFO:root:[37,   800] training loss: 0.00015874
INFO:root:[37,   850] training loss: 0.00011328
INFO:root:[37,   900] training loss: 0.01855392
INFO:root:[37,   950] training loss: 0.00561431
INFO:root:[37,  1000] training loss: 0.00013328
INFO:root:[37,  1050] training loss: 0.00004452
INFO:root:              precision    recall  f1-score   support

           S     0.3333    1.0000    0.5000         2
          G2     1.0000    0.0006    0.0012      1720
   Telophase     0.7477    0.3130    0.4413      1032
          G1     0.3571    0.6250    0.4545         8
    Prophase     0.1079    0.9178    0.1931        73
   Metaphase     0.2934    0.7930    0.4283      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.3153      3872
   macro avg     0.5485    0.6642    0.4312      3872
weighted avg     0.7256    0.3153    0.2381      3872

INFO:root:Accuracy of the network on the 3872 validation images: 31 %
INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.01946804
INFO:root:[38,   100] training loss: 0.01322500
INFO:root:[38,   150] training loss: 0.01333771
INFO:root:[38,   200] training loss: 0.01153952
INFO:root:[38,   250] training loss: 0.01464262
INFO:root:[38,   300] training loss: 0.01651214
INFO:root:[38,   350] training loss: 0.01337061
INFO:root:[38,   400] training loss: 0.00010246
INFO:root:[38,   450] training loss: 0.00007268
INFO:root:[38,   500] training loss: 0.00128672
INFO:root:[38,   550] training loss: 0.00102240
INFO:root:[38,   600] training loss: 0.00574830
INFO:root:[38,   650] training loss: 0.00028714
INFO:root:[38,   700] training loss: 0.00019026
INFO:root:[38,   750] training loss: 0.00030720
INFO:root:[38,   800] training loss: 0.00017861
INFO:root:[38,   850] training loss: 0.00014163
INFO:root:[38,   900] training loss: 0.01903510
INFO:root:[38,   950] training loss: 0.00557201
INFO:root:[38,  1000] training loss: 0.00011716
INFO:root:[38,  1050] training loss: 0.00003684
INFO:root:              precision    recall  f1-score   support

           S     0.4000    1.0000    0.5714         2
          G2     1.0000    0.0041    0.0081      1720
   Telophase     0.7973    0.4041    0.5363      1032
          G1     0.3333    0.7500    0.4615         8
    Prophase     0.1171    0.8082    0.2045        73
   Metaphase     0.2941    0.7998    0.4301      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.3412      3872
   macro avg     0.5631    0.6809    0.4589      3872
weighted avg     0.7391    0.3412    0.2673      3872

INFO:root:Accuracy of the network on the 3872 validation images: 34 %
INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.01710540
INFO:root:[39,   100] training loss: 0.01377297
INFO:root:[39,   150] training loss: 0.01375371
INFO:root:[39,   200] training loss: 0.01103113
INFO:root:[39,   250] training loss: 0.01219299
INFO:root:[39,   300] training loss: 0.01374886
INFO:root:[39,   350] training loss: 0.01091005
INFO:root:[39,   400] training loss: 0.00012824
INFO:root:[39,   450] training loss: 0.00005289
INFO:root:[39,   500] training loss: 0.00100095
INFO:root:[39,   550] training loss: 0.00082848
INFO:root:[39,   600] training loss: 0.00552263
INFO:root:[39,   650] training loss: 0.00029374
INFO:root:[39,   700] training loss: 0.00017203
INFO:root:[39,   750] training loss: 0.00027136
INFO:root:[39,   800] training loss: 0.00016718
INFO:root:[39,   850] training loss: 0.00011175
INFO:root:[39,   900] training loss: 0.01769615
INFO:root:[39,   950] training loss: 0.00460606
INFO:root:[39,  1000] training loss: 0.00008376
INFO:root:[39,  1050] training loss: 0.00003333
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     1.0000    0.0029    0.0058      1720
   Telophase     0.7568    0.4612    0.5731      1032
          G1     0.3750    0.7500    0.5000         8
    Prophase     0.1309    0.9178    0.2291        73
   Metaphase     0.2945    0.7698    0.4260      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.3499      3872
   macro avg     0.5796    0.7003    0.4858      3872
weighted avg     0.7288    0.3499    0.2756      3872

INFO:root:Accuracy of the network on the 3872 validation images: 34 %
INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.01651732
INFO:root:[40,   100] training loss: 0.01210491
INFO:root:[40,   150] training loss: 0.01193513
INFO:root:[40,   200] training loss: 0.01170623
INFO:root:[40,   250] training loss: 0.01189146
INFO:root:[40,   300] training loss: 0.01304529
INFO:root:[40,   350] training loss: 0.01139591
INFO:root:[40,   400] training loss: 0.00005927
INFO:root:[40,   450] training loss: 0.00010552
INFO:root:[40,   500] training loss: 0.00094509
INFO:root:[40,   550] training loss: 0.00084721
INFO:root:[40,   600] training loss: 0.00439808
INFO:root:[40,   650] training loss: 0.00025626
INFO:root:[40,   700] training loss: 0.00016876
INFO:root:[40,   750] training loss: 0.00030284
INFO:root:[40,   800] training loss: 0.00016815
INFO:root:[40,   850] training loss: 0.00011535
INFO:root:[40,   900] training loss: 0.01795274
INFO:root:[40,   950] training loss: 0.00538135
INFO:root:[40,  1000] training loss: 0.00010143
INFO:root:[40,  1050] training loss: 0.00003864
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9905    0.0605    0.1140      1720
   Telophase     0.8317    0.4981    0.6230      1032
          G1     0.4000    0.7500    0.5217         8
    Prophase     0.1630    0.9178    0.2769        73
   Metaphase     0.3213    0.8443    0.4655      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.4052      3872
   macro avg     0.6247    0.7244    0.5430      3872
weighted avg     0.7525    0.4052    0.3485      3872

INFO:root:Accuracy of the network on the 3872 validation images: 40 %
INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.01701427
INFO:root:[41,   100] training loss: 0.01199365
INFO:root:[41,   150] training loss: 0.01271633
INFO:root:[41,   200] training loss: 0.01121661
INFO:root:[41,   250] training loss: 0.01239092
INFO:root:[41,   300] training loss: 0.01523617
INFO:root:[41,   350] training loss: 0.01130847
INFO:root:[41,   400] training loss: 0.00007760
INFO:root:[41,   450] training loss: 0.00006301
INFO:root:[41,   500] training loss: 0.00095960
INFO:root:[41,   550] training loss: 0.00077101
INFO:root:[41,   600] training loss: 0.00488123
INFO:root:[41,   650] training loss: 0.00024299
INFO:root:[41,   700] training loss: 0.00016026
INFO:root:[41,   750] training loss: 0.00034119
INFO:root:[41,   800] training loss: 0.00016880
INFO:root:[41,   850] training loss: 0.00011964
INFO:root:[41,   900] training loss: 0.01882591
INFO:root:[41,   950] training loss: 0.00449162
INFO:root:[41,  1000] training loss: 0.00013006
INFO:root:[41,  1050] training loss: 0.00004359
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9889    0.0517    0.0983      1720
   Telophase     0.6637    0.5795    0.6187      1032
          G1     0.2121    0.8750    0.3415         8
    Prophase     0.1294    0.8630    0.2250        73
   Metaphase     0.2777    0.6325    0.3860      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.3657      3872
   macro avg     0.5626    0.7145    0.4956      3872
weighted avg     0.6943    0.3657    0.3178      3872

INFO:root:Accuracy of the network on the 3872 validation images: 36 %
INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.01768023
INFO:root:[42,   100] training loss: 0.01661762
INFO:root:[42,   150] training loss: 0.01358961
INFO:root:[42,   200] training loss: 0.01271243
INFO:root:[42,   250] training loss: 0.01250150
INFO:root:[42,   300] training loss: 0.01303001
INFO:root:[42,   350] training loss: 0.01086478
INFO:root:[42,   400] training loss: 0.00008116
INFO:root:[42,   450] training loss: 0.00006181
INFO:root:[42,   500] training loss: 0.00085714
INFO:root:[42,   550] training loss: 0.00060151
INFO:root:[42,   600] training loss: 0.00356276
INFO:root:[42,   650] training loss: 0.00022933
INFO:root:[42,   700] training loss: 0.00015949
INFO:root:[42,   750] training loss: 0.00033313
INFO:root:[42,   800] training loss: 0.00018039
INFO:root:[42,   850] training loss: 0.00010562
INFO:root:[42,   900] training loss: 0.01582416
INFO:root:[42,   950] training loss: 0.00363178
INFO:root:[42,  1000] training loss: 0.00018823
INFO:root:[42,  1050] training loss: 0.00004832
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9881    0.2419    0.3886      1720
   Telophase     0.6531    0.6366    0.6447      1032
          G1     0.4615    0.7500    0.5714         8
    Prophase     0.1486    0.9041    0.2553        73
   Metaphase     0.3179    0.6093    0.4178      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.4597      3872
   macro avg     0.6051    0.7346    0.5826      3872
weighted avg     0.7028    0.4597    0.4632      3872

INFO:root:Accuracy of the network on the 3872 validation images: 45 %
INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.01363631
INFO:root:[43,   100] training loss: 0.01260121
INFO:root:[43,   150] training loss: 0.01155648
INFO:root:[43,   200] training loss: 0.01058049
INFO:root:[43,   250] training loss: 0.01104838
INFO:root:[43,   300] training loss: 0.01327019
INFO:root:[43,   350] training loss: 0.01165172
INFO:root:[43,   400] training loss: 0.00004885
INFO:root:[43,   450] training loss: 0.00008764
INFO:root:[43,   500] training loss: 0.00070728
INFO:root:[43,   550] training loss: 0.00064740
INFO:root:[43,   600] training loss: 0.00407112
INFO:root:[43,   650] training loss: 0.00023664
INFO:root:[43,   700] training loss: 0.00014992
INFO:root:[43,   750] training loss: 0.00029485
INFO:root:[43,   800] training loss: 0.00014428
INFO:root:[43,   850] training loss: 0.00010494
INFO:root:[43,   900] training loss: 0.01672181
INFO:root:[43,   950] training loss: 0.00375158
INFO:root:[43,  1000] training loss: 0.00015599
INFO:root:[43,  1050] training loss: 0.00004351
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.9843    0.2186    0.3578      1720
   Telophase     0.7023    0.5853    0.6385      1032
          G1     0.1667    0.8750    0.2800         8
    Prophase     0.1345    0.7534    0.2282        73
   Metaphase     0.3361    0.7060    0.4554      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.4589      3872
   macro avg     0.5463    0.7340    0.5181      3872
weighted avg     0.7181    0.4589    0.4567      3872

INFO:root:Accuracy of the network on the 3872 validation images: 45 %
INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.01439724
INFO:root:[44,   100] training loss: 0.01137842
INFO:root:[44,   150] training loss: 0.01188183
INFO:root:[44,   200] training loss: 0.01060857
INFO:root:[44,   250] training loss: 0.01025719
INFO:root:[44,   300] training loss: 0.01236742
INFO:root:[44,   350] training loss: 0.01014225
INFO:root:[44,   400] training loss: 0.00002848
INFO:root:[44,   450] training loss: 0.00002940
INFO:root:[44,   500] training loss: 0.00045888
INFO:root:[44,   550] training loss: 0.00061236
INFO:root:[44,   600] training loss: 0.00260191
INFO:root:[44,   650] training loss: 0.00020738
INFO:root:[44,   700] training loss: 0.00012773
INFO:root:[44,   750] training loss: 0.00038507
INFO:root:[44,   800] training loss: 0.00014747
INFO:root:[44,   850] training loss: 0.00009244
INFO:root:[44,   900] training loss: 0.01515131
INFO:root:[44,   950] training loss: 0.00344718
INFO:root:[44,  1000] training loss: 0.00022680
INFO:root:[44,  1050] training loss: 0.00005609
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9711    0.3512    0.5158      1720
   Telophase     0.5599    0.5979    0.5783      1032
          G1     0.3750    0.7500    0.5000         8
    Prophase     0.1213    0.8493    0.2123        73
   Metaphase     0.3046    0.4758    0.3715      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.4613      3872
   macro avg     0.5712    0.7177    0.5683      3872
weighted avg     0.6661    0.4613    0.4887      3872

INFO:root:Accuracy of the network on the 3872 validation images: 46 %
INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.01244814
INFO:root:[45,   100] training loss: 0.01204768
INFO:root:[45,   150] training loss: 0.01111276
INFO:root:[45,   200] training loss: 0.01127742
INFO:root:[45,   250] training loss: 0.01154502
INFO:root:[45,   300] training loss: 0.01335251
INFO:root:[45,   350] training loss: 0.01012843
INFO:root:[45,   400] training loss: 0.00005187
INFO:root:[45,   450] training loss: 0.00004158
INFO:root:[45,   500] training loss: 0.00053191
INFO:root:[45,   550] training loss: 0.00049633
INFO:root:[45,   600] training loss: 0.00239011
INFO:root:[45,   650] training loss: 0.00018768
INFO:root:[45,   700] training loss: 0.00011824
INFO:root:[45,   750] training loss: 0.00033516
INFO:root:[45,   800] training loss: 0.00016153
INFO:root:[45,   850] training loss: 0.00008802
INFO:root:[45,   900] training loss: 0.01373134
INFO:root:[45,   950] training loss: 0.00294577
INFO:root:[45,  1000] training loss: 0.00023209
INFO:root:[45,  1050] training loss: 0.00003964
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.9790    0.3797    0.5471      1720
   Telophase     0.6651    0.7045    0.6842      1032
          G1     0.4615    0.7500    0.5714         8
    Prophase     0.1680    0.8767    0.2819        73
   Metaphase     0.3624    0.5996    0.4517      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.5359      3872
   macro avg     0.5909    0.7586    0.6004      3872
weighted avg     0.7141    0.5359    0.5537      3872

INFO:root:Accuracy of the network on the 3872 validation images: 53 %
INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.01254654
INFO:root:[46,   100] training loss: 0.01091688
INFO:root:[46,   150] training loss: 0.01333107
INFO:root:[46,   200] training loss: 0.01476141
INFO:root:[46,   250] training loss: 0.01387848
INFO:root:[46,   300] training loss: 0.01327380
INFO:root:[46,   350] training loss: 0.01168411
INFO:root:[46,   400] training loss: 0.00003756
INFO:root:[46,   450] training loss: 0.00003235
INFO:root:[46,   500] training loss: 0.00051012
INFO:root:[46,   550] training loss: 0.00055508
INFO:root:[46,   600] training loss: 0.00211179
INFO:root:[46,   650] training loss: 0.00021537
INFO:root:[46,   700] training loss: 0.00013009
INFO:root:[46,   750] training loss: 0.00035219
INFO:root:[46,   800] training loss: 0.00013847
INFO:root:[46,   850] training loss: 0.00009971
INFO:root:[46,   900] training loss: 0.01380181
INFO:root:[46,   950] training loss: 0.00354465
INFO:root:[46,  1000] training loss: 0.00021286
INFO:root:[46,  1050] training loss: 0.00004378
INFO:root:              precision    recall  f1-score   support

           S     0.4000    1.0000    0.5714         2
          G2     0.9792    0.3552    0.5213      1720
   Telophase     0.7096    0.6938    0.7016      1032
          G1     0.2800    0.8750    0.4242         8
    Prophase     0.1657    0.7534    0.2716        73
   Metaphase     0.3698    0.6702    0.4766      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.5390      3872
   macro avg     0.5577    0.7640    0.5667      3872
weighted avg     0.7275    0.5390    0.5529      3872

INFO:root:Accuracy of the network on the 3872 validation images: 53 %
INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.01318065
INFO:root:[47,   100] training loss: 0.01391370
INFO:root:[47,   150] training loss: 0.01131906
INFO:root:[47,   200] training loss: 0.01088297
INFO:root:[47,   250] training loss: 0.01095717
INFO:root:[47,   300] training loss: 0.01218699
INFO:root:[47,   350] training loss: 0.01137952
INFO:root:[47,   400] training loss: 0.00003335
INFO:root:[47,   450] training loss: 0.00007035
INFO:root:[47,   500] training loss: 0.00049557
INFO:root:[47,   550] training loss: 0.00061929
INFO:root:[47,   600] training loss: 0.00200269
INFO:root:[47,   650] training loss: 0.00019281
INFO:root:[47,   700] training loss: 0.00011304
INFO:root:[47,   750] training loss: 0.00030449
INFO:root:[47,   800] training loss: 0.00012335
INFO:root:[47,   850] training loss: 0.00007795
INFO:root:[47,   900] training loss: 0.01472861
INFO:root:[47,   950] training loss: 0.00342762
INFO:root:[47,  1000] training loss: 0.00023108
INFO:root:[47,  1050] training loss: 0.00005179
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.9823    0.3552    0.5218      1720
   Telophase     0.6267    0.6638    0.6447      1032
          G1     0.1364    0.7500    0.2308         8
    Prophase     0.1250    0.6575    0.2101        73
   Metaphase     0.3368    0.5609    0.4209      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.4997      3872
   macro avg     0.5296    0.7125    0.5278      3872
weighted avg     0.6970    0.4997    0.5216      3872

INFO:root:Accuracy of the network on the 3872 validation images: 49 %
INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.01222181
INFO:root:[48,   100] training loss: 0.01189756
INFO:root:[48,   150] training loss: 0.01124645
INFO:root:[48,   200] training loss: 0.01032664
INFO:root:[48,   250] training loss: 0.01114762
INFO:root:[48,   300] training loss: 0.01180571
INFO:root:[48,   350] training loss: 0.01050953
INFO:root:[48,   400] training loss: 0.00002856
INFO:root:[48,   450] training loss: 0.00003476
INFO:root:[48,   500] training loss: 0.00032777
INFO:root:[48,   550] training loss: 0.00053694
INFO:root:[48,   600] training loss: 0.00180937
INFO:root:[48,   650] training loss: 0.00015402
INFO:root:[48,   700] training loss: 0.00010539
INFO:root:[48,   750] training loss: 0.00034333
INFO:root:[48,   800] training loss: 0.00014324
INFO:root:[48,   850] training loss: 0.00008083
INFO:root:[48,   900] training loss: 0.01278004
INFO:root:[48,   950] training loss: 0.00489536
INFO:root:[48,  1000] training loss: 0.00025804
INFO:root:[48,  1050] training loss: 0.00005435
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.9600    0.5023    0.6595      1720
   Telophase     0.6759    0.7112    0.6931      1032
          G1     0.1842    0.8750    0.3043         8
    Prophase     0.1634    0.6849    0.2639        73
   Metaphase     0.4072    0.6044    0.4866      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.5901      3872
   macro avg     0.5558    0.7683    0.5820      3872
weighted avg     0.7198    0.5901    0.6144      3872

INFO:root:Accuracy of the network on the 3872 validation images: 59 %
INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.01314573
INFO:root:[49,   100] training loss: 0.01115721
INFO:root:[49,   150] training loss: 0.01124200
INFO:root:[49,   200] training loss: 0.01193907
INFO:root:[49,   250] training loss: 0.01510538
INFO:root:[49,   300] training loss: 0.01258568
INFO:root:[49,   350] training loss: 0.01041655
INFO:root:[49,   400] training loss: 0.00002323
INFO:root:[49,   450] training loss: 0.00003251
INFO:root:[49,   500] training loss: 0.00037377
INFO:root:[49,   550] training loss: 0.00044557
INFO:root:[49,   600] training loss: 0.00215257
INFO:root:[49,   650] training loss: 0.00016128
INFO:root:[49,   700] training loss: 0.00011233
INFO:root:[49,   750] training loss: 0.00031793
INFO:root:[49,   800] training loss: 0.00015143
INFO:root:[49,   850] training loss: 0.00008400
INFO:root:[49,   900] training loss: 0.01375685
INFO:root:[49,   950] training loss: 0.00253461
INFO:root:[49,  1000] training loss: 0.00027883
INFO:root:[49,  1050] training loss: 0.00005013
INFO:root:              precision    recall  f1-score   support

           S     0.5000    0.5000    0.5000         2
          G2     0.9650    0.5297    0.6839      1720
   Telophase     0.6608    0.7607    0.7072      1032
          G1     0.4000    0.7500    0.5217         8
    Prophase     0.1883    0.8356    0.3073        73
   Metaphase     0.4062    0.5484    0.4667      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.6028      3872
   macro avg     0.5886    0.7035    0.5981      3872
weighted avg     0.7187    0.6028    0.6248      3872

INFO:root:Accuracy of the network on the 3872 validation images: 60 %
INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.01154200
INFO:root:[50,   100] training loss: 0.01078121
INFO:root:[50,   150] training loss: 0.01113157
INFO:root:[50,   200] training loss: 0.01021243
INFO:root:[50,   250] training loss: 0.01035542
INFO:root:[50,   300] training loss: 0.01119182
INFO:root:[50,   350] training loss: 0.00963514
INFO:root:[50,   400] training loss: 0.00002841
INFO:root:[50,   450] training loss: 0.00006401
INFO:root:[50,   500] training loss: 0.00033648
INFO:root:[50,   550] training loss: 0.00054561
INFO:root:[50,   600] training loss: 0.00154629
INFO:root:[50,   650] training loss: 0.00017030
INFO:root:[50,   700] training loss: 0.00011537
INFO:root:[50,   750] training loss: 0.00039346
INFO:root:[50,   800] training loss: 0.00011890
INFO:root:[50,   850] training loss: 0.00008419
INFO:root:[50,   900] training loss: 0.01222431
INFO:root:[50,   950] training loss: 0.00311505
INFO:root:[50,  1000] training loss: 0.00021319
INFO:root:[50,  1050] training loss: 0.00004194
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9589    0.5297    0.6824      1720
   Telophase     0.6218    0.7616    0.6847      1032
          G1     0.2800    0.8750    0.4242         8
    Prophase     0.1812    0.7945    0.2952        73
   Metaphase     0.3780    0.4778    0.4220      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.5839      3872
   macro avg     0.5838    0.7769    0.6155      3872
weighted avg     0.6978    0.5839    0.6059      3872

INFO:root:Accuracy of the network on the 3872 validation images: 58 %
INFO:root:epoch50
INFO:root:[51,    50] training loss: 0.01096825
INFO:root:[51,   100] training loss: 0.01089372
INFO:root:[51,   150] training loss: 0.01081565
INFO:root:[51,   200] training loss: 0.01382794
INFO:root:[51,   250] training loss: 0.01217869
INFO:root:[51,   300] training loss: 0.01239744
INFO:root:[51,   350] training loss: 0.01035881
INFO:root:[51,   400] training loss: 0.00001469
INFO:root:[51,   450] training loss: 0.00003288
INFO:root:[51,   500] training loss: 0.00021551
INFO:root:[51,   550] training loss: 0.00049824
INFO:root:[51,   600] training loss: 0.00149738
INFO:root:[51,   650] training loss: 0.00016253
INFO:root:[51,   700] training loss: 0.00009977
INFO:root:[51,   750] training loss: 0.00034930
INFO:root:[51,   800] training loss: 0.00012777
INFO:root:[51,   850] training loss: 0.00007703
INFO:root:[51,   900] training loss: 0.01081900
INFO:root:[51,   950] training loss: 0.00273170
INFO:root:[51,  1000] training loss: 0.00022881
INFO:root:[51,  1050] training loss: 0.00003928
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9536    0.5855    0.7255      1720
   Telophase     0.6617    0.7674    0.7106      1032
          G1     0.5000    0.6250    0.5556         8
    Prophase     0.2012    0.8904    0.3283        73
   Metaphase     0.4383    0.5426    0.4849      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.6289      3872
   macro avg     0.6316    0.7730    0.6578      3872
weighted avg     0.7229    0.6289    0.6497      3872

INFO:root:Accuracy of the network on the 3872 validation images: 62 %
INFO:root:epoch51
INFO:root:[52,    50] training loss: 0.01151199
INFO:root:[52,   100] training loss: 0.01065871
INFO:root:[52,   150] training loss: 0.01045030
INFO:root:[52,   200] training loss: 0.01137469
INFO:root:[52,   250] training loss: 0.01074863
INFO:root:[52,   300] training loss: 0.01142454
INFO:root:[52,   350] training loss: 0.00989996
INFO:root:[52,   400] training loss: 0.00005983
INFO:root:[52,   450] training loss: 0.00001685
INFO:root:[52,   500] training loss: 0.00026431
INFO:root:[52,   550] training loss: 0.00034678
INFO:root:[52,   600] training loss: 0.00125589
INFO:root:[52,   650] training loss: 0.00014749
INFO:root:[52,   700] training loss: 0.00010005
INFO:root:[52,   750] training loss: 0.00039827
INFO:root:[52,   800] training loss: 0.00011582
INFO:root:[52,   850] training loss: 0.00006902
INFO:root:[52,   900] training loss: 0.01141451
INFO:root:[52,   950] training loss: 0.00229765
INFO:root:[52,  1000] training loss: 0.00025741
INFO:root:[52,  1050] training loss: 0.00003943
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9273    0.7047    0.8008      1720
   Telophase     0.6591    0.7849    0.7165      1032
          G1     0.5556    0.6250    0.5882         8
    Prophase     0.2520    0.8630    0.3901        73
   Metaphase     0.5061    0.5242    0.5150      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.6810      3872
   macro avg     0.6524    0.7860    0.6872      3872
weighted avg     0.7298    0.6810    0.6940      3872

INFO:root:Accuracy of the network on the 3872 validation images: 68 %
INFO:root:epoch52
INFO:root:[53,    50] training loss: 0.01084640
INFO:root:[53,   100] training loss: 0.01207875
INFO:root:[53,   150] training loss: 0.01064625
INFO:root:[53,   200] training loss: 0.00992111
INFO:root:[53,   250] training loss: 0.00964277
INFO:root:[53,   300] training loss: 0.01060510
INFO:root:[53,   350] training loss: 0.00891820
INFO:root:[53,   400] training loss: 0.00001417
INFO:root:[53,   450] training loss: 0.00005500
INFO:root:[53,   500] training loss: 0.00021089
INFO:root:[53,   550] training loss: 0.00045721
INFO:root:[53,   600] training loss: 0.00132988
INFO:root:[53,   650] training loss: 0.00013470
INFO:root:[53,   700] training loss: 0.00008911
INFO:root:[53,   750] training loss: 0.00034879
INFO:root:[53,   800] training loss: 0.00011860
INFO:root:[53,   850] training loss: 0.00007055
INFO:root:[53,   900] training loss: 0.01074492
INFO:root:[53,   950] training loss: 0.00250074
INFO:root:[53,  1000] training loss: 0.00022102
INFO:root:[53,  1050] training loss: 0.00004501
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9420    0.6134    0.7430      1720
   Telophase     0.6420    0.7665    0.6988      1032
          G1     0.2917    0.8750    0.4375         8
    Prophase     0.2204    0.7397    0.3396        73
   Metaphase     0.4418    0.5319    0.4827      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.6358      3872
   macro avg     0.6006    0.7895    0.6431      3872
weighted avg     0.7134    0.6358    0.6537      3872

INFO:root:Accuracy of the network on the 3872 validation images: 63 %
INFO:root:epoch53
INFO:root:[54,    50] training loss: 0.01071668
INFO:root:[54,   100] training loss: 0.01168664
INFO:root:[54,   150] training loss: 0.01123546
INFO:root:[54,   200] training loss: 0.00994512
INFO:root:[54,   250] training loss: 0.00971431
INFO:root:[54,   300] training loss: 0.01166911
INFO:root:[54,   350] training loss: 0.00962504
INFO:root:[54,   400] training loss: 0.00001390
INFO:root:[54,   450] training loss: 0.00002177
INFO:root:[54,   500] training loss: 0.00027563
INFO:root:[54,   550] training loss: 0.00036107
INFO:root:[54,   600] training loss: 0.00119887
INFO:root:[54,   650] training loss: 0.00013257
INFO:root:[54,   700] training loss: 0.00008634
INFO:root:[54,   750] training loss: 0.00033929
INFO:root:[54,   800] training loss: 0.00012651
INFO:root:[54,   850] training loss: 0.00006843
INFO:root:[54,   900] training loss: 0.00971643
INFO:root:[54,   950] training loss: 0.00215518
INFO:root:[54,  1000] training loss: 0.00023029
INFO:root:[54,  1050] training loss: 0.00004308
INFO:root:              precision    recall  f1-score   support

           S     0.5000    0.5000    0.5000         2
          G2     0.9141    0.7238    0.8079      1720
   Telophase     0.6677    0.8178    0.7352      1032
          G1     0.3889    0.8750    0.5385         8
    Prophase     0.2994    0.7260    0.4240        73
   Metaphase     0.4990    0.5048    0.5019      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.6909      3872
   macro avg     0.6099    0.7354    0.6439      3872
weighted avg     0.7248    0.6909    0.6990      3872

INFO:root:Accuracy of the network on the 3872 validation images: 69 %
INFO:root:epoch54
INFO:root:[55,    50] training loss: 0.01028804
INFO:root:[55,   100] training loss: 0.01017030
INFO:root:[55,   150] training loss: 0.01029524
INFO:root:[55,   200] training loss: 0.00893587
INFO:root:[55,   250] training loss: 0.00962990
INFO:root:[55,   300] training loss: 0.01106623
INFO:root:[55,   350] training loss: 0.00877367
INFO:root:[55,   400] training loss: 0.00001077
INFO:root:[55,   450] training loss: 0.00001561
INFO:root:[55,   500] training loss: 0.00015286
INFO:root:[55,   550] training loss: 0.00038387
INFO:root:[55,   600] training loss: 0.00108265
INFO:root:[55,   650] training loss: 0.00010193
INFO:root:[55,   700] training loss: 0.00006810
INFO:root:[55,   750] training loss: 0.00034218
INFO:root:[55,   800] training loss: 0.00010119
INFO:root:[55,   850] training loss: 0.00005716
INFO:root:[55,   900] training loss: 0.00908579
INFO:root:[55,   950] training loss: 0.00230915
INFO:root:[55,  1000] training loss: 0.00023752
INFO:root:[55,  1050] training loss: 0.00003565
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8924    0.7715    0.8276      1720
   Telophase     0.6983    0.7490    0.7228      1032
          G1     0.1818    0.7500    0.2927         8
    Prophase     0.1929    0.6712    0.2997        73
   Metaphase     0.5498    0.5232    0.5362      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.6976      3872
   macro avg     0.5736    0.7807    0.6208      3872
weighted avg     0.7344    0.6976    0.7108      3872

INFO:root:Accuracy of the network on the 3872 validation images: 69 %
INFO:root:epoch55
INFO:root:[56,    50] training loss: 0.01114638
INFO:root:[56,   100] training loss: 0.01211536
INFO:root:[56,   150] training loss: 0.01054315
INFO:root:[56,   200] training loss: 0.01030383
INFO:root:[56,   250] training loss: 0.01124235
INFO:root:[56,   300] training loss: 0.01113869
INFO:root:[56,   350] training loss: 0.00962846
INFO:root:[56,   400] training loss: 0.00001044
INFO:root:[56,   450] training loss: 0.00002132
INFO:root:[56,   500] training loss: 0.00017206
INFO:root:[56,   550] training loss: 0.00031541
INFO:root:[56,   600] training loss: 0.00104777
INFO:root:[56,   650] training loss: 0.00010372
INFO:root:[56,   700] training loss: 0.00007556
INFO:root:[56,   750] training loss: 0.00031177
INFO:root:[56,   800] training loss: 0.00009888
INFO:root:[56,   850] training loss: 0.00006191
INFO:root:[56,   900] training loss: 0.01022556
INFO:root:[56,   950] training loss: 0.00193530
INFO:root:[56,  1000] training loss: 0.00025012
INFO:root:[56,  1050] training loss: 0.00003813
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9076    0.7593    0.8268      1720
   Telophase     0.7166    0.7839    0.7487      1032
          G1     0.6667    0.7500    0.7059         8
    Prophase     0.2564    0.8219    0.3909        73
   Metaphase     0.5555    0.5667    0.5610      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7159      3872
   macro avg     0.6813    0.8117    0.7191      3872
weighted avg     0.7498    0.7159    0.7267      3872

INFO:root:Accuracy of the network on the 3872 validation images: 71 %
INFO:root:epoch56
INFO:root:[57,    50] training loss: 0.01051302
INFO:root:[57,   100] training loss: 0.01125339
INFO:root:[57,   150] training loss: 0.01094990
INFO:root:[57,   200] training loss: 0.00926012
INFO:root:[57,   250] training loss: 0.01030608
INFO:root:[57,   300] training loss: 0.01032590
INFO:root:[57,   350] training loss: 0.00840975
INFO:root:[57,   400] training loss: 0.00001251
INFO:root:[57,   450] training loss: 0.00001324
INFO:root:[57,   500] training loss: 0.00021014
INFO:root:[57,   550] training loss: 0.00041572
INFO:root:[57,   600] training loss: 0.00095870
INFO:root:[57,   650] training loss: 0.00012426
INFO:root:[57,   700] training loss: 0.00007781
INFO:root:[57,   750] training loss: 0.00041652
INFO:root:[57,   800] training loss: 0.00011034
INFO:root:[57,   850] training loss: 0.00005538
INFO:root:[57,   900] training loss: 0.00951630
INFO:root:[57,   950] training loss: 0.00221291
INFO:root:[57,  1000] training loss: 0.00023912
INFO:root:[57,  1050] training loss: 0.00003190
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9207    0.7494    0.8263      1720
   Telophase     0.6907    0.7984    0.7407      1032
          G1     0.4211    1.0000    0.5926         8
    Prophase     0.2415    0.7808    0.3689        73
   Metaphase     0.5472    0.5387    0.5429      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7076      3872
   macro avg     0.6411    0.8382    0.6959      3872
weighted avg     0.7457    0.7076    0.7188      3872

INFO:root:Accuracy of the network on the 3872 validation images: 70 %
INFO:root:epoch57
INFO:root:[58,    50] training loss: 0.00906634
INFO:root:[58,   100] training loss: 0.00987089
INFO:root:[58,   150] training loss: 0.01015048
INFO:root:[58,   200] training loss: 0.01256139
INFO:root:[58,   250] training loss: 0.01278737
INFO:root:[58,   300] training loss: 0.01030055
INFO:root:[58,   350] training loss: 0.00920347
INFO:root:[58,   400] training loss: 0.00001341
INFO:root:[58,   450] training loss: 0.00002783
INFO:root:[58,   500] training loss: 0.00014829
INFO:root:[58,   550] training loss: 0.00034672
INFO:root:[58,   600] training loss: 0.00073502
INFO:root:[58,   650] training loss: 0.00012371
INFO:root:[58,   700] training loss: 0.00007982
INFO:root:[58,   750] training loss: 0.00032853
INFO:root:[58,   800] training loss: 0.00009262
INFO:root:[58,   850] training loss: 0.00006137
INFO:root:[58,   900] training loss: 0.00886852
INFO:root:[58,   950] training loss: 0.00191489
INFO:root:[58,  1000] training loss: 0.00022203
INFO:root:[58,  1050] training loss: 0.00003271
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.8891    0.7831    0.8328      1720
   Telophase     0.7257    0.7820    0.7528      1032
          G1     0.4286    0.7500    0.5455         8
    Prophase     0.2731    0.8082    0.4083        73
   Metaphase     0.5590    0.5455    0.5521      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7200      3872
   macro avg     0.6489    0.8098    0.6988      3872
weighted avg     0.7448    0.7200    0.7280      3872

INFO:root:Accuracy of the network on the 3872 validation images: 72 %
INFO:root:epoch58
INFO:root:[59,    50] training loss: 0.00930036
INFO:root:[59,   100] training loss: 0.00984115
INFO:root:[59,   150] training loss: 0.01011537
INFO:root:[59,   200] training loss: 0.00882413
INFO:root:[59,   250] training loss: 0.01062079
INFO:root:[59,   300] training loss: 0.01289206
INFO:root:[59,   350] training loss: 0.00872061
INFO:root:[59,   400] training loss: 0.00001530
INFO:root:[59,   450] training loss: 0.00001668
INFO:root:[59,   500] training loss: 0.00013501
INFO:root:[59,   550] training loss: 0.00029796
INFO:root:[59,   600] training loss: 0.00074726
INFO:root:[59,   650] training loss: 0.00010433
INFO:root:[59,   700] training loss: 0.00006831
INFO:root:[59,   750] training loss: 0.00038274
INFO:root:[59,   800] training loss: 0.00008657
INFO:root:[59,   850] training loss: 0.00005304
INFO:root:[59,   900] training loss: 0.00829068
INFO:root:[59,   950] training loss: 0.00176089
INFO:root:[59,  1000] training loss: 0.00022592
INFO:root:[59,  1050] training loss: 0.00003453
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.8845    0.7971    0.8385      1720
   Telophase     0.6862    0.8178    0.7462      1032
          G1     0.5714    1.0000    0.7273         8
    Prophase     0.3194    0.8356    0.4621        73
   Metaphase     0.5653    0.4816    0.5201      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7198      3872
   macro avg     0.6705    0.8475    0.7278      3872
weighted avg     0.7351    0.7198    0.7217      3872

INFO:root:Accuracy of the network on the 3872 validation images: 71 %
INFO:root:epoch59
INFO:root:[60,    50] training loss: 0.00928670
INFO:root:[60,   100] training loss: 0.01223903
INFO:root:[60,   150] training loss: 0.01332287
INFO:root:[60,   200] training loss: 0.01168132
INFO:root:[60,   250] training loss: 0.01031791
INFO:root:[60,   300] training loss: 0.01166278
INFO:root:[60,   350] training loss: 0.01066142
INFO:root:[60,   400] training loss: 0.00009903
INFO:root:[60,   450] training loss: 0.00006928
INFO:root:[60,   500] training loss: 0.00015831
INFO:root:[60,   550] training loss: 0.00032010
INFO:root:[60,   600] training loss: 0.00099159
INFO:root:[60,   650] training loss: 0.00009039
INFO:root:[60,   700] training loss: 0.00006845
INFO:root:[60,   750] training loss: 0.00035730
INFO:root:[60,   800] training loss: 0.00008241
INFO:root:[60,   850] training loss: 0.00004621
INFO:root:[60,   900] training loss: 0.01111748
INFO:root:[60,   950] training loss: 0.00250124
INFO:root:[60,  1000] training loss: 0.00024167
INFO:root:[60,  1050] training loss: 0.00003115
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.8944    0.7924    0.8403      1720
   Telophase     0.7650    0.7791    0.7720      1032
          G1     0.5714    0.5000    0.5333         8
    Prophase     0.2941    0.8219    0.4332        73
   Metaphase     0.5907    0.6170    0.6036      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7423      3872
   macro avg     0.6832    0.7872    0.7118      3872
weighted avg     0.7668    0.7423    0.7507      3872

INFO:root:Accuracy of the network on the 3872 validation images: 74 %
INFO:root:epoch60
INFO:root:[61,    50] training loss: 0.01014575
INFO:root:[61,   100] training loss: 0.00976307
INFO:root:[61,   150] training loss: 0.00974142
INFO:root:[61,   200] training loss: 0.01033293
INFO:root:[61,   250] training loss: 0.00907778
INFO:root:[61,   300] training loss: 0.01028128
INFO:root:[61,   350] training loss: 0.00827002
INFO:root:[61,   400] training loss: 0.00001084
INFO:root:[61,   450] training loss: 0.00001799
INFO:root:[61,   500] training loss: 0.00009370
INFO:root:[61,   550] training loss: 0.00026882
INFO:root:[61,   600] training loss: 0.00078697
INFO:root:[61,   650] training loss: 0.00011611
INFO:root:[61,   700] training loss: 0.00007666
INFO:root:[61,   750] training loss: 0.00034634
INFO:root:[61,   800] training loss: 0.00008180
INFO:root:[61,   850] training loss: 0.00004674
INFO:root:[61,   900] training loss: 0.00806337
INFO:root:[61,   950] training loss: 0.00286259
INFO:root:[61,  1000] training loss: 0.00017516
INFO:root:[61,  1050] training loss: 0.00012675
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.9299    0.5942    0.7251      1720
   Telophase     0.6591    0.7907    0.7189      1032
          G1     0.5385    0.8750    0.6667         8
    Prophase     0.3005    0.8767    0.4476        73
   Metaphase     0.4428    0.5580    0.4938      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.6433      3872
   macro avg     0.6482    0.8135    0.6931      3872
weighted avg     0.7149    0.6433    0.6566      3872

INFO:root:Accuracy of the network on the 3872 validation images: 64 %
INFO:root:epoch61
INFO:root:[62,    50] training loss: 0.01050393
INFO:root:[62,   100] training loss: 0.00951745
INFO:root:[62,   150] training loss: 0.00956105
INFO:root:[62,   200] training loss: 0.00840053
INFO:root:[62,   250] training loss: 0.01428418
INFO:root:[62,   300] training loss: 0.01428493
INFO:root:[62,   350] training loss: 0.00970017
INFO:root:[62,   400] training loss: 0.00001859
INFO:root:[62,   450] training loss: 0.00002684
INFO:root:[62,   500] training loss: 0.00020672
INFO:root:[62,   550] training loss: 0.00025722
INFO:root:[62,   600] training loss: 0.00081072
INFO:root:[62,   650] training loss: 0.00009978
INFO:root:[62,   700] training loss: 0.00006661
INFO:root:[62,   750] training loss: 0.00034600
INFO:root:[62,   800] training loss: 0.00007598
INFO:root:[62,   850] training loss: 0.00004829
INFO:root:[62,   900] training loss: 0.00871923
INFO:root:[62,   950] training loss: 0.00225363
INFO:root:[62,  1000] training loss: 0.00020019
INFO:root:[62,  1050] training loss: 0.00003162
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8576    0.8122    0.8343      1720
   Telophase     0.7195    0.7083    0.7139      1032
          G1     0.7143    0.6250    0.6667         8
    Prophase     0.2095    0.8493    0.3360        73
   Metaphase     0.5583    0.4952    0.5249      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7004      3872
   macro avg     0.6513    0.7843    0.6775      3872
weighted avg     0.7283    0.7004    0.7099      3872

INFO:root:Accuracy of the network on the 3872 validation images: 70 %
INFO:root:epoch62
INFO:root:[63,    50] training loss: 0.00976416
INFO:root:[63,   100] training loss: 0.01013115
INFO:root:[63,   150] training loss: 0.00987563
INFO:root:[63,   200] training loss: 0.00844486
INFO:root:[63,   250] training loss: 0.00868326
INFO:root:[63,   300] training loss: 0.01000749
INFO:root:[63,   350] training loss: 0.00940137
INFO:root:[63,   400] training loss: 0.00001351
INFO:root:[63,   450] training loss: 0.00002309
INFO:root:[63,   500] training loss: 0.00011560
INFO:root:[63,   550] training loss: 0.00022553
INFO:root:[63,   600] training loss: 0.00072325
INFO:root:[63,   650] training loss: 0.00010218
INFO:root:[63,   700] training loss: 0.00005680
INFO:root:[63,   750] training loss: 0.00034007
INFO:root:[63,   800] training loss: 0.00007572
INFO:root:[63,   850] training loss: 0.00004678
INFO:root:[63,   900] training loss: 0.00712108
INFO:root:[63,   950] training loss: 0.00318738
INFO:root:[63,  1000] training loss: 0.00021190
INFO:root:[63,  1050] training loss: 0.00003930
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8940    0.7401    0.8098      1720
   Telophase     0.6613    0.8362    0.7386      1032
          G1     0.4118    0.8750    0.5600         8
    Prophase     0.3602    0.7945    0.4957        73
   Metaphase     0.5084    0.4710    0.4890      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.6955      3872
   macro avg     0.6194    0.8167    0.6800      3872
weighted avg     0.7178    0.6955    0.6988      3872

INFO:root:Accuracy of the network on the 3872 validation images: 69 %
INFO:root:epoch63
INFO:root:[64,    50] training loss: 0.00964652
INFO:root:[64,   100] training loss: 0.01106695
INFO:root:[64,   150] training loss: 0.00976181
INFO:root:[64,   200] training loss: 0.00848347
INFO:root:[64,   250] training loss: 0.01008455
INFO:root:[64,   300] training loss: 0.00999717
INFO:root:[64,   350] training loss: 0.00886925
INFO:root:[64,   400] training loss: 0.00000790
INFO:root:[64,   450] training loss: 0.00001230
INFO:root:[64,   500] training loss: 0.00011051
INFO:root:[64,   550] training loss: 0.00034490
INFO:root:[64,   600] training loss: 0.00067948
INFO:root:[64,   650] training loss: 0.00008162
INFO:root:[64,   700] training loss: 0.00006160
INFO:root:[64,   750] training loss: 0.00041037
INFO:root:[64,   800] training loss: 0.00007420
INFO:root:[64,   850] training loss: 0.00004305
INFO:root:[64,   900] training loss: 0.00842942
INFO:root:[64,   950] training loss: 0.00202089
INFO:root:[64,  1000] training loss: 0.00024055
INFO:root:[64,  1050] training loss: 0.00003987
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.8680    0.8523    0.8601      1720
   Telophase     0.7292    0.7645    0.7465      1032
          G1     0.4706    1.0000    0.6400         8
    Prophase     0.2844    0.8219    0.4225        73
   Metaphase     0.6275    0.5261    0.5723      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7417      3872
   macro avg     0.6637    0.8521    0.7202      3872
weighted avg     0.7549    0.7417    0.7443      3872

INFO:root:Accuracy of the network on the 3872 validation images: 74 %
INFO:root:epoch64
INFO:root:[65,    50] training loss: 0.00962269
INFO:root:[65,   100] training loss: 0.00943185
INFO:root:[65,   150] training loss: 0.00903467
INFO:root:[65,   200] training loss: 0.00911055
INFO:root:[65,   250] training loss: 0.00936563
INFO:root:[65,   300] training loss: 0.00989486
INFO:root:[65,   350] training loss: 0.00780229
INFO:root:[65,   400] training loss: 0.00000714
INFO:root:[65,   450] training loss: 0.00000850
INFO:root:[65,   500] training loss: 0.00010164
INFO:root:[65,   550] training loss: 0.00021496
INFO:root:[65,   600] training loss: 0.00061139
INFO:root:[65,   650] training loss: 0.00007785
INFO:root:[65,   700] training loss: 0.00005410
INFO:root:[65,   750] training loss: 0.00038388
INFO:root:[65,   800] training loss: 0.00007433
INFO:root:[65,   850] training loss: 0.00003614
INFO:root:[65,   900] training loss: 0.00659166
INFO:root:[65,   950] training loss: 0.00161747
INFO:root:[65,  1000] training loss: 0.00033250
INFO:root:[65,  1050] training loss: 0.00004531
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8634    0.8378    0.8504      1720
   Telophase     0.7166    0.8256    0.7672      1032
          G1     0.6000    0.7500    0.6667         8
    Prophase     0.3806    0.8082    0.5175        73
   Metaphase     0.5950    0.4845    0.5341      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7397      3872
   macro avg     0.6651    0.8152    0.7147      3872
weighted avg     0.7429    0.7397    0.7371      3872

INFO:root:Accuracy of the network on the 3872 validation images: 73 %
INFO:root:epoch65
INFO:root:[66,    50] training loss: 0.00934156
INFO:root:[66,   100] training loss: 0.00876811
INFO:root:[66,   150] training loss: 0.00875687
INFO:root:[66,   200] training loss: 0.00925957
INFO:root:[66,   250] training loss: 0.00944880
INFO:root:[66,   300] training loss: 0.01020525
INFO:root:[66,   350] training loss: 0.00789117
INFO:root:[66,   400] training loss: 0.00000591
INFO:root:[66,   450] training loss: 0.00001063
INFO:root:[66,   500] training loss: 0.00008825
INFO:root:[66,   550] training loss: 0.00024592
INFO:root:[66,   600] training loss: 0.00042670
INFO:root:[66,   650] training loss: 0.00008596
INFO:root:[66,   700] training loss: 0.00005232
INFO:root:[66,   750] training loss: 0.00042078
INFO:root:[66,   800] training loss: 0.00005522
INFO:root:[66,   850] training loss: 0.00003686
INFO:root:[66,   900] training loss: 0.00764641
INFO:root:[66,   950] training loss: 0.00208620
INFO:root:[66,  1000] training loss: 0.00017491
INFO:root:[66,  1050] training loss: 0.00003350
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8642    0.8767    0.8704      1720
   Telophase     0.7414    0.7917    0.7657      1032
          G1     0.2593    0.8750    0.4000         8
    Prophase     0.2616    0.6164    0.3673        73
   Metaphase     0.6593    0.5222    0.5828      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7546      3872
   macro avg     0.6123    0.8117    0.6647      3872
weighted avg     0.7641    0.7546    0.7552      3872

INFO:root:Accuracy of the network on the 3872 validation images: 75 %
INFO:root:epoch66
INFO:root:[67,    50] training loss: 0.00968274
INFO:root:[67,   100] training loss: 0.00922399
INFO:root:[67,   150] training loss: 0.00882412
INFO:root:[67,   200] training loss: 0.00809209
INFO:root:[67,   250] training loss: 0.00874406
INFO:root:[67,   300] training loss: 0.00920608
INFO:root:[67,   350] training loss: 0.00803677
INFO:root:[67,   400] training loss: 0.00001784
INFO:root:[67,   450] training loss: 0.00002309
INFO:root:[67,   500] training loss: 0.00009569
INFO:root:[67,   550] training loss: 0.00022324
INFO:root:[67,   600] training loss: 0.00046651
INFO:root:[67,   650] training loss: 0.00007564
INFO:root:[67,   700] training loss: 0.00005052
INFO:root:[67,   750] training loss: 0.00032861
INFO:root:[67,   800] training loss: 0.00006681
INFO:root:[67,   850] training loss: 0.00004029
INFO:root:[67,   900] training loss: 0.00667267
INFO:root:[67,   950] training loss: 0.00164295
INFO:root:[67,  1000] training loss: 0.00016935
INFO:root:[67,  1050] training loss: 0.00003385
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.8741    0.8721    0.8731      1720
   Telophase     0.7337    0.8304    0.7791      1032
          G1     0.6364    0.8750    0.7368         8
    Prophase     0.3394    0.7671    0.4706        73
   Metaphase     0.6663    0.5193    0.5837      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7650      3872
   macro avg     0.7024    0.8377    0.7490      3872
weighted avg     0.7706    0.7650    0.7630      3872

INFO:root:Accuracy of the network on the 3872 validation images: 76 %
INFO:root:epoch67
INFO:root:[68,    50] training loss: 0.00897314
INFO:root:[68,   100] training loss: 0.00863975
INFO:root:[68,   150] training loss: 0.00997262
INFO:root:[68,   200] training loss: 0.00848513
INFO:root:[68,   250] training loss: 0.00891877
INFO:root:[68,   300] training loss: 0.00965528
INFO:root:[68,   350] training loss: 0.00969226
INFO:root:[68,   400] training loss: 0.00001597
INFO:root:[68,   450] training loss: 0.00004430
INFO:root:[68,   500] training loss: 0.00005512
INFO:root:[68,   550] training loss: 0.00020712
INFO:root:[68,   600] training loss: 0.00047897
INFO:root:[68,   650] training loss: 0.00006764
INFO:root:[68,   700] training loss: 0.00006445
INFO:root:[68,   750] training loss: 0.00039507
INFO:root:[68,   800] training loss: 0.00005864
INFO:root:[68,   850] training loss: 0.00003333
INFO:root:[68,   900] training loss: 0.00627908
INFO:root:[68,   950] training loss: 0.00166604
INFO:root:[68,  1000] training loss: 0.00022576
INFO:root:[68,  1050] training loss: 0.00002754
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8556    0.9029    0.8786      1720
   Telophase     0.7532    0.7955    0.7738      1032
          G1     0.2917    0.8750    0.4375         8
    Prophase     0.2927    0.6575    0.4051        73
   Metaphase     0.6813    0.5087    0.5825      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7645      3872
   macro avg     0.6249    0.8200    0.6777      3872
weighted avg     0.7699    0.7645    0.7618      3872

INFO:root:Accuracy of the network on the 3872 validation images: 76 %
INFO:root:epoch68
INFO:root:[69,    50] training loss: 0.00920034
INFO:root:[69,   100] training loss: 0.00930038
INFO:root:[69,   150] training loss: 0.00915659
INFO:root:[69,   200] training loss: 0.00814943
INFO:root:[69,   250] training loss: 0.00797706
INFO:root:[69,   300] training loss: 0.00959034
INFO:root:[69,   350] training loss: 0.00785763
INFO:root:[69,   400] training loss: 0.00001307
INFO:root:[69,   450] training loss: 0.00001025
INFO:root:[69,   500] training loss: 0.00015953
INFO:root:[69,   550] training loss: 0.00027169
INFO:root:[69,   600] training loss: 0.00065638
INFO:root:[69,   650] training loss: 0.00005508
INFO:root:[69,   700] training loss: 0.00003917
INFO:root:[69,   750] training loss: 0.00033791
INFO:root:[69,   800] training loss: 0.00005017
INFO:root:[69,   850] training loss: 0.00002934
INFO:root:[69,   900] training loss: 0.00712218
INFO:root:[69,   950] training loss: 0.00156791
INFO:root:[69,  1000] training loss: 0.00019187
INFO:root:[69,  1050] training loss: 0.00002990
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.8680    0.8866    0.8772      1720
   Telophase     0.7096    0.8430    0.7706      1032
          G1     0.4000    1.0000    0.5714         8
    Prophase     0.2988    0.6712    0.4135        73
   Metaphase     0.6667    0.4507    0.5378      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7549      3872
   macro avg     0.6585    0.8359    0.7101      3872
weighted avg     0.7603    0.7549    0.7488      3872

INFO:root:Accuracy of the network on the 3872 validation images: 75 %
INFO:root:epoch69
INFO:root:[70,    50] training loss: 0.00864502
INFO:root:[70,   100] training loss: 0.00866501
INFO:root:[70,   150] training loss: 0.00858118
INFO:root:[70,   200] training loss: 0.00827524
INFO:root:[70,   250] training loss: 0.01041113
INFO:root:[70,   300] training loss: 0.00946926
INFO:root:[70,   350] training loss: 0.00998444
INFO:root:[70,   400] training loss: 0.00001825
INFO:root:[70,   450] training loss: 0.00001694
INFO:root:[70,   500] training loss: 0.00014722
INFO:root:[70,   550] training loss: 0.00024727
INFO:root:[70,   600] training loss: 0.00059341
INFO:root:[70,   650] training loss: 0.00005238
INFO:root:[70,   700] training loss: 0.00003944
INFO:root:[70,   750] training loss: 0.00029170
INFO:root:[70,   800] training loss: 0.00005255
INFO:root:[70,   850] training loss: 0.00003598
INFO:root:[70,   900] training loss: 0.00694671
INFO:root:[70,   950] training loss: 0.00196442
INFO:root:[70,  1000] training loss: 0.00016593
INFO:root:[70,  1050] training loss: 0.00002642
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8552    0.8581    0.8566      1720
   Telophase     0.7130    0.7897    0.7494      1032
          G1     0.3529    0.7500    0.4800         8
    Prophase     0.2971    0.7123    0.4194        73
   Metaphase     0.6231    0.4845    0.5452      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7373      3872
   macro avg     0.6202    0.7992    0.6739      3872
weighted avg     0.7437    0.7373    0.7359      3872

INFO:root:Accuracy of the network on the 3872 validation images: 73 %
INFO:root:epoch70
INFO:root:[71,    50] training loss: 0.01003759
INFO:root:[71,   100] training loss: 0.00927510
INFO:root:[71,   150] training loss: 0.00872796
INFO:root:[71,   200] training loss: 0.00883372
INFO:root:[71,   250] training loss: 0.00969173
INFO:root:[71,   300] training loss: 0.00904237
INFO:root:[71,   350] training loss: 0.00870860
INFO:root:[71,   400] training loss: 0.00000674
INFO:root:[71,   450] training loss: 0.00001104
INFO:root:[71,   500] training loss: 0.00005990
INFO:root:[71,   550] training loss: 0.00018905
INFO:root:[71,   600] training loss: 0.00040060
INFO:root:[71,   650] training loss: 0.00007372
INFO:root:[71,   700] training loss: 0.00004123
INFO:root:[71,   750] training loss: 0.00030101
INFO:root:[71,   800] training loss: 0.00006057
INFO:root:[71,   850] training loss: 0.00003569
INFO:root:[71,   900] training loss: 0.00508029
INFO:root:[71,   950] training loss: 0.00180475
INFO:root:[71,  1000] training loss: 0.00013917
INFO:root:[71,  1050] training loss: 0.00002884
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8595    0.8645    0.8620      1720
   Telophase     0.7678    0.8043    0.7856      1032
          G1     0.3750    0.7500    0.5000         8
    Prophase     0.3396    0.7397    0.4655        73
   Metaphase     0.6394    0.5435    0.5876      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7603      3872
   macro avg     0.6402    0.8146    0.6953      3872
weighted avg     0.7654    0.7603    0.7601      3872

INFO:root:Accuracy of the network on the 3872 validation images: 76 %
INFO:root:epoch71
INFO:root:[72,    50] training loss: 0.00916107
INFO:root:[72,   100] training loss: 0.00839820
INFO:root:[72,   150] training loss: 0.00829795
INFO:root:[72,   200] training loss: 0.00875492
INFO:root:[72,   250] training loss: 0.00915536
INFO:root:[72,   300] training loss: 0.00920128
INFO:root:[72,   350] training loss: 0.00814022
INFO:root:[72,   400] training loss: 0.00001167
INFO:root:[72,   450] training loss: 0.00001348
INFO:root:[72,   500] training loss: 0.00008184
INFO:root:[72,   550] training loss: 0.00021574
INFO:root:[72,   600] training loss: 0.00050325
INFO:root:[72,   650] training loss: 0.00006523
INFO:root:[72,   700] training loss: 0.00004654
INFO:root:[72,   750] training loss: 0.00038138
INFO:root:[72,   800] training loss: 0.00005032
INFO:root:[72,   850] training loss: 0.00003037
INFO:root:[72,   900] training loss: 0.00563701
INFO:root:[72,   950] training loss: 0.00226018
INFO:root:[72,  1000] training loss: 0.00017772
INFO:root:[72,  1050] training loss: 0.00005848
INFO:root:              precision    recall  f1-score   support

           S     0.6667    1.0000    0.8000         2
          G2     0.8423    0.8698    0.8558      1720
   Telophase     0.6840    0.7762    0.7272      1032
          G1     0.7000    0.8750    0.7778         8
    Prophase     0.2944    0.7945    0.4296        73
   Metaphase     0.6152    0.4236    0.5017      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7244      3872
   macro avg     0.6861    0.8199    0.7275      3872
weighted avg     0.7289    0.7244    0.7189      3872

INFO:root:Accuracy of the network on the 3872 validation images: 72 %
INFO:root:epoch72
INFO:root:[73,    50] training loss: 0.00845818
INFO:root:[73,   100] training loss: 0.00853651
INFO:root:[73,   150] training loss: 0.00903364
INFO:root:[73,   200] training loss: 0.00921528
INFO:root:[73,   250] training loss: 0.00927023
INFO:root:[73,   300] training loss: 0.00876115
INFO:root:[73,   350] training loss: 0.00962894
INFO:root:[73,   400] training loss: 0.00004710
INFO:root:[73,   450] training loss: 0.00003211
INFO:root:[73,   500] training loss: 0.00037977
INFO:root:[73,   550] training loss: 0.00052206
INFO:root:[73,   600] training loss: 0.00108080
INFO:root:[73,   650] training loss: 0.00004925
INFO:root:[73,   700] training loss: 0.00003207
INFO:root:[73,   750] training loss: 0.00019983
INFO:root:[73,   800] training loss: 0.00006247
INFO:root:[73,   850] training loss: 0.00002880
INFO:root:[73,   900] training loss: 0.01330648
INFO:root:[73,   950] training loss: 0.00191842
INFO:root:[73,  1000] training loss: 0.00012839
INFO:root:[73,  1050] training loss: 0.00002866
INFO:root:              precision    recall  f1-score   support

           S     0.5000    0.5000    0.5000         2
          G2     0.9121    0.7297    0.8107      1720
   Telophase     0.7940    0.8178    0.8057      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4098    0.6849    0.5128        73
   Metaphase     0.5558    0.6934    0.6170      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7430      3872
   macro avg     0.6585    0.7573    0.6899      3872
weighted avg     0.7749    0.7430    0.7516      3872

INFO:root:Accuracy of the network on the 3872 validation images: 74 %
INFO:root:epoch73
INFO:root:[74,    50] training loss: 0.01030835
INFO:root:[74,   100] training loss: 0.01110535
INFO:root:[74,   150] training loss: 0.01855187
INFO:root:[74,   200] training loss: 0.01184197
INFO:root:[74,   250] training loss: 0.01102119
INFO:root:[74,   300] training loss: 0.01090106
INFO:root:[74,   350] training loss: 0.00853737
INFO:root:[74,   400] training loss: 0.00004587
INFO:root:[74,   450] training loss: 0.00004867
INFO:root:[74,   500] training loss: 0.00014710
INFO:root:[74,   550] training loss: 0.00019960
INFO:root:[74,   600] training loss: 0.00063766
INFO:root:[74,   650] training loss: 0.00006224
INFO:root:[74,   700] training loss: 0.00004259
INFO:root:[74,   750] training loss: 0.00032669
INFO:root:[74,   800] training loss: 0.00005775
INFO:root:[74,   850] training loss: 0.00003592
INFO:root:[74,   900] training loss: 0.00662768
INFO:root:[74,   950] training loss: 0.00174474
INFO:root:[74,  1000] training loss: 0.00015784
INFO:root:[74,  1050] training loss: 0.00002714
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8685    0.8715    0.8700      1720
   Telophase     0.7343    0.8517    0.7887      1032
          G1     0.5000    0.7500    0.6000         8
    Prophase     0.3542    0.6986    0.4700        73
   Metaphase     0.6590    0.5010    0.5692      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7639      3872
   macro avg     0.6594    0.8104    0.7092      3872
weighted avg     0.7662    0.7639    0.7599      3872

INFO:root:Accuracy of the network on the 3872 validation images: 76 %
INFO:root:epoch74
INFO:root:[75,    50] training loss: 0.00911799
INFO:root:[75,   100] training loss: 0.00963584
INFO:root:[75,   150] training loss: 0.00894380
INFO:root:[75,   200] training loss: 0.01018485
INFO:root:[75,   250] training loss: 0.00849517
INFO:root:[75,   300] training loss: 0.00927292
INFO:root:[75,   350] training loss: 0.00850467
INFO:root:[75,   400] training loss: 0.00001021
INFO:root:[75,   450] training loss: 0.00001217
INFO:root:[75,   500] training loss: 0.00008307
INFO:root:[75,   550] training loss: 0.00020223
INFO:root:[75,   600] training loss: 0.00045224
INFO:root:[75,   650] training loss: 0.00006588
INFO:root:[75,   700] training loss: 0.00004149
INFO:root:[75,   750] training loss: 0.00036206
INFO:root:[75,   800] training loss: 0.00005291
INFO:root:[75,   850] training loss: 0.00002854
INFO:root:[75,   900] training loss: 0.00574516
INFO:root:[75,   950] training loss: 0.00142898
INFO:root:[75,  1000] training loss: 0.00011146
INFO:root:[75,  1050] training loss: 0.00002696
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8519    0.9029    0.8767      1720
   Telophase     0.7321    0.8207    0.7739      1032
          G1     0.2727    0.7500    0.4000         8
    Prophase     0.2500    0.5890    0.3510        73
   Metaphase     0.6918    0.4623    0.5542      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7572      3872
   macro avg     0.6141    0.7893    0.6603      3872
weighted avg     0.7646    0.7572    0.7522      3872

INFO:root:Accuracy of the network on the 3872 validation images: 75 %
INFO:root:epoch75
INFO:root:[76,    50] training loss: 0.00979625
INFO:root:[76,   100] training loss: 0.00922421
INFO:root:[76,   150] training loss: 0.00854156
INFO:root:[76,   200] training loss: 0.00767730
INFO:root:[76,   250] training loss: 0.00856046
INFO:root:[76,   300] training loss: 0.00832967
INFO:root:[76,   350] training loss: 0.00643809
INFO:root:[76,   400] training loss: 0.00000788
INFO:root:[76,   450] training loss: 0.00001520
INFO:root:[76,   500] training loss: 0.00004802
INFO:root:[76,   550] training loss: 0.00054662
INFO:root:[76,   600] training loss: 0.00118485
INFO:root:[76,   650] training loss: 0.00014551
INFO:root:[76,   700] training loss: 0.00014654
INFO:root:[76,   750] training loss: 0.00200563
INFO:root:[76,   800] training loss: 0.00065257
INFO:root:[76,   850] training loss: 0.00030799
INFO:root:[76,   900] training loss: 0.00542635
INFO:root:[76,   950] training loss: 0.00151428
INFO:root:[76,  1000] training loss: 0.00010005
INFO:root:[76,  1050] training loss: 0.00003581
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8592    0.9192    0.8882      1720
   Telophase     0.7591    0.8459    0.8002      1032
          G1     0.3333    0.8750    0.4828         8
    Prophase     0.4259    0.6301    0.5083        73
   Metaphase     0.7172    0.5174    0.6011      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7869      3872
   macro avg     0.6564    0.8268    0.7067      3872
weighted avg     0.7853    0.7869    0.7801      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch76
INFO:root:[77,    50] training loss: 0.00852076
INFO:root:[77,   100] training loss: 0.00861706
INFO:root:[77,   150] training loss: 0.00775798
INFO:root:[77,   200] training loss: 0.00774144
INFO:root:[77,   250] training loss: 0.00736929
INFO:root:[77,   300] training loss: 0.00810753
INFO:root:[77,   350] training loss: 0.00625829
INFO:root:[77,   400] training loss: 0.00000654
INFO:root:[77,   450] training loss: 0.00000975
INFO:root:[77,   500] training loss: 0.00004488
INFO:root:[77,   550] training loss: 0.00041171
INFO:root:[77,   600] training loss: 0.00039989
INFO:root:[77,   650] training loss: 0.00010423
INFO:root:[77,   700] training loss: 0.00008398
INFO:root:[77,   750] training loss: 0.00106445
INFO:root:[77,   800] training loss: 0.00054756
INFO:root:[77,   850] training loss: 0.00029791
INFO:root:[77,   900] training loss: 0.00599368
INFO:root:[77,   950] training loss: 0.00163567
INFO:root:[77,  1000] training loss: 0.00011400
INFO:root:[77,  1050] training loss: 0.00003994
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8621    0.9157    0.8881      1720
   Telophase     0.7914    0.8236    0.8072      1032
          G1     0.3043    0.8750    0.4516         8
    Prophase     0.3866    0.6301    0.4792        73
   Metaphase     0.7165    0.5696    0.6347      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7934      3872
   macro avg     0.6516    0.8306    0.7039      3872
weighted avg     0.7942    0.7934    0.7902      3872

INFO:root:Accuracy of the network on the 3872 validation images: 79 %
INFO:root:epoch77
INFO:root:[78,    50] training loss: 0.00871702
INFO:root:[78,   100] training loss: 0.00857617
INFO:root:[78,   150] training loss: 0.00779492
INFO:root:[78,   200] training loss: 0.00842975
INFO:root:[78,   250] training loss: 0.00739858
INFO:root:[78,   300] training loss: 0.00762140
INFO:root:[78,   350] training loss: 0.00628485
INFO:root:[78,   400] training loss: 0.00000641
INFO:root:[78,   450] training loss: 0.00000993
INFO:root:[78,   500] training loss: 0.00003817
INFO:root:[78,   550] training loss: 0.00040453
INFO:root:[78,   600] training loss: 0.00029293
INFO:root:[78,   650] training loss: 0.00006796
INFO:root:[78,   700] training loss: 0.00007893
INFO:root:[78,   750] training loss: 0.00083571
INFO:root:[78,   800] training loss: 0.00045764
INFO:root:[78,   850] training loss: 0.00024636
INFO:root:[78,   900] training loss: 0.00509729
INFO:root:[78,   950] training loss: 0.00099487
INFO:root:[78,  1000] training loss: 0.00009050
INFO:root:[78,  1050] training loss: 0.00003413
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8660    0.9093    0.8871      1720
   Telophase     0.7926    0.8333    0.8125      1032
          G1     0.3684    0.8750    0.5185         8
    Prophase     0.4528    0.6575    0.5363        73
   Metaphase     0.7091    0.5822    0.6394      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7970      3872
   macro avg     0.6698    0.8368    0.7229      3872
weighted avg     0.7956    0.7970    0.7937      3872

INFO:root:Accuracy of the network on the 3872 validation images: 79 %
INFO:root:epoch78
INFO:root:[79,    50] training loss: 0.00839448
INFO:root:[79,   100] training loss: 0.00760023
INFO:root:[79,   150] training loss: 0.00736077
INFO:root:[79,   200] training loss: 0.00716796
INFO:root:[79,   250] training loss: 0.00784583
INFO:root:[79,   300] training loss: 0.00759028
INFO:root:[79,   350] training loss: 0.00648875
INFO:root:[79,   400] training loss: 0.00000580
INFO:root:[79,   450] training loss: 0.00002365
INFO:root:[79,   500] training loss: 0.00003989
INFO:root:[79,   550] training loss: 0.00041615
INFO:root:[79,   600] training loss: 0.00032027
INFO:root:[79,   650] training loss: 0.00007488
INFO:root:[79,   700] training loss: 0.00005997
INFO:root:[79,   750] training loss: 0.00078115
INFO:root:[79,   800] training loss: 0.00033694
INFO:root:[79,   850] training loss: 0.00022254
INFO:root:[79,   900] training loss: 0.00439576
INFO:root:[79,   950] training loss: 0.00114730
INFO:root:[79,  1000] training loss: 0.00008406
INFO:root:[79,  1050] training loss: 0.00004122
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8693    0.9052    0.8869      1720
   Telophase     0.7938    0.8169    0.8052      1032
          G1     0.4118    0.8750    0.5600         8
    Prophase     0.4080    0.6986    0.5152        73
   Metaphase     0.7046    0.5928    0.6439      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7944      3872
   macro avg     0.6696    0.8412    0.7254      3872
weighted avg     0.7955    0.7944    0.7925      3872

INFO:root:Accuracy of the network on the 3872 validation images: 79 %
INFO:root:epoch79
INFO:root:[80,    50] training loss: 0.00747854
INFO:root:[80,   100] training loss: 0.00761938
INFO:root:[80,   150] training loss: 0.00782613
INFO:root:[80,   200] training loss: 0.00713039
INFO:root:[80,   250] training loss: 0.00708837
INFO:root:[80,   300] training loss: 0.00751204
INFO:root:[80,   350] training loss: 0.00647065
INFO:root:[80,   400] training loss: 0.00000628
INFO:root:[80,   450] training loss: 0.00001860
INFO:root:[80,   500] training loss: 0.00003489
INFO:root:[80,   550] training loss: 0.00032462
INFO:root:[80,   600] training loss: 0.00026738
INFO:root:[80,   650] training loss: 0.00006233
INFO:root:[80,   700] training loss: 0.00004990
INFO:root:[80,   750] training loss: 0.00060960
INFO:root:[80,   800] training loss: 0.00032811
INFO:root:[80,   850] training loss: 0.00021088
INFO:root:[80,   900] training loss: 0.00436011
INFO:root:[80,   950] training loss: 0.00164557
INFO:root:[80,  1000] training loss: 0.00008185
INFO:root:[80,  1050] training loss: 0.00002906
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8743    0.8977    0.8858      1720
   Telophase     0.7974    0.8275    0.8122      1032
          G1     0.3684    0.8750    0.5185         8
    Prophase     0.4537    0.6712    0.5414        73
   Metaphase     0.6981    0.6083    0.6501      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7975      3872
   macro avg     0.6703    0.8400    0.7250      3872
weighted avg     0.7977    0.7975    0.7960      3872

INFO:root:Accuracy of the network on the 3872 validation images: 79 %
INFO:root:epoch80
INFO:root:[81,    50] training loss: 0.00797346
INFO:root:[81,   100] training loss: 0.00743741
INFO:root:[81,   150] training loss: 0.00753352
INFO:root:[81,   200] training loss: 0.00693462
INFO:root:[81,   250] training loss: 0.00755326
INFO:root:[81,   300] training loss: 0.00761373
INFO:root:[81,   350] training loss: 0.00635387
INFO:root:[81,   400] training loss: 0.00000562
INFO:root:[81,   450] training loss: 0.00008078
INFO:root:[81,   500] training loss: 0.00002961
INFO:root:[81,   550] training loss: 0.00037362
INFO:root:[81,   600] training loss: 0.00022937
INFO:root:[81,   650] training loss: 0.00005237
INFO:root:[81,   700] training loss: 0.00004805
INFO:root:[81,   750] training loss: 0.00064890
INFO:root:[81,   800] training loss: 0.00027449
INFO:root:[81,   850] training loss: 0.00022523
INFO:root:[81,   900] training loss: 0.00470359
INFO:root:[81,   950] training loss: 0.00114482
INFO:root:[81,  1000] training loss: 0.00010026
INFO:root:[81,  1050] training loss: 0.00003073
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8675    0.8948    0.8809      1720
   Telophase     0.7967    0.8052    0.8010      1032
          G1     0.4118    0.8750    0.5600         8
    Prophase     0.4000    0.6849    0.5051        73
   Metaphase     0.6854    0.6006    0.6402      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7885      3872
   macro avg     0.6659    0.8372    0.7220      3872
weighted avg     0.7902    0.7885    0.7876      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch81
INFO:root:[82,    50] training loss: 0.00741526
INFO:root:[82,   100] training loss: 0.00708354
INFO:root:[82,   150] training loss: 0.00711756
INFO:root:[82,   200] training loss: 0.00683269
INFO:root:[82,   250] training loss: 0.00654144
INFO:root:[82,   300] training loss: 0.00778716
INFO:root:[82,   350] training loss: 0.00654534
INFO:root:[82,   400] training loss: 0.00000489
INFO:root:[82,   450] training loss: 0.00001355
INFO:root:[82,   500] training loss: 0.00003476
INFO:root:[82,   550] training loss: 0.00031079
INFO:root:[82,   600] training loss: 0.00023402
INFO:root:[82,   650] training loss: 0.00006008
INFO:root:[82,   700] training loss: 0.00005115
INFO:root:[82,   750] training loss: 0.00061416
INFO:root:[82,   800] training loss: 0.00032334
INFO:root:[82,   850] training loss: 0.00022448
INFO:root:[82,   900] training loss: 0.00397511
INFO:root:[82,   950] training loss: 0.00149439
INFO:root:[82,  1000] training loss: 0.00009313
INFO:root:[82,  1050] training loss: 0.00003507
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8724    0.8901    0.8812      1720
   Telophase     0.7881    0.8217    0.8046      1032
          G1     0.4118    0.8750    0.5600         8
    Prophase     0.4286    0.6986    0.5312        73
   Metaphase     0.6837    0.5938    0.6356      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7893      3872
   macro avg     0.6692    0.8399    0.7256      3872
weighted avg     0.7901    0.7893    0.7879      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch82
INFO:root:[83,    50] training loss: 0.00737835
INFO:root:[83,   100] training loss: 0.00775677
INFO:root:[83,   150] training loss: 0.00739382
INFO:root:[83,   200] training loss: 0.00671759
INFO:root:[83,   250] training loss: 0.00704386
INFO:root:[83,   300] training loss: 0.00720734
INFO:root:[83,   350] training loss: 0.00606739
INFO:root:[83,   400] training loss: 0.00000449
INFO:root:[83,   450] training loss: 0.00000871
INFO:root:[83,   500] training loss: 0.00003306
INFO:root:[83,   550] training loss: 0.00033300
INFO:root:[83,   600] training loss: 0.00017258
INFO:root:[83,   650] training loss: 0.00005208
INFO:root:[83,   700] training loss: 0.00004212
INFO:root:[83,   750] training loss: 0.00053997
INFO:root:[83,   800] training loss: 0.00034997
INFO:root:[83,   850] training loss: 0.00017594
INFO:root:[83,   900] training loss: 0.00406937
INFO:root:[83,   950] training loss: 0.00111734
INFO:root:[83,  1000] training loss: 0.00009522
INFO:root:[83,  1050] training loss: 0.00002991
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8724    0.8907    0.8815      1720
   Telophase     0.7923    0.8169    0.8044      1032
          G1     0.4118    0.8750    0.5600         8
    Prophase     0.4464    0.6849    0.5405        73
   Metaphase     0.6790    0.6015    0.6379      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7900      3872
   macro avg     0.6717    0.8384    0.7273      3872
weighted avg     0.7904    0.7900    0.7888      3872

INFO:root:Accuracy of the network on the 3872 validation images: 79 %
INFO:root:epoch83
INFO:root:[84,    50] training loss: 0.00705757
INFO:root:[84,   100] training loss: 0.00732495
INFO:root:[84,   150] training loss: 0.00729488
INFO:root:[84,   200] training loss: 0.00751211
INFO:root:[84,   250] training loss: 0.00669608
INFO:root:[84,   300] training loss: 0.00724283
INFO:root:[84,   350] training loss: 0.00658123
INFO:root:[84,   400] training loss: 0.00000530
INFO:root:[84,   450] training loss: 0.00000933
INFO:root:[84,   500] training loss: 0.00003034
INFO:root:[84,   550] training loss: 0.00025959
INFO:root:[84,   600] training loss: 0.00017942
INFO:root:[84,   650] training loss: 0.00004744
INFO:root:[84,   700] training loss: 0.00003878
INFO:root:[84,   750] training loss: 0.00043028
INFO:root:[84,   800] training loss: 0.00046274
INFO:root:[84,   850] training loss: 0.00015148
INFO:root:[84,   900] training loss: 0.00397373
INFO:root:[84,   950] training loss: 0.00120783
INFO:root:[84,  1000] training loss: 0.00009395
INFO:root:[84,  1050] training loss: 0.00004975
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8835    0.8733    0.8784      1720
   Telophase     0.7873    0.8285    0.8074      1032
          G1     0.4118    0.8750    0.5600         8
    Prophase     0.4673    0.6849    0.5556        73
   Metaphase     0.6681    0.6170    0.6415      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7895      3872
   macro avg     0.6740    0.8398    0.7299      3872
weighted avg     0.7914    0.7895    0.7894      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch84
INFO:root:[85,    50] training loss: 0.00750265
INFO:root:[85,   100] training loss: 0.00721726
INFO:root:[85,   150] training loss: 0.00673907
INFO:root:[85,   200] training loss: 0.00719229
INFO:root:[85,   250] training loss: 0.00664794
INFO:root:[85,   300] training loss: 0.00967526
INFO:root:[85,   350] training loss: 0.00637827
INFO:root:[85,   400] training loss: 0.00000617
INFO:root:[85,   450] training loss: 0.00001172
INFO:root:[85,   500] training loss: 0.00003229
INFO:root:[85,   550] training loss: 0.00021513
INFO:root:[85,   600] training loss: 0.00017391
INFO:root:[85,   650] training loss: 0.00005168
INFO:root:[85,   700] training loss: 0.00003858
INFO:root:[85,   750] training loss: 0.00050705
INFO:root:[85,   800] training loss: 0.00033659
INFO:root:[85,   850] training loss: 0.00019824
INFO:root:[85,   900] training loss: 0.00404784
INFO:root:[85,   950] training loss: 0.00126436
INFO:root:[85,  1000] training loss: 0.00006499
INFO:root:[85,  1050] training loss: 0.00002967
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8711    0.8919    0.8814      1720
   Telophase     0.8060    0.8130    0.8095      1032
          G1     0.3684    0.8750    0.5185         8
    Prophase     0.4261    0.6712    0.5213        73
   Metaphase     0.6835    0.6141    0.6470      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7926      3872
   macro avg     0.6650    0.8379    0.7206      3872
weighted avg     0.7941    0.7926    0.7920      3872

INFO:root:Accuracy of the network on the 3872 validation images: 79 %
INFO:root:epoch85
INFO:root:[86,    50] training loss: 0.00707965
INFO:root:[86,   100] training loss: 0.00741050
INFO:root:[86,   150] training loss: 0.00720932
INFO:root:[86,   200] training loss: 0.00674495
INFO:root:[86,   250] training loss: 0.00673221
INFO:root:[86,   300] training loss: 0.00740221
INFO:root:[86,   350] training loss: 0.00639557
INFO:root:[86,   400] training loss: 0.00000706
INFO:root:[86,   450] training loss: 0.00000998
INFO:root:[86,   500] training loss: 0.00002565
INFO:root:[86,   550] training loss: 0.00025299
INFO:root:[86,   600] training loss: 0.00019697
INFO:root:[86,   650] training loss: 0.00004048
INFO:root:[86,   700] training loss: 0.00003704
INFO:root:[86,   750] training loss: 0.00050899
INFO:root:[86,   800] training loss: 0.00028015
INFO:root:[86,   850] training loss: 0.00015566
INFO:root:[86,   900] training loss: 0.00353677
INFO:root:[86,   950] training loss: 0.00137154
INFO:root:[86,  1000] training loss: 0.00006668
INFO:root:[86,  1050] training loss: 0.00006655
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8803    0.8855    0.8829      1720
   Telophase     0.7962    0.8217    0.8088      1032
          G1     0.5000    0.8750    0.6364         8
    Prophase     0.4390    0.7397    0.5510        73
   Metaphase     0.6870    0.6199    0.6518      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7949      3872
   macro avg     0.6861    0.8488    0.7425      3872
weighted avg     0.7971    0.7949    0.7946      3872

INFO:root:Accuracy of the network on the 3872 validation images: 79 %
INFO:root:epoch86
INFO:root:[87,    50] training loss: 0.00754726
INFO:root:[87,   100] training loss: 0.00837747
INFO:root:[87,   150] training loss: 0.00713226
INFO:root:[87,   200] training loss: 0.00691372
INFO:root:[87,   250] training loss: 0.00655876
INFO:root:[87,   300] training loss: 0.00750103
INFO:root:[87,   350] training loss: 0.00578180
INFO:root:[87,   400] training loss: 0.00000547
INFO:root:[87,   450] training loss: 0.00001152
INFO:root:[87,   500] training loss: 0.00003709
INFO:root:[87,   550] training loss: 0.00026374
INFO:root:[87,   600] training loss: 0.00014912
INFO:root:[87,   650] training loss: 0.00004191
INFO:root:[87,   700] training loss: 0.00003873
INFO:root:[87,   750] training loss: 0.00044549
INFO:root:[87,   800] training loss: 0.00052357
INFO:root:[87,   850] training loss: 0.00039463
INFO:root:[87,   900] training loss: 0.00413624
INFO:root:[87,   950] training loss: 0.00114556
INFO:root:[87,  1000] training loss: 0.00006018
INFO:root:[87,  1050] training loss: 0.00002292
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8911    0.8471    0.8686      1720
   Telophase     0.7975    0.8169    0.8071      1032
          G1     0.4667    0.8750    0.6087         8
    Prophase     0.4370    0.7123    0.5417        73
   Metaphase     0.6468    0.6499    0.6483      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7841      3872
   macro avg     0.6770    0.8430    0.7344      3872
weighted avg     0.7914    0.7841    0.7867      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch87
INFO:root:[88,    50] training loss: 0.00764468
INFO:root:[88,   100] training loss: 0.00699467
INFO:root:[88,   150] training loss: 0.00691657
INFO:root:[88,   200] training loss: 0.01156705
INFO:root:[88,   250] training loss: 0.00628033
INFO:root:[88,   300] training loss: 0.00734866
INFO:root:[88,   350] training loss: 0.00594163
INFO:root:[88,   400] training loss: 0.00000508
INFO:root:[88,   450] training loss: 0.00001285
INFO:root:[88,   500] training loss: 0.00003142
INFO:root:[88,   550] training loss: 0.00024313
INFO:root:[88,   600] training loss: 0.00014596
INFO:root:[88,   650] training loss: 0.00003472
INFO:root:[88,   700] training loss: 0.00003891
INFO:root:[88,   750] training loss: 0.00045635
INFO:root:[88,   800] training loss: 0.00046409
INFO:root:[88,   850] training loss: 0.00034949
INFO:root:[88,   900] training loss: 0.00350743
INFO:root:[88,   950] training loss: 0.00206502
INFO:root:[88,  1000] training loss: 0.00006170
INFO:root:[88,  1050] training loss: 0.00002569
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8935    0.8390    0.8654      1720
   Telophase     0.8011    0.8159    0.8084      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4322    0.6986    0.5340        73
   Metaphase     0.6413    0.6605    0.6508      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7828      3872
   macro avg     0.6722    0.8413    0.7298      3872
weighted avg     0.7918    0.7828    0.7861      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch88
INFO:root:[89,    50] training loss: 0.00687489
INFO:root:[89,   100] training loss: 0.00706061
INFO:root:[89,   150] training loss: 0.00677105
INFO:root:[89,   200] training loss: 0.00700004
INFO:root:[89,   250] training loss: 0.00732514
INFO:root:[89,   300] training loss: 0.00705708
INFO:root:[89,   350] training loss: 0.00626431
INFO:root:[89,   400] training loss: 0.00000528
INFO:root:[89,   450] training loss: 0.00000962
INFO:root:[89,   500] training loss: 0.00002106
INFO:root:[89,   550] training loss: 0.00024642
INFO:root:[89,   600] training loss: 0.00013859
INFO:root:[89,   650] training loss: 0.00003881
INFO:root:[89,   700] training loss: 0.00003615
INFO:root:[89,   750] training loss: 0.00043994
INFO:root:[89,   800] training loss: 0.00035437
INFO:root:[89,   850] training loss: 0.00030776
INFO:root:[89,   900] training loss: 0.00380145
INFO:root:[89,   950] training loss: 0.00102346
INFO:root:[89,  1000] training loss: 0.00006635
INFO:root:[89,  1050] training loss: 0.00002766
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8937    0.8407    0.8664      1720
   Telophase     0.7996    0.8081    0.8039      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4397    0.6986    0.5397        73
   Metaphase     0.6381    0.6615    0.6496      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7818      3872
   macro avg     0.6726    0.8406    0.7299      3872
weighted avg     0.7907    0.7818    0.7851      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch89
INFO:root:[90,    50] training loss: 0.00706819
INFO:root:[90,   100] training loss: 0.00968901
INFO:root:[90,   150] training loss: 0.00658378
INFO:root:[90,   200] training loss: 0.00647810
INFO:root:[90,   250] training loss: 0.00678652
INFO:root:[90,   300] training loss: 0.00710444
INFO:root:[90,   350] training loss: 0.00627756
INFO:root:[90,   400] training loss: 0.00000519
INFO:root:[90,   450] training loss: 0.00001054
INFO:root:[90,   500] training loss: 0.00002348
INFO:root:[90,   550] training loss: 0.00027125
INFO:root:[90,   600] training loss: 0.00021475
INFO:root:[90,   650] training loss: 0.00003793
INFO:root:[90,   700] training loss: 0.00003615
INFO:root:[90,   750] training loss: 0.00039102
INFO:root:[90,   800] training loss: 0.00040286
INFO:root:[90,   850] training loss: 0.00038063
INFO:root:[90,   900] training loss: 0.00353708
INFO:root:[90,   950] training loss: 0.00158191
INFO:root:[90,  1000] training loss: 0.00006844
INFO:root:[90,  1050] training loss: 0.00002950
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8952    0.8390    0.8661      1720
   Telophase     0.8004    0.8120    0.8062      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4298    0.7123    0.5361        73
   Metaphase     0.6408    0.6625    0.6515      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7825      3872
   macro avg     0.6719    0.8430    0.7300      3872
weighted avg     0.7921    0.7825    0.7860      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch90
INFO:root:[91,    50] training loss: 0.00720898
INFO:root:[91,   100] training loss: 0.00756321
INFO:root:[91,   150] training loss: 0.00681592
INFO:root:[91,   200] training loss: 0.00692387
INFO:root:[91,   250] training loss: 0.00657199
INFO:root:[91,   300] training loss: 0.00871607
INFO:root:[91,   350] training loss: 0.00586249
INFO:root:[91,   400] training loss: 0.00000565
INFO:root:[91,   450] training loss: 0.00001057
INFO:root:[91,   500] training loss: 0.00003208
INFO:root:[91,   550] training loss: 0.00025784
INFO:root:[91,   600] training loss: 0.00016849
INFO:root:[91,   650] training loss: 0.00004383
INFO:root:[91,   700] training loss: 0.00004026
INFO:root:[91,   750] training loss: 0.00032392
INFO:root:[91,   800] training loss: 0.00041989
INFO:root:[91,   850] training loss: 0.00027418
INFO:root:[91,   900] training loss: 0.00335073
INFO:root:[91,   950] training loss: 0.00132178
INFO:root:[91,  1000] training loss: 0.00007146
INFO:root:[91,  1050] training loss: 0.00002992
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8934    0.8331    0.8622      1720
   Telophase     0.7977    0.8101    0.8038      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4298    0.7123    0.5361        73
   Metaphase     0.6320    0.6576    0.6445      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7782      3872
   macro avg     0.6700    0.8412    0.7281      3872
weighted avg     0.7883    0.7782    0.7818      3872

INFO:root:Accuracy of the network on the 3872 validation images: 77 %
INFO:root:epoch91
INFO:root:[92,    50] training loss: 0.00737953
INFO:root:[92,   100] training loss: 0.00734059
INFO:root:[92,   150] training loss: 0.00725441
INFO:root:[92,   200] training loss: 0.00630323
INFO:root:[92,   250] training loss: 0.00636580
INFO:root:[92,   300] training loss: 0.00730906
INFO:root:[92,   350] training loss: 0.00636314
INFO:root:[92,   400] training loss: 0.00000534
INFO:root:[92,   450] training loss: 0.00001008
INFO:root:[92,   500] training loss: 0.00003287
INFO:root:[92,   550] training loss: 0.00024219
INFO:root:[92,   600] training loss: 0.00015905
INFO:root:[92,   650] training loss: 0.00003623
INFO:root:[92,   700] training loss: 0.00003197
INFO:root:[92,   750] training loss: 0.00029762
INFO:root:[92,   800] training loss: 0.00035963
INFO:root:[92,   850] training loss: 0.00026662
INFO:root:[92,   900] training loss: 0.00370320
INFO:root:[92,   950] training loss: 0.00104418
INFO:root:[92,  1000] training loss: 0.00009128
INFO:root:[92,  1050] training loss: 0.00002292
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8927    0.8366    0.8637      1720
   Telophase     0.8031    0.8062    0.8046      1032
          G1     0.4118    0.8750    0.5600         8
    Prophase     0.4215    0.6986    0.5258        73
   Metaphase     0.6348    0.6625    0.6484      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7797      3872
   macro avg     0.6663    0.8398    0.7242      3872
weighted avg     0.7900    0.7797    0.7835      3872

INFO:root:Accuracy of the network on the 3872 validation images: 77 %
INFO:root:epoch92
INFO:root:[93,    50] training loss: 0.00691663
INFO:root:[93,   100] training loss: 0.00703799
INFO:root:[93,   150] training loss: 0.00661026
INFO:root:[93,   200] training loss: 0.00648447
INFO:root:[93,   250] training loss: 0.00648273
INFO:root:[93,   300] training loss: 0.00708906
INFO:root:[93,   350] training loss: 0.00552666
INFO:root:[93,   400] training loss: 0.00000650
INFO:root:[93,   450] training loss: 0.00001159
INFO:root:[93,   500] training loss: 0.00002607
INFO:root:[93,   550] training loss: 0.00029776
INFO:root:[93,   600] training loss: 0.00024569
INFO:root:[93,   650] training loss: 0.00003382
INFO:root:[93,   700] training loss: 0.00003451
INFO:root:[93,   750] training loss: 0.00034380
INFO:root:[93,   800] training loss: 0.00028979
INFO:root:[93,   850] training loss: 0.00033354
INFO:root:[93,   900] training loss: 0.00371304
INFO:root:[93,   950] training loss: 0.00119929
INFO:root:[93,  1000] training loss: 0.00005717
INFO:root:[93,  1050] training loss: 0.00002641
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8941    0.8442    0.8684      1720
   Telophase     0.8078    0.8062    0.8070      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4333    0.7123    0.5389        73
   Metaphase     0.6437    0.6692    0.6562      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7851      3872
   macro avg     0.6738    0.8439    0.7315      3872
weighted avg     0.7945    0.7851    0.7886      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch93
INFO:root:[94,    50] training loss: 0.00688852
INFO:root:[94,   100] training loss: 0.01066022
INFO:root:[94,   150] training loss: 0.00678222
INFO:root:[94,   200] training loss: 0.00645205
INFO:root:[94,   250] training loss: 0.00616916
INFO:root:[94,   300] training loss: 0.00729184
INFO:root:[94,   350] training loss: 0.00596924
INFO:root:[94,   400] training loss: 0.00000502
INFO:root:[94,   450] training loss: 0.00001264
INFO:root:[94,   500] training loss: 0.00002827
INFO:root:[94,   550] training loss: 0.00029866
INFO:root:[94,   600] training loss: 0.00017154
INFO:root:[94,   650] training loss: 0.00004074
INFO:root:[94,   700] training loss: 0.00003447
INFO:root:[94,   750] training loss: 0.00034381
INFO:root:[94,   800] training loss: 0.00027248
INFO:root:[94,   850] training loss: 0.00025549
INFO:root:[94,   900] training loss: 0.00354220
INFO:root:[94,   950] training loss: 0.00084318
INFO:root:[94,  1000] training loss: 0.00008713
INFO:root:[94,  1050] training loss: 0.00002375
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8931    0.8401    0.8658      1720
   Telophase     0.8035    0.8081    0.8058      1032
          G1     0.3889    0.8750    0.5385         8
    Prophase     0.4153    0.6712    0.5131        73
   Metaphase     0.6393    0.6634    0.6512      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7815      3872
   macro avg     0.6629    0.8368    0.7201      3872
weighted avg     0.7913    0.7815    0.7852      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch94
INFO:root:[95,    50] training loss: 0.00654165
INFO:root:[95,   100] training loss: 0.00741647
INFO:root:[95,   150] training loss: 0.00666311
INFO:root:[95,   200] training loss: 0.00720543
INFO:root:[95,   250] training loss: 0.00632174
INFO:root:[95,   300] training loss: 0.00702898
INFO:root:[95,   350] training loss: 0.00594946
INFO:root:[95,   400] training loss: 0.00000550
INFO:root:[95,   450] training loss: 0.00001037
INFO:root:[95,   500] training loss: 0.00003485
INFO:root:[95,   550] training loss: 0.00028329
INFO:root:[95,   600] training loss: 0.00018934
INFO:root:[95,   650] training loss: 0.00003740
INFO:root:[95,   700] training loss: 0.00003528
INFO:root:[95,   750] training loss: 0.00033235
INFO:root:[95,   800] training loss: 0.00032685
INFO:root:[95,   850] training loss: 0.00022959
INFO:root:[95,   900] training loss: 0.00447569
INFO:root:[95,   950] training loss: 0.00089523
INFO:root:[95,  1000] training loss: 0.00008351
INFO:root:[95,  1050] training loss: 0.00002132
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8939    0.8424    0.8674      1720
   Telophase     0.8072    0.8072    0.8072      1032
          G1     0.3889    0.8750    0.5385         8
    Prophase     0.4188    0.6712    0.5158        73
   Metaphase     0.6425    0.6692    0.6556      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7838      3872
   macro avg     0.6645    0.8379    0.7216      3872
weighted avg     0.7935    0.7838    0.7875      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch95
INFO:root:[96,    50] training loss: 0.00661643
INFO:root:[96,   100] training loss: 0.00706147
INFO:root:[96,   150] training loss: 0.00674062
INFO:root:[96,   200] training loss: 0.00625032
INFO:root:[96,   250] training loss: 0.00845269
INFO:root:[96,   300] training loss: 0.00703658
INFO:root:[96,   350] training loss: 0.00585343
INFO:root:[96,   400] training loss: 0.00000497
INFO:root:[96,   450] training loss: 0.00000954
INFO:root:[96,   500] training loss: 0.00002551
INFO:root:[96,   550] training loss: 0.00027681
INFO:root:[96,   600] training loss: 0.00014872
INFO:root:[96,   650] training loss: 0.00003248
INFO:root:[96,   700] training loss: 0.00002998
INFO:root:[96,   750] training loss: 0.00024512
INFO:root:[96,   800] training loss: 0.00030131
INFO:root:[96,   850] training loss: 0.00026061
INFO:root:[96,   900] training loss: 0.00363322
INFO:root:[96,   950] training loss: 0.00106439
INFO:root:[96,  1000] training loss: 0.00006474
INFO:root:[96,  1050] training loss: 0.00002318
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8944    0.8419    0.8673      1720
   Telophase     0.8070    0.8062    0.8066      1032
          G1     0.3889    0.8750    0.5385         8
    Prophase     0.4224    0.6712    0.5185        73
   Metaphase     0.6411    0.6702    0.6553      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7836      3872
   macro avg     0.6648    0.8378    0.7218      3872
weighted avg     0.7934    0.7836    0.7873      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch96
INFO:root:[97,    50] training loss: 0.00731599
INFO:root:[97,   100] training loss: 0.00691441
INFO:root:[97,   150] training loss: 0.00709093
INFO:root:[97,   200] training loss: 0.00651149
INFO:root:[97,   250] training loss: 0.00682115
INFO:root:[97,   300] training loss: 0.00724598
INFO:root:[97,   350] training loss: 0.00644271
INFO:root:[97,   400] training loss: 0.00000535
INFO:root:[97,   450] training loss: 0.00001641
INFO:root:[97,   500] training loss: 0.00002717
INFO:root:[97,   550] training loss: 0.00025509
INFO:root:[97,   600] training loss: 0.00017248
INFO:root:[97,   650] training loss: 0.00003283
INFO:root:[97,   700] training loss: 0.00003309
INFO:root:[97,   750] training loss: 0.00036511
INFO:root:[97,   800] training loss: 0.00030739
INFO:root:[97,   850] training loss: 0.00020338
INFO:root:[97,   900] training loss: 0.00359427
INFO:root:[97,   950] training loss: 0.00090978
INFO:root:[97,  1000] training loss: 0.00007489
INFO:root:[97,  1050] training loss: 0.00002552
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8941    0.8390    0.8656      1720
   Telophase     0.8052    0.8091    0.8072      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4322    0.6986    0.5340        73
   Metaphase     0.6389    0.6673    0.6528      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7828      3872
   macro avg     0.6726    0.8413    0.7299      3872
weighted avg     0.7925    0.7828    0.7864      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch97
INFO:root:[98,    50] training loss: 0.00751891
INFO:root:[98,   100] training loss: 0.00837439
INFO:root:[98,   150] training loss: 0.00695399
INFO:root:[98,   200] training loss: 0.00613428
INFO:root:[98,   250] training loss: 0.00664690
INFO:root:[98,   300] training loss: 0.00691961
INFO:root:[98,   350] training loss: 0.00571331
INFO:root:[98,   400] training loss: 0.00000492
INFO:root:[98,   450] training loss: 0.00001139
INFO:root:[98,   500] training loss: 0.00003038
INFO:root:[98,   550] training loss: 0.00025223
INFO:root:[98,   600] training loss: 0.00016214
INFO:root:[98,   650] training loss: 0.00003621
INFO:root:[98,   700] training loss: 0.00003080
INFO:root:[98,   750] training loss: 0.00030071
INFO:root:[98,   800] training loss: 0.00030507
INFO:root:[98,   850] training loss: 0.00030931
INFO:root:[98,   900] training loss: 0.00329672
INFO:root:[98,   950] training loss: 0.00100951
INFO:root:[98,  1000] training loss: 0.00007676
INFO:root:[98,  1050] training loss: 0.00003357
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8941    0.8390    0.8656      1720
   Telophase     0.8060    0.8091    0.8075      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4322    0.6986    0.5340        73
   Metaphase     0.6392    0.6683    0.6534      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7831      3872
   macro avg     0.6727    0.8414    0.7301      3872
weighted avg     0.7928    0.7831    0.7866      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch98
INFO:root:[99,    50] training loss: 0.00683947
INFO:root:[99,   100] training loss: 0.00695940
INFO:root:[99,   150] training loss: 0.00722499
INFO:root:[99,   200] training loss: 0.00651697
INFO:root:[99,   250] training loss: 0.00641338
INFO:root:[99,   300] training loss: 0.00744388
INFO:root:[99,   350] training loss: 0.00583199
INFO:root:[99,   400] training loss: 0.00000624
INFO:root:[99,   450] training loss: 0.00000970
INFO:root:[99,   500] training loss: 0.00003134
INFO:root:[99,   550] training loss: 0.00026815
INFO:root:[99,   600] training loss: 0.00020366
INFO:root:[99,   650] training loss: 0.00003074
INFO:root:[99,   700] training loss: 0.00003167
INFO:root:[99,   750] training loss: 0.00025050
INFO:root:[99,   800] training loss: 0.00034180
INFO:root:[99,   850] training loss: 0.00023723
INFO:root:[99,   900] training loss: 0.00349492
INFO:root:[99,   950] training loss: 0.00084481
INFO:root:[99,  1000] training loss: 0.00005643
INFO:root:[99,  1050] training loss: 0.00002575
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8940    0.8384    0.8653      1720
   Telophase     0.8056    0.8072    0.8064      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4322    0.6986    0.5340        73
   Metaphase     0.6375    0.6683    0.6525      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7823      3872
   macro avg     0.6724    0.8411    0.7297      3872
weighted avg     0.7922    0.7823    0.7859      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:epoch99
INFO:root:[100,    50] training loss: 0.00686562
INFO:root:[100,   100] training loss: 0.00694568
INFO:root:[100,   150] training loss: 0.00694838
INFO:root:[100,   200] training loss: 0.00676870
INFO:root:[100,   250] training loss: 0.00624980
INFO:root:[100,   300] training loss: 0.00745914
INFO:root:[100,   350] training loss: 0.00584878
INFO:root:[100,   400] training loss: 0.00000643
INFO:root:[100,   450] training loss: 0.00001196
INFO:root:[100,   500] training loss: 0.00003734
INFO:root:[100,   550] training loss: 0.00025519
INFO:root:[100,   600] training loss: 0.00014094
INFO:root:[100,   650] training loss: 0.00002929
INFO:root:[100,   700] training loss: 0.00003079
INFO:root:[100,   750] training loss: 0.00027146
INFO:root:[100,   800] training loss: 0.00034512
INFO:root:[100,   850] training loss: 0.00025754
INFO:root:[100,   900] training loss: 0.00339420
INFO:root:[100,   950] training loss: 0.00091882
INFO:root:[100,  1000] training loss: 0.00005944
INFO:root:[100,  1050] training loss: 0.00002281
INFO:root:              precision    recall  f1-score   support

           S     0.5000    1.0000    0.6667         2
          G2     0.8945    0.8384    0.8655      1720
   Telophase     0.8064    0.8072    0.8068      1032
          G1     0.4375    0.8750    0.5833         8
    Prophase     0.4322    0.6986    0.5340        73
   Metaphase     0.6381    0.6702    0.6538      1034
    Anaphase     1.0000    1.0000    1.0000         3

    accuracy                         0.7828      3872
   macro avg     0.6727    0.8413    0.7300      3872
weighted avg     0.7928    0.7828    0.7865      3872

INFO:root:Accuracy of the network on the 3872 validation images: 78 %
INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6454 test images: 77 %
INFO:root:The model saved: final_model_dict_jcd_h5.pth
INFO:root:              precision    recall  f1-score   support

           S     0.6667    0.6667    0.6667         3
          G2     0.9051    0.8280    0.8648      2867
   Telophase     0.7929    0.8215    0.8070      1720
          G1     0.4583    0.7857    0.5789        14
    Prophase     0.4278    0.6860    0.5270       121
   Metaphase     0.6279    0.6636    0.6452      1724
    Anaphase     0.8333    1.0000    0.9091         5

    accuracy                         0.7797      6454
   macro avg     0.6731    0.7788    0.7141      6454
weighted avg     0.7911    0.7797    0.7837      6454

INFO:root:          S        G2  Telophase        G1  Prophase  Metaphase  Anaphase
0  0.666667  0.864845   0.806967  0.578947  0.526984   0.645234  0.909091
