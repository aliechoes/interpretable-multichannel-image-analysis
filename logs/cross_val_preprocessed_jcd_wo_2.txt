INFO:root:the deviced being used is cuda:0
INFO:root:Start validation
INFO:root:statistics used: {'mean': tensor([0.3076, 0.0616]), 'std': tensor([0.1652, 0.1627])}
INFO:root:train dataset: 68201, test dataset: 6454
INFO:root:used only channels: [0, 1]; only classes: None
INFO:root:epoch0
INFO:root:[1,    50] training loss: 0.04980656
INFO:root:[1,   100] training loss: 0.04452585
INFO:root:[1,   150] training loss: 0.03487126
INFO:root:[1,   200] training loss: 0.03226764
INFO:root:[1,   250] training loss: 0.03789692
INFO:root:[1,   300] training loss: 0.03042661
INFO:root:[1,   350] training loss: 0.03300882
INFO:root:[1,   400] training loss: 0.00194258
INFO:root:[1,   450] training loss: 0.00021426
INFO:root:[1,   500] training loss: 0.00851101
INFO:root:[1,   550] training loss: 0.00639899
INFO:root:[1,   600] training loss: 0.02364899
INFO:root:[1,   650] training loss: 0.00000167
INFO:root:[1,   700] training loss: 0.00000251
INFO:root:[1,   750] training loss: 0.00000250
INFO:root:[1,   800] training loss: 0.00000291
INFO:root:[1,   850] training loss: 0.00000135
INFO:root:[1,   900] training loss: 0.08387837
INFO:root:[1,   950] training loss: 0.01710897
INFO:root:[1,  1000] training loss: 0.00000849
INFO:root:[1,  1050] training loss: 0.00000891
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2629    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch1
INFO:root:[2,    50] training loss: 0.08896569
INFO:root:[2,   100] training loss: 0.04949288
INFO:root:[2,   150] training loss: 0.03624455
INFO:root:[2,   200] training loss: 0.03391568
INFO:root:[2,   250] training loss: 0.03410919
INFO:root:[2,   300] training loss: 0.02797382
INFO:root:[2,   350] training loss: 0.03470942
INFO:root:[2,   400] training loss: 0.00326794
INFO:root:[2,   450] training loss: 0.00024610
INFO:root:[2,   500] training loss: 0.00862322
INFO:root:[2,   550] training loss: 0.00804853
INFO:root:[2,   600] training loss: 0.02457193
INFO:root:[2,   650] training loss: 0.00003334
INFO:root:[2,   700] training loss: 0.00002905
INFO:root:[2,   750] training loss: 0.00002532
INFO:root:[2,   800] training loss: 0.00002316
INFO:root:[2,   850] training loss: 0.00002084
INFO:root:[2,   900] training loss: 0.06735904
INFO:root:[2,   950] training loss: 0.02334115
INFO:root:[2,  1000] training loss: 0.00004407
INFO:root:[2,  1050] training loss: 0.00002686
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2629    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch2
INFO:root:[3,    50] training loss: 0.10597375
INFO:root:[3,   100] training loss: 0.04573392
INFO:root:[3,   150] training loss: 0.03452847
INFO:root:[3,   200] training loss: 0.03444217
INFO:root:[3,   250] training loss: 0.04162870
INFO:root:[3,   300] training loss: 0.03368191
INFO:root:[3,   350] training loss: 0.03462097
INFO:root:[3,   400] training loss: 0.00387744
INFO:root:[3,   450] training loss: 0.00025019
INFO:root:[3,   500] training loss: 0.00807366
INFO:root:[3,   550] training loss: 0.00806153
INFO:root:[3,   600] training loss: 0.02093427
INFO:root:[3,   650] training loss: 0.00006045
INFO:root:[3,   700] training loss: 0.00002262
INFO:root:[3,   750] training loss: 0.00001477
INFO:root:[3,   800] training loss: 0.00001254
INFO:root:[3,   850] training loss: 0.00001321
INFO:root:[3,   900] training loss: 0.07476927
INFO:root:[3,   950] training loss: 0.01557345
INFO:root:[3,  1000] training loss: 0.00008069
INFO:root:[3,  1050] training loss: 0.00004574
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2629    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch3
INFO:root:[4,    50] training loss: 0.08213329
INFO:root:[4,   100] training loss: 0.03460540
INFO:root:[4,   150] training loss: 0.03125700
INFO:root:[4,   200] training loss: 0.03241260
INFO:root:[4,   250] training loss: 0.03537091
INFO:root:[4,   300] training loss: 0.02855807
INFO:root:[4,   350] training loss: 0.03146861
INFO:root:[4,   400] training loss: 0.00093367
INFO:root:[4,   450] training loss: 0.00019449
INFO:root:[4,   500] training loss: 0.00918464
INFO:root:[4,   550] training loss: 0.00619262
INFO:root:[4,   600] training loss: 0.02040791
INFO:root:[4,   650] training loss: 0.00005405
INFO:root:[4,   700] training loss: 0.00002699
INFO:root:[4,   750] training loss: 0.00002552
INFO:root:[4,   800] training loss: 0.00001971
INFO:root:[4,   850] training loss: 0.00001783
INFO:root:[4,   900] training loss: 0.06686715
INFO:root:[4,   950] training loss: 0.01669202
INFO:root:[4,  1000] training loss: 0.00014560
INFO:root:[4,  1050] training loss: 0.00008080
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2629    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch4
INFO:root:[5,    50] training loss: 0.07046808
INFO:root:[5,   100] training loss: 0.03255250
INFO:root:[5,   150] training loss: 0.02985179
INFO:root:[5,   200] training loss: 0.02898864
INFO:root:[5,   250] training loss: 0.03378932
INFO:root:[5,   300] training loss: 0.02744940
INFO:root:[5,   350] training loss: 0.03003269
INFO:root:[5,   400] training loss: 0.00213823
INFO:root:[5,   450] training loss: 0.00023655
INFO:root:[5,   500] training loss: 0.00991613
INFO:root:[5,   550] training loss: 0.00538747
INFO:root:[5,   600] training loss: 0.01958550
INFO:root:[5,   650] training loss: 0.00002056
INFO:root:[5,   700] training loss: 0.00001383
INFO:root:[5,   750] training loss: 0.00001095
INFO:root:[5,   800] training loss: 0.00000822
INFO:root:[5,   850] training loss: 0.00000988
INFO:root:[5,   900] training loss: 0.06177418
INFO:root:[5,   950] training loss: 0.01627515
INFO:root:[5,  1000] training loss: 0.00007490
INFO:root:[5,  1050] training loss: 0.00004076
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2629    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch5
INFO:root:[6,    50] training loss: 0.06550839
INFO:root:[6,   100] training loss: 0.03443713
INFO:root:[6,   150] training loss: 0.03058005
INFO:root:[6,   200] training loss: 0.02778168
INFO:root:[6,   250] training loss: 0.03310786
INFO:root:[6,   300] training loss: 0.02841210
INFO:root:[6,   350] training loss: 0.03077102
INFO:root:[6,   400] training loss: 0.00240635
INFO:root:[6,   450] training loss: 0.00023733
INFO:root:[6,   500] training loss: 0.00913684
INFO:root:[6,   550] training loss: 0.00794124
INFO:root:[6,   600] training loss: 0.02103038
INFO:root:[6,   650] training loss: 0.00001862
INFO:root:[6,   700] training loss: 0.00000904
INFO:root:[6,   750] training loss: 0.00000824
INFO:root:[6,   800] training loss: 0.00001359
INFO:root:[6,   850] training loss: 0.00000987
INFO:root:[6,   900] training loss: 0.05973835
INFO:root:[6,   950] training loss: 0.02036234
INFO:root:[6,  1000] training loss: 0.00022877
INFO:root:[6,  1050] training loss: 0.00011340
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2629    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch6
INFO:root:[7,    50] training loss: 0.07018011
INFO:root:[7,   100] training loss: 0.03408932
INFO:root:[7,   150] training loss: 0.03049250
INFO:root:[7,   200] training loss: 0.02642580
INFO:root:[7,   250] training loss: 0.02730091
INFO:root:[7,   300] training loss: 0.02402156
INFO:root:[7,   350] training loss: 0.03073277
INFO:root:[7,   400] training loss: 0.00329990
INFO:root:[7,   450] training loss: 0.00030090
INFO:root:[7,   500] training loss: 0.00915737
INFO:root:[7,   550] training loss: 0.01098828
INFO:root:[7,   600] training loss: 0.02373776
INFO:root:[7,   650] training loss: 0.00001296
INFO:root:[7,   700] training loss: 0.00001223
INFO:root:[7,   750] training loss: 0.00000760
INFO:root:[7,   800] training loss: 0.00001273
INFO:root:[7,   850] training loss: 0.00000492
INFO:root:[7,   900] training loss: 0.06246964
INFO:root:[7,   950] training loss: 0.01669796
INFO:root:[7,  1000] training loss: 0.00020179
INFO:root:[7,  1050] training loss: 0.00009272
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2629    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch7
INFO:root:[8,    50] training loss: 0.05932575
INFO:root:[8,   100] training loss: 0.03057008
INFO:root:[8,   150] training loss: 0.02593808
INFO:root:[8,   200] training loss: 0.02584507
INFO:root:[8,   250] training loss: 0.02588575
INFO:root:[8,   300] training loss: 0.02429092
INFO:root:[8,   350] training loss: 0.02818030
INFO:root:[8,   400] training loss: 0.00172433
INFO:root:[8,   450] training loss: 0.00019489
INFO:root:[8,   500] training loss: 0.00805999
INFO:root:[8,   550] training loss: 0.00722060
INFO:root:[8,   600] training loss: 0.02305340
INFO:root:[8,   650] training loss: 0.00000970
INFO:root:[8,   700] training loss: 0.00000746
INFO:root:[8,   750] training loss: 0.00000810
INFO:root:[8,   800] training loss: 0.00000777
INFO:root:[8,   850] training loss: 0.00001699
INFO:root:[8,   900] training loss: 0.05729762
INFO:root:[8,   950] training loss: 0.01598453
INFO:root:[8,  1000] training loss: 0.00018132
INFO:root:[8,  1050] training loss: 0.00010620
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2629    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch8
INFO:root:[9,    50] training loss: 0.06923794
INFO:root:[9,   100] training loss: 0.02846431
INFO:root:[9,   150] training loss: 0.02590349
INFO:root:[9,   200] training loss: 0.02511789
INFO:root:[9,   250] training loss: 0.02912497
INFO:root:[9,   300] training loss: 0.02605669
INFO:root:[9,   350] training loss: 0.02820602
INFO:root:[9,   400] training loss: 0.00372821
INFO:root:[9,   450] training loss: 0.00024507
INFO:root:[9,   500] training loss: 0.00703496
INFO:root:[9,   550] training loss: 0.00965953
INFO:root:[9,   600] training loss: 0.02288141
INFO:root:[9,   650] training loss: 0.00001021
INFO:root:[9,   700] training loss: 0.00000965
INFO:root:[9,   750] training loss: 0.00000762
INFO:root:[9,   800] training loss: 0.00001031
INFO:root:[9,   850] training loss: 0.00000999
INFO:root:[9,   900] training loss: 0.06344963
INFO:root:[9,   950] training loss: 0.01731271
INFO:root:[9,  1000] training loss: 0.00021153
INFO:root:[9,  1050] training loss: 0.00009753
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2629    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2629      3872
   macro avg     0.0376    0.1429    0.0595      3872
weighted avg     0.0691    0.2629    0.1095      3872

INFO:root:epoch9
INFO:root:[10,    50] training loss: 0.06505242
INFO:root:[10,   100] training loss: 0.02989826
INFO:root:[10,   150] training loss: 0.02660986
INFO:root:[10,   200] training loss: 0.02564269
INFO:root:[10,   250] training loss: 0.02576098
INFO:root:[10,   300] training loss: 0.02324443
INFO:root:[10,   350] training loss: 0.02777454
INFO:root:[10,   400] training loss: 0.00287655
INFO:root:[10,   450] training loss: 0.00026422
INFO:root:[10,   500] training loss: 0.00713215
INFO:root:[10,   550] training loss: 0.00948426
INFO:root:[10,   600] training loss: 0.02451386
INFO:root:[10,   650] training loss: 0.00001179
INFO:root:[10,   700] training loss: 0.00000731
INFO:root:[10,   750] training loss: 0.00000891
INFO:root:[10,   800] training loss: 0.00000839
INFO:root:[10,   850] training loss: 0.00000891
INFO:root:[10,   900] training loss: 0.05737402
INFO:root:[10,   950] training loss: 0.01683966
INFO:root:[10,  1000] training loss: 0.00039027
INFO:root:[10,  1050] training loss: 0.00016445
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     1.0000    0.0135    0.0267        74
          G1     0.2630    1.0000    0.4164      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2632      3872
   macro avg     0.1804    0.1448    0.0633      3872
weighted avg     0.0883    0.2632    0.1100      3872

INFO:root:epoch10
INFO:root:[11,    50] training loss: 0.05757467
INFO:root:[11,   100] training loss: 0.02783932
INFO:root:[11,   150] training loss: 0.02614147
INFO:root:[11,   200] training loss: 0.02397576
INFO:root:[11,   250] training loss: 0.02349763
INFO:root:[11,   300] training loss: 0.02513696
INFO:root:[11,   350] training loss: 0.02601514
INFO:root:[11,   400] training loss: 0.00314304
INFO:root:[11,   450] training loss: 0.00022734
INFO:root:[11,   500] training loss: 0.00698140
INFO:root:[11,   550] training loss: 0.00690343
INFO:root:[11,   600] training loss: 0.02528841
INFO:root:[11,   650] training loss: 0.00000524
INFO:root:[11,   700] training loss: 0.00000370
INFO:root:[11,   750] training loss: 0.00000757
INFO:root:[11,   800] training loss: 0.00000600
INFO:root:[11,   850] training loss: 0.00000651
INFO:root:[11,   900] training loss: 0.05508624
INFO:root:[11,   950] training loss: 0.01526990
INFO:root:[11,  1000] training loss: 0.00029781
INFO:root:[11,  1050] training loss: 0.00013781
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.1250    0.0270    0.0444        74
          G1     0.2650    1.0000    0.4190      1018
    Anaphase     1.0000    0.5000    0.6667         6

    accuracy                         0.2642      3872
   macro avg     0.1986    0.2181    0.1614      3872
weighted avg     0.0736    0.2642    0.1120      3872

INFO:root:epoch11
INFO:root:[12,    50] training loss: 0.05396963
INFO:root:[12,   100] training loss: 0.02444932
INFO:root:[12,   150] training loss: 0.02233866
INFO:root:[12,   200] training loss: 0.02971565
INFO:root:[12,   250] training loss: 0.02463932
INFO:root:[12,   300] training loss: 0.02274303
INFO:root:[12,   350] training loss: 0.02319399
INFO:root:[12,   400] training loss: 0.00208140
INFO:root:[12,   450] training loss: 0.00022999
INFO:root:[12,   500] training loss: 0.00602289
INFO:root:[12,   550] training loss: 0.00641961
INFO:root:[12,   600] training loss: 0.02594374
INFO:root:[12,   650] training loss: 0.00002242
INFO:root:[12,   700] training loss: 0.00001645
INFO:root:[12,   750] training loss: 0.00004022
INFO:root:[12,   800] training loss: 0.00002727
INFO:root:[12,   850] training loss: 0.00002534
INFO:root:[12,   900] training loss: 0.04938675
INFO:root:[12,   950] training loss: 0.01563251
INFO:root:[12,  1000] training loss: 0.00024002
INFO:root:[12,  1050] training loss: 0.00012685
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.2500    0.0135    0.0256        74
          G1     0.2636    1.0000    0.4172      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2632      3872
   macro avg     0.0734    0.1448    0.0633      3872
weighted avg     0.0741    0.2632    0.1102      3872

INFO:root:epoch12
INFO:root:[13,    50] training loss: 0.04862094
INFO:root:[13,   100] training loss: 0.02456502
INFO:root:[13,   150] training loss: 0.02218896
INFO:root:[13,   200] training loss: 0.02138663
INFO:root:[13,   250] training loss: 0.02146607
INFO:root:[13,   300] training loss: 0.02072517
INFO:root:[13,   350] training loss: 0.02177543
INFO:root:[13,   400] training loss: 0.00102489
INFO:root:[13,   450] training loss: 0.00022062
INFO:root:[13,   500] training loss: 0.00523290
INFO:root:[13,   550] training loss: 0.00531782
INFO:root:[13,   600] training loss: 0.02355077
INFO:root:[13,   650] training loss: 0.00001010
INFO:root:[13,   700] training loss: 0.00000666
INFO:root:[13,   750] training loss: 0.00000732
INFO:root:[13,   800] training loss: 0.00000847
INFO:root:[13,   850] training loss: 0.00000810
INFO:root:[13,   900] training loss: 0.04909143
INFO:root:[13,   950] training loss: 0.01595844
INFO:root:[13,  1000] training loss: 0.00016065
INFO:root:[13,  1050] training loss: 0.00008630
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.6667    0.0270    0.0519        74
          G1     0.2631    1.0000    0.4166      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2634      3872
   macro avg     0.1328    0.1467    0.0669      3872
weighted avg     0.0819    0.2634    0.1105      3872

INFO:root:epoch13
INFO:root:[14,    50] training loss: 0.04871905
INFO:root:[14,   100] training loss: 0.02347977
INFO:root:[14,   150] training loss: 0.02144463
INFO:root:[14,   200] training loss: 0.02230309
INFO:root:[14,   250] training loss: 0.02073157
INFO:root:[14,   300] training loss: 0.02169986
INFO:root:[14,   350] training loss: 0.02086212
INFO:root:[14,   400] training loss: 0.00086320
INFO:root:[14,   450] training loss: 0.00018380
INFO:root:[14,   500] training loss: 0.00529910
INFO:root:[14,   550] training loss: 0.00540481
INFO:root:[14,   600] training loss: 0.02284842
INFO:root:[14,   650] training loss: 0.00002116
INFO:root:[14,   700] training loss: 0.00002128
INFO:root:[14,   750] training loss: 0.00002171
INFO:root:[14,   800] training loss: 0.00002133
INFO:root:[14,   850] training loss: 0.00002317
INFO:root:[14,   900] training loss: 0.04362493
INFO:root:[14,   950] training loss: 0.01511710
INFO:root:[14,  1000] training loss: 0.00029275
INFO:root:[14,  1050] training loss: 0.00013817
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.1429    0.1000    0.1176        10
          G2     1.0000    0.0811    0.1500        74
          G1     0.2640    1.0000    0.4177      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2647      3872
   macro avg     0.2010    0.1687    0.0979      3872
weighted avg     0.0889    0.2647    0.1130      3872

INFO:root:epoch14
INFO:root:[15,    50] training loss: 0.04851154
INFO:root:[15,   100] training loss: 0.02213915
INFO:root:[15,   150] training loss: 0.02064141
INFO:root:[15,   200] training loss: 0.02102105
INFO:root:[15,   250] training loss: 0.02053542
INFO:root:[15,   300] training loss: 0.02177210
INFO:root:[15,   350] training loss: 0.02003453
INFO:root:[15,   400] training loss: 0.00071766
INFO:root:[15,   450] training loss: 0.00027834
INFO:root:[15,   500] training loss: 0.00563814
INFO:root:[15,   550] training loss: 0.00568784
INFO:root:[15,   600] training loss: 0.02421477
INFO:root:[15,   650] training loss: 0.00002895
INFO:root:[15,   700] training loss: 0.00002324
INFO:root:[15,   750] training loss: 0.00003023
INFO:root:[15,   800] training loss: 0.00002918
INFO:root:[15,   850] training loss: 0.00003848
INFO:root:[15,   900] training loss: 0.04437551
INFO:root:[15,   950] training loss: 0.01474498
INFO:root:[15,  1000] training loss: 0.00035858
INFO:root:[15,  1050] training loss: 0.00014347
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.3824    0.1757    0.2407        74
          G1     0.2653    1.0000    0.4194      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2663      3872
   macro avg     0.0925    0.1680    0.0943      3872
weighted avg     0.0771    0.2663    0.1149      3872

INFO:root:epoch15
INFO:root:[16,    50] training loss: 0.04586233
INFO:root:[16,   100] training loss: 0.02128339
INFO:root:[16,   150] training loss: 0.02100804
INFO:root:[16,   200] training loss: 0.02172496
INFO:root:[16,   250] training loss: 0.02094111
INFO:root:[16,   300] training loss: 0.02092353
INFO:root:[16,   350] training loss: 0.01911763
INFO:root:[16,   400] training loss: 0.00097775
INFO:root:[16,   450] training loss: 0.00017262
INFO:root:[16,   500] training loss: 0.00533559
INFO:root:[16,   550] training loss: 0.00571339
INFO:root:[16,   600] training loss: 0.02529821
INFO:root:[16,   650] training loss: 0.00002909
INFO:root:[16,   700] training loss: 0.00002599
INFO:root:[16,   750] training loss: 0.00006336
INFO:root:[16,   800] training loss: 0.00004367
INFO:root:[16,   850] training loss: 0.00003495
INFO:root:[16,   900] training loss: 0.04714235
INFO:root:[16,   950] training loss: 0.01659420
INFO:root:[16,  1000] training loss: 0.00043683
INFO:root:[16,  1050] training loss: 0.00018042
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.5000    0.2432    0.3273        74
          G1     0.2657    1.0000    0.4199      1018
    Anaphase     1.0000    0.1667    0.2857         6

    accuracy                         0.2678      3872
   macro avg     0.2522    0.2014    0.1476      3872
weighted avg     0.0810    0.2678    0.1171      3872

INFO:root:epoch16
INFO:root:[17,    50] training loss: 0.05282832
INFO:root:[17,   100] training loss: 0.02103958
INFO:root:[17,   150] training loss: 0.01909492
INFO:root:[17,   200] training loss: 0.02103752
INFO:root:[17,   250] training loss: 0.01961945
INFO:root:[17,   300] training loss: 0.02426250
INFO:root:[17,   350] training loss: 0.02043377
INFO:root:[17,   400] training loss: 0.00098206
INFO:root:[17,   450] training loss: 0.00019704
INFO:root:[17,   500] training loss: 0.00535592
INFO:root:[17,   550] training loss: 0.00558968
INFO:root:[17,   600] training loss: 0.02770738
INFO:root:[17,   650] training loss: 0.00003618
INFO:root:[17,   700] training loss: 0.00002303
INFO:root:[17,   750] training loss: 0.00006902
INFO:root:[17,   800] training loss: 0.00005248
INFO:root:[17,   850] training loss: 0.00004117
INFO:root:[17,   900] training loss: 0.03993216
INFO:root:[17,   950] training loss: 0.01558012
INFO:root:[17,  1000] training loss: 0.00039707
INFO:root:[17,  1050] training loss: 0.00015945
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.2000    0.1000    0.1333        10
          G2     0.3731    0.3378    0.3546        74
          G1     0.2680    1.0000    0.4227      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2696      3872
   macro avg     0.1202    0.2054    0.1301      3872
weighted avg     0.0781    0.2696    0.1182      3872

INFO:root:epoch17
INFO:root:[18,    50] training loss: 0.04489669
INFO:root:[18,   100] training loss: 0.02017643
INFO:root:[18,   150] training loss: 0.02214074
INFO:root:[18,   200] training loss: 0.01886659
INFO:root:[18,   250] training loss: 0.01782532
INFO:root:[18,   300] training loss: 0.01860923
INFO:root:[18,   350] training loss: 0.01914068
INFO:root:[18,   400] training loss: 0.00060180
INFO:root:[18,   450] training loss: 0.00015940
INFO:root:[18,   500] training loss: 0.00537080
INFO:root:[18,   550] training loss: 0.00541120
INFO:root:[18,   600] training loss: 0.02565120
INFO:root:[18,   650] training loss: 0.00003226
INFO:root:[18,   700] training loss: 0.00003080
INFO:root:[18,   750] training loss: 0.00006840
INFO:root:[18,   800] training loss: 0.00006035
INFO:root:[18,   850] training loss: 0.00005133
INFO:root:[18,   900] training loss: 0.04188752
INFO:root:[18,   950] training loss: 0.01676766
INFO:root:[18,  1000] training loss: 0.00047632
INFO:root:[18,  1050] training loss: 0.00017915
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.5000    0.0541    0.0976        74
          G1     0.2635    1.0000    0.4171      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2639      3872
   macro avg     0.1091    0.1506    0.0735      3872
weighted avg     0.0788    0.2639    0.1115      3872

INFO:root:epoch18
INFO:root:[19,    50] training loss: 0.04452248
INFO:root:[19,   100] training loss: 0.02308323
INFO:root:[19,   150] training loss: 0.02080364
INFO:root:[19,   200] training loss: 0.02083916
INFO:root:[19,   250] training loss: 0.02016282
INFO:root:[19,   300] training loss: 0.01978186
INFO:root:[19,   350] training loss: 0.01732989
INFO:root:[19,   400] training loss: 0.00047007
INFO:root:[19,   450] training loss: 0.00014411
INFO:root:[19,   500] training loss: 0.00526660
INFO:root:[19,   550] training loss: 0.00523038
INFO:root:[19,   600] training loss: 0.02520692
INFO:root:[19,   650] training loss: 0.00005004
INFO:root:[19,   700] training loss: 0.00003697
INFO:root:[19,   750] training loss: 0.00004093
INFO:root:[19,   800] training loss: 0.00004567
INFO:root:[19,   850] training loss: 0.00002771
INFO:root:[19,   900] training loss: 0.03761050
INFO:root:[19,   950] training loss: 0.01624996
INFO:root:[19,  1000] training loss: 0.00035209
INFO:root:[19,  1050] training loss: 0.00015481
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.2500    0.1000    0.1429        10
          G2     0.5000    0.0405    0.0750        74
          G1     0.2641    1.0000    0.4178      1018
    Anaphase     1.0000    0.8333    0.9091         6

    accuracy                         0.2652      3872
   macro avg     0.2877    0.2820    0.2207      3872
weighted avg     0.0812    0.2652    0.1131      3872

INFO:root:epoch19
INFO:root:[20,    50] training loss: 0.04645273
INFO:root:[20,   100] training loss: 0.02161031
INFO:root:[20,   150] training loss: 0.01994291
INFO:root:[20,   200] training loss: 0.01970524
INFO:root:[20,   250] training loss: 0.02055521
INFO:root:[20,   300] training loss: 0.01919299
INFO:root:[20,   350] training loss: 0.01835228
INFO:root:[20,   400] training loss: 0.00022312
INFO:root:[20,   450] training loss: 0.00010318
INFO:root:[20,   500] training loss: 0.00507064
INFO:root:[20,   550] training loss: 0.00503372
INFO:root:[20,   600] training loss: 0.02549293
INFO:root:[20,   650] training loss: 0.00004942
INFO:root:[20,   700] training loss: 0.00004585
INFO:root:[20,   750] training loss: 0.00005473
INFO:root:[20,   800] training loss: 0.00005061
INFO:root:[20,   850] training loss: 0.00004256
INFO:root:[20,   900] training loss: 0.03950512
INFO:root:[20,   950] training loss: 0.01850831
INFO:root:[20,  1000] training loss: 0.00035953
INFO:root:[20,  1050] training loss: 0.00014465
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.1667    0.6667    0.2667         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.2727    0.3000    0.2857        10
          G2     0.3333    0.3514    0.3421        74
          G1     0.2697    0.9980    0.4247      1018
    Anaphase     0.7500    0.5000    0.6000         6

    accuracy                         0.2712      3872
   macro avg     0.2561    0.4023    0.2742      3872
weighted avg     0.0793    0.2712    0.1201      3872

INFO:root:epoch20
INFO:root:[21,    50] training loss: 0.04543193
INFO:root:[21,   100] training loss: 0.02031556
INFO:root:[21,   150] training loss: 0.02169042
INFO:root:[21,   200] training loss: 0.02288302
INFO:root:[21,   250] training loss: 0.01947901
INFO:root:[21,   300] training loss: 0.01977852
INFO:root:[21,   350] training loss: 0.01909190
INFO:root:[21,   400] training loss: 0.00021658
INFO:root:[21,   450] training loss: 0.00015986
INFO:root:[21,   500] training loss: 0.00490821
INFO:root:[21,   550] training loss: 0.00510925
INFO:root:[21,   600] training loss: 0.02390052
INFO:root:[21,   650] training loss: 0.00006420
INFO:root:[21,   700] training loss: 0.00005454
INFO:root:[21,   750] training loss: 0.00005329
INFO:root:[21,   800] training loss: 0.00004647
INFO:root:[21,   850] training loss: 0.00006005
INFO:root:[21,   900] training loss: 0.03713536
INFO:root:[21,   950] training loss: 0.01654337
INFO:root:[21,  1000] training loss: 0.00047779
INFO:root:[21,  1050] training loss: 0.00019468
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.3023    0.3514    0.3250        74
          G1     0.2693    0.9990    0.4242      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2699      3872
   macro avg     0.1531    0.2881    0.1887      3872
weighted avg     0.0770    0.2699    0.1182      3872

INFO:root:epoch21
INFO:root:[22,    50] training loss: 0.04444171
INFO:root:[22,   100] training loss: 0.02116759
INFO:root:[22,   150] training loss: 0.01852275
INFO:root:[22,   200] training loss: 0.01989726
INFO:root:[22,   250] training loss: 0.01821648
INFO:root:[22,   300] training loss: 0.01924717
INFO:root:[22,   350] training loss: 0.01864533
INFO:root:[22,   400] training loss: 0.00036301
INFO:root:[22,   450] training loss: 0.00046651
INFO:root:[22,   500] training loss: 0.00482158
INFO:root:[22,   550] training loss: 0.00527359
INFO:root:[22,   600] training loss: 0.02385374
INFO:root:[22,   650] training loss: 0.00007134
INFO:root:[22,   700] training loss: 0.00006228
INFO:root:[22,   750] training loss: 0.00005865
INFO:root:[22,   800] training loss: 0.00005008
INFO:root:[22,   850] training loss: 0.00004637
INFO:root:[22,   900] training loss: 0.03669878
INFO:root:[22,   950] training loss: 0.01756276
INFO:root:[22,  1000] training loss: 0.00050434
INFO:root:[22,  1050] training loss: 0.00021406
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.0000    0.0000    0.0000        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2630    1.0000    0.4164      1018
    Anaphase     1.0000    0.1667    0.2857         6

    accuracy                         0.2632      3872
   macro avg     0.1804    0.1667    0.1003      3872
weighted avg     0.0707    0.2632    0.1099      3872

INFO:root:epoch22
INFO:root:[23,    50] training loss: 0.04071492
INFO:root:[23,   100] training loss: 0.01845929
INFO:root:[23,   150] training loss: 0.01974497
INFO:root:[23,   200] training loss: 0.01935790
INFO:root:[23,   250] training loss: 0.01679387
INFO:root:[23,   300] training loss: 0.01791436
INFO:root:[23,   350] training loss: 0.01673626
INFO:root:[23,   400] training loss: 0.00022534
INFO:root:[23,   450] training loss: 0.00014010
INFO:root:[23,   500] training loss: 0.00481613
INFO:root:[23,   550] training loss: 0.00361279
INFO:root:[23,   600] training loss: 0.02107425
INFO:root:[23,   650] training loss: 0.00011671
INFO:root:[23,   700] training loss: 0.00008908
INFO:root:[23,   750] training loss: 0.00009501
INFO:root:[23,   800] training loss: 0.00008695
INFO:root:[23,   850] training loss: 0.00007395
INFO:root:[23,   900] training loss: 0.03819846
INFO:root:[23,   950] training loss: 0.01566529
INFO:root:[23,  1000] training loss: 0.00048934
INFO:root:[23,  1050] training loss: 0.00022852
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.3333    0.3333         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.2000    0.1000    0.1333        10
          G2     0.5667    0.2297    0.3269        74
          G1     0.2655    1.0000    0.4196      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2678      3872
   macro avg     0.1951    0.2376    0.1733      3872
weighted avg     0.0814    0.2678    0.1172      3872

INFO:root:epoch23
INFO:root:[24,    50] training loss: 0.04062671
INFO:root:[24,   100] training loss: 0.01749718
INFO:root:[24,   150] training loss: 0.01759405
INFO:root:[24,   200] training loss: 0.01782846
INFO:root:[24,   250] training loss: 0.01683380
INFO:root:[24,   300] training loss: 0.01919060
INFO:root:[24,   350] training loss: 0.01817674
INFO:root:[24,   400] training loss: 0.00044115
INFO:root:[24,   450] training loss: 0.00013591
INFO:root:[24,   500] training loss: 0.00456090
INFO:root:[24,   550] training loss: 0.00463015
INFO:root:[24,   600] training loss: 0.02396772
INFO:root:[24,   650] training loss: 0.00010440
INFO:root:[24,   700] training loss: 0.00008545
INFO:root:[24,   750] training loss: 0.00008772
INFO:root:[24,   800] training loss: 0.00008673
INFO:root:[24,   850] training loss: 0.00007429
INFO:root:[24,   900] training loss: 0.03471186
INFO:root:[24,   950] training loss: 0.01599031
INFO:root:[24,  1000] training loss: 0.00036998
INFO:root:[24,  1050] training loss: 0.00018530
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.6000    0.3000    0.4000        10
          G2     0.4146    0.4595    0.4359        74
          G1     0.2695    1.0000    0.4245      1018
    Anaphase     1.0000    0.1667    0.2857         6

    accuracy                         0.2732      3872
   macro avg     0.3739    0.3704    0.2844      3872
weighted avg     0.0821    0.2732    0.1218      3872

INFO:root:epoch24
INFO:root:[25,    50] training loss: 0.03773592
INFO:root:[25,   100] training loss: 0.01710896
INFO:root:[25,   150] training loss: 0.01694858
INFO:root:[25,   200] training loss: 0.01733031
INFO:root:[25,   250] training loss: 0.01942819
INFO:root:[25,   300] training loss: 0.01627094
INFO:root:[25,   350] training loss: 0.01676086
INFO:root:[25,   400] training loss: 0.00027340
INFO:root:[25,   450] training loss: 0.00023904
INFO:root:[25,   500] training loss: 0.00424353
INFO:root:[25,   550] training loss: 0.00500399
INFO:root:[25,   600] training loss: 0.02323999
INFO:root:[25,   650] training loss: 0.00014195
INFO:root:[25,   700] training loss: 0.00010249
INFO:root:[25,   750] training loss: 0.00011720
INFO:root:[25,   800] training loss: 0.00010142
INFO:root:[25,   850] training loss: 0.00008008
INFO:root:[25,   900] training loss: 0.03527465
INFO:root:[25,   950] training loss: 0.01670001
INFO:root:[25,  1000] training loss: 0.00040132
INFO:root:[25,  1050] training loss: 0.00019096
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.4444    0.4000    0.4211        10
          G2     0.4286    0.4459    0.4371        74
          G1     0.2697    1.0000    0.4249      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2745      3872
   macro avg     0.3537    0.5018    0.3896      3872
weighted avg     0.0821    0.2745    0.1230      3872

INFO:root:epoch25
INFO:root:[26,    50] training loss: 0.03829351
INFO:root:[26,   100] training loss: 0.01764833
INFO:root:[26,   150] training loss: 0.01528501
INFO:root:[26,   200] training loss: 0.01634698
INFO:root:[26,   250] training loss: 0.01719607
INFO:root:[26,   300] training loss: 0.01867086
INFO:root:[26,   350] training loss: 0.01556544
INFO:root:[26,   400] training loss: 0.00039250
INFO:root:[26,   450] training loss: 0.00014317
INFO:root:[26,   500] training loss: 0.00483428
INFO:root:[26,   550] training loss: 0.00546481
INFO:root:[26,   600] training loss: 0.02413205
INFO:root:[26,   650] training loss: 0.00018652
INFO:root:[26,   700] training loss: 0.00012056
INFO:root:[26,   750] training loss: 0.00017101
INFO:root:[26,   800] training loss: 0.00014257
INFO:root:[26,   850] training loss: 0.00010213
INFO:root:[26,   900] training loss: 0.03399859
INFO:root:[26,   950] training loss: 0.02056284
INFO:root:[26,  1000] training loss: 0.00040432
INFO:root:[26,  1050] training loss: 0.00020330
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2500    0.3333    0.2857         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.5000    0.5000    0.5000        10
          G2     0.5075    0.4595    0.4823        74
          G1     0.2689    1.0000    0.4238      1018
    Anaphase     1.0000    0.8333    0.9091         6

    accuracy                         0.2745      3872
   macro avg     0.3609    0.4466    0.3716      3872
weighted avg     0.0834    0.2745    0.1236      3872

INFO:root:epoch26
INFO:root:[27,    50] training loss: 0.04043238
INFO:root:[27,   100] training loss: 0.01703793
INFO:root:[27,   150] training loss: 0.01539896
INFO:root:[27,   200] training loss: 0.01668226
INFO:root:[27,   250] training loss: 0.01716667
INFO:root:[27,   300] training loss: 0.01562861
INFO:root:[27,   350] training loss: 0.01482461
INFO:root:[27,   400] training loss: 0.00034046
INFO:root:[27,   450] training loss: 0.00014076
INFO:root:[27,   500] training loss: 0.00453384
INFO:root:[27,   550] training loss: 0.00513650
INFO:root:[27,   600] training loss: 0.02440330
INFO:root:[27,   650] training loss: 0.00017670
INFO:root:[27,   700] training loss: 0.00011984
INFO:root:[27,   750] training loss: 0.00011937
INFO:root:[27,   800] training loss: 0.00011306
INFO:root:[27,   850] training loss: 0.00008152
INFO:root:[27,   900] training loss: 0.03512311
INFO:root:[27,   950] training loss: 0.01734362
INFO:root:[27,  1000] training loss: 0.00040220
INFO:root:[27,  1050] training loss: 0.00019862
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.4615    0.6000    0.5217        10
          G2     0.4737    0.4865    0.4800        74
          G1     0.2697    1.0000    0.4248      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2753      3872
   macro avg     0.3150    0.4409    0.3466      3872
weighted avg     0.0827    0.2753    0.1238      3872

INFO:root:epoch27
INFO:root:[28,    50] training loss: 0.03816286
INFO:root:[28,   100] training loss: 0.01774287
INFO:root:[28,   150] training loss: 0.01504721
INFO:root:[28,   200] training loss: 0.02005280
INFO:root:[28,   250] training loss: 0.01545613
INFO:root:[28,   300] training loss: 0.01704052
INFO:root:[28,   350] training loss: 0.01688993
INFO:root:[28,   400] training loss: 0.00022765
INFO:root:[28,   450] training loss: 0.00027002
INFO:root:[28,   500] training loss: 0.00446483
INFO:root:[28,   550] training loss: 0.00474271
INFO:root:[28,   600] training loss: 0.02411226
INFO:root:[28,   650] training loss: 0.00020505
INFO:root:[28,   700] training loss: 0.00016247
INFO:root:[28,   750] training loss: 0.00014878
INFO:root:[28,   800] training loss: 0.00013343
INFO:root:[28,   850] training loss: 0.00010546
INFO:root:[28,   900] training loss: 0.03235971
INFO:root:[28,   950] training loss: 0.02020772
INFO:root:[28,  1000] training loss: 0.00025969
INFO:root:[28,  1050] training loss: 0.00018432
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.4167    0.5000    0.4545        10
          G2     0.5818    0.4324    0.4961        74
          G1     0.2678    1.0000    0.4225      1018
    Anaphase     1.0000    0.3333    0.5000         6

    accuracy                         0.2730      3872
   macro avg     0.3238    0.3237    0.2676      3872
weighted avg     0.0842    0.2730    0.1225      3872

INFO:root:epoch28
INFO:root:[29,    50] training loss: 0.04059578
INFO:root:[29,   100] training loss: 0.02260630
INFO:root:[29,   150] training loss: 0.02319493
INFO:root:[29,   200] training loss: 0.02314392
INFO:root:[29,   250] training loss: 0.02281466
INFO:root:[29,   300] training loss: 0.02053282
INFO:root:[29,   350] training loss: 0.01887661
INFO:root:[29,   400] training loss: 0.00016253
INFO:root:[29,   450] training loss: 0.00011743
INFO:root:[29,   500] training loss: 0.00494875
INFO:root:[29,   550] training loss: 0.00445336
INFO:root:[29,   600] training loss: 0.02365605
INFO:root:[29,   650] training loss: 0.00020343
INFO:root:[29,   700] training loss: 0.00014488
INFO:root:[29,   750] training loss: 0.00016440
INFO:root:[29,   800] training loss: 0.00011849
INFO:root:[29,   850] training loss: 0.00011486
INFO:root:[29,   900] training loss: 0.03594471
INFO:root:[29,   950] training loss: 0.01892875
INFO:root:[29,  1000] training loss: 0.00044127
INFO:root:[29,  1050] training loss: 0.00022899
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.3333    0.4000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.3636    0.4000    0.3810        10
          G2     0.5294    0.4865    0.5070        74
          G1     0.2690    1.0000    0.4240      1018
    Anaphase     0.8571    1.0000    0.9231         6

    accuracy                         0.2751      3872
   macro avg     0.3599    0.4600    0.3764      3872
weighted avg     0.0835    0.2751    0.1239      3872

INFO:root:epoch29
INFO:root:[30,    50] training loss: 0.03769375
INFO:root:[30,   100] training loss: 0.02055117
INFO:root:[30,   150] training loss: 0.01721961
INFO:root:[30,   200] training loss: 0.01840419
INFO:root:[30,   250] training loss: 0.01688897
INFO:root:[30,   300] training loss: 0.01935241
INFO:root:[30,   350] training loss: 0.01667183
INFO:root:[30,   400] training loss: 0.00016235
INFO:root:[30,   450] training loss: 0.00011211
INFO:root:[30,   500] training loss: 0.00429185
INFO:root:[30,   550] training loss: 0.00513287
INFO:root:[30,   600] training loss: 0.02287302
INFO:root:[30,   650] training loss: 0.00026264
INFO:root:[30,   700] training loss: 0.00019329
INFO:root:[30,   750] training loss: 0.00018472
INFO:root:[30,   800] training loss: 0.00014753
INFO:root:[30,   850] training loss: 0.00011975
INFO:root:[30,   900] training loss: 0.03335099
INFO:root:[30,   950] training loss: 0.01680320
INFO:root:[30,  1000] training loss: 0.00047824
INFO:root:[30,  1050] training loss: 0.00021973
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.3684    0.7000    0.4828        10
          G2     0.4078    0.5676    0.4746        74
          G1     0.2720    1.0000    0.4276      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2771      3872
   macro avg     0.2926    0.4668    0.3407      3872
weighted avg     0.0818    0.2771    0.1243      3872

INFO:root:epoch30
INFO:root:[31,    50] training loss: 0.03635327
INFO:root:[31,   100] training loss: 0.01708936
INFO:root:[31,   150] training loss: 0.01767085
INFO:root:[31,   200] training loss: 0.01641190
INFO:root:[31,   250] training loss: 0.01452199
INFO:root:[31,   300] training loss: 0.01813432
INFO:root:[31,   350] training loss: 0.01572487
INFO:root:[31,   400] training loss: 0.00017064
INFO:root:[31,   450] training loss: 0.00018846
INFO:root:[31,   500] training loss: 0.00453381
INFO:root:[31,   550] training loss: 0.00494042
INFO:root:[31,   600] training loss: 0.02244185
INFO:root:[31,   650] training loss: 0.00025002
INFO:root:[31,   700] training loss: 0.00018972
INFO:root:[31,   750] training loss: 0.00020303
INFO:root:[31,   800] training loss: 0.00015574
INFO:root:[31,   850] training loss: 0.00013600
INFO:root:[31,   900] training loss: 0.03417972
INFO:root:[31,   950] training loss: 0.01782353
INFO:root:[31,  1000] training loss: 0.00052797
INFO:root:[31,  1050] training loss: 0.00023836
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.2000    0.3000    0.2400        10
          G2     0.4151    0.2973    0.3465        74
          G1     0.2681    1.0000    0.4228      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2709      3872
   macro avg     0.2690    0.3710    0.2870      3872
weighted avg     0.0805    0.2709    0.1200      3872

INFO:root:epoch31
INFO:root:[32,    50] training loss: 0.03752228
INFO:root:[32,   100] training loss: 0.01808242
INFO:root:[32,   150] training loss: 0.01585259
INFO:root:[32,   200] training loss: 0.01693446
INFO:root:[32,   250] training loss: 0.01479808
INFO:root:[32,   300] training loss: 0.01568444
INFO:root:[32,   350] training loss: 0.01401097
INFO:root:[32,   400] training loss: 0.00041239
INFO:root:[32,   450] training loss: 0.00037394
INFO:root:[32,   500] training loss: 0.00453228
INFO:root:[32,   550] training loss: 0.00546190
INFO:root:[32,   600] training loss: 0.02238290
INFO:root:[32,   650] training loss: 0.00030240
INFO:root:[32,   700] training loss: 0.00023461
INFO:root:[32,   750] training loss: 0.00021365
INFO:root:[32,   800] training loss: 0.00019704
INFO:root:[32,   850] training loss: 0.00014611
INFO:root:[32,   900] training loss: 0.03173960
INFO:root:[32,   950] training loss: 0.01679925
INFO:root:[32,  1000] training loss: 0.00032326
INFO:root:[32,  1050] training loss: 0.00017853
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     1.0000    0.0038    0.0077      1039
   Telophase     0.5000    0.5000    0.5000        10
          G2     0.4087    0.6351    0.4974        74
          G1     0.2724    0.9990    0.4281      1018
    Anaphase     0.8571    1.0000    0.9231         6

    accuracy                         0.2792      3872
   macro avg     0.5293    0.5435    0.4318      3872
weighted avg     0.3509    0.2792    0.1274      3872

INFO:root:epoch32
INFO:root:[33,    50] training loss: 0.03348906
INFO:root:[33,   100] training loss: 0.01643384
INFO:root:[33,   150] training loss: 0.01529553
INFO:root:[33,   200] training loss: 0.01702177
INFO:root:[33,   250] training loss: 0.01462529
INFO:root:[33,   300] training loss: 0.01542930
INFO:root:[33,   350] training loss: 0.01448745
INFO:root:[33,   400] training loss: 0.00016166
INFO:root:[33,   450] training loss: 0.00010534
INFO:root:[33,   500] training loss: 0.00450324
INFO:root:[33,   550] training loss: 0.00514378
INFO:root:[33,   600] training loss: 0.02163186
INFO:root:[33,   650] training loss: 0.00040553
INFO:root:[33,   700] training loss: 0.00025217
INFO:root:[33,   750] training loss: 0.00025424
INFO:root:[33,   800] training loss: 0.00021667
INFO:root:[33,   850] training loss: 0.00017043
INFO:root:[33,   900] training loss: 0.03424531
INFO:root:[33,   950] training loss: 0.01755141
INFO:root:[33,  1000] training loss: 0.00036947
INFO:root:[33,  1050] training loss: 0.00021267
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     1.0000    0.0010    0.0019      1039
   Telophase     0.4444    0.8000    0.5714        10
          G2     0.2400    0.7297    0.3612        74
          G1     0.2803    0.9961    0.4374      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2802      3872
   macro avg     0.4950    0.5991    0.4205      3872
weighted avg     0.3497    0.2802    0.1259      3872

INFO:root:epoch33
INFO:root:[34,    50] training loss: 0.03337719
INFO:root:[34,   100] training loss: 0.01797069
INFO:root:[34,   150] training loss: 0.01697159
INFO:root:[34,   200] training loss: 0.01490562
INFO:root:[34,   250] training loss: 0.01583208
INFO:root:[34,   300] training loss: 0.01475345
INFO:root:[34,   350] training loss: 0.01290322
INFO:root:[34,   400] training loss: 0.00017364
INFO:root:[34,   450] training loss: 0.00010676
INFO:root:[34,   500] training loss: 0.00455490
INFO:root:[34,   550] training loss: 0.00468619
INFO:root:[34,   600] training loss: 0.02159421
INFO:root:[34,   650] training loss: 0.00042067
INFO:root:[34,   700] training loss: 0.00030291
INFO:root:[34,   750] training loss: 0.00027777
INFO:root:[34,   800] training loss: 0.00022294
INFO:root:[34,   850] training loss: 0.00017713
INFO:root:[34,   900] training loss: 0.03366458
INFO:root:[34,   950] training loss: 0.01772219
INFO:root:[34,  1000] training loss: 0.00029385
INFO:root:[34,  1050] training loss: 0.00019621
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     1.0000    0.0010    0.0019      1039
   Telophase     0.5833    0.7000    0.6364        10
          G2     0.2791    0.6486    0.3902        74
          G1     0.2756    0.9951    0.4317      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.2782      3872
   macro avg     0.4840    0.5731    0.4127      3872
weighted avg     0.3492    0.2782    0.1249      3872

INFO:root:epoch34
INFO:root:[35,    50] training loss: 0.03629519
INFO:root:[35,   100] training loss: 0.01747985
INFO:root:[35,   150] training loss: 0.01549443
INFO:root:[35,   200] training loss: 0.02021438
INFO:root:[35,   250] training loss: 0.01628422
INFO:root:[35,   300] training loss: 0.01642723
INFO:root:[35,   350] training loss: 0.01638174
INFO:root:[35,   400] training loss: 0.00029935
INFO:root:[35,   450] training loss: 0.00010897
INFO:root:[35,   500] training loss: 0.00436193
INFO:root:[35,   550] training loss: 0.00470508
INFO:root:[35,   600] training loss: 0.02021403
INFO:root:[35,   650] training loss: 0.00045235
INFO:root:[35,   700] training loss: 0.00027469
INFO:root:[35,   750] training loss: 0.00024486
INFO:root:[35,   800] training loss: 0.00024255
INFO:root:[35,   850] training loss: 0.00017308
INFO:root:[35,   900] training loss: 0.03747978
INFO:root:[35,   950] training loss: 0.02292311
INFO:root:[35,  1000] training loss: 0.00088043
INFO:root:[35,  1050] training loss: 0.00035168
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.1667    0.1000    0.1250        10
          G2     0.0000    0.0000    0.0000        74
          G1     0.2635    1.0000    0.4171      1018
    Anaphase     1.0000    0.1667    0.2857         6

    accuracy                         0.2634      3872
   macro avg     0.2043    0.1810    0.1183      3872
weighted avg     0.0713    0.2634    0.1104      3872

INFO:root:epoch35
INFO:root:[36,    50] training loss: 0.04303976
INFO:root:[36,   100] training loss: 0.02099709
INFO:root:[36,   150] training loss: 0.02053913
INFO:root:[36,   200] training loss: 0.02050370
INFO:root:[36,   250] training loss: 0.01671359
INFO:root:[36,   300] training loss: 0.01725318
INFO:root:[36,   350] training loss: 0.01597531
INFO:root:[36,   400] training loss: 0.00026216
INFO:root:[36,   450] training loss: 0.00021030
INFO:root:[36,   500] training loss: 0.00406364
INFO:root:[36,   550] training loss: 0.00343458
INFO:root:[36,   600] training loss: 0.01960989
INFO:root:[36,   650] training loss: 0.00045075
INFO:root:[36,   700] training loss: 0.00032411
INFO:root:[36,   750] training loss: 0.00025644
INFO:root:[36,   800] training loss: 0.00020726
INFO:root:[36,   850] training loss: 0.00017507
INFO:root:[36,   900] training loss: 0.03587328
INFO:root:[36,   950] training loss: 0.01904322
INFO:root:[36,  1000] training loss: 0.00051582
INFO:root:[36,  1050] training loss: 0.00025365
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.7143    0.5000    0.5882        10
          G2     0.5484    0.4595    0.5000        74
          G1     0.2684    1.0000    0.4232      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2751      3872
   macro avg     0.4330    0.5180    0.4404      3872
weighted avg     0.0848    0.2751    0.1243      3872

INFO:root:epoch36
INFO:root:[37,    50] training loss: 0.03662457
INFO:root:[37,   100] training loss: 0.01918932
INFO:root:[37,   150] training loss: 0.01784285
INFO:root:[37,   200] training loss: 0.01663793
INFO:root:[37,   250] training loss: 0.01590172
INFO:root:[37,   300] training loss: 0.01756076
INFO:root:[37,   350] training loss: 0.01669544
INFO:root:[37,   400] training loss: 0.00041049
INFO:root:[37,   450] training loss: 0.00027132
INFO:root:[37,   500] training loss: 0.00412338
INFO:root:[37,   550] training loss: 0.00338890
INFO:root:[37,   600] training loss: 0.01988752
INFO:root:[37,   650] training loss: 0.00056097
INFO:root:[37,   700] training loss: 0.00035357
INFO:root:[37,   750] training loss: 0.00031508
INFO:root:[37,   800] training loss: 0.00023216
INFO:root:[37,   850] training loss: 0.00019258
INFO:root:[37,   900] training loss: 0.03270035
INFO:root:[37,   950] training loss: 0.01994430
INFO:root:[37,  1000] training loss: 0.00055914
INFO:root:[37,  1050] training loss: 0.00027156
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.3333    0.4000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.3333    0.5000    0.4000        10
          G2     0.3838    0.5135    0.4393        74
          G1     0.2713    0.9990    0.4267      1018
    Anaphase     0.8571    1.0000    0.9231         6

    accuracy                         0.2756      3872
   macro avg     0.3351    0.4780    0.3699      3872
weighted avg     0.0812    0.2756    0.1233      3872

INFO:root:epoch37
INFO:root:[38,    50] training loss: 0.03342402
INFO:root:[38,   100] training loss: 0.01657473
INFO:root:[38,   150] training loss: 0.01447023
INFO:root:[38,   200] training loss: 0.01567172
INFO:root:[38,   250] training loss: 0.01517288
INFO:root:[38,   300] training loss: 0.01520865
INFO:root:[38,   350] training loss: 0.01419795
INFO:root:[38,   400] training loss: 0.00019505
INFO:root:[38,   450] training loss: 0.00011174
INFO:root:[38,   500] training loss: 0.00358200
INFO:root:[38,   550] training loss: 0.00296126
INFO:root:[38,   600] training loss: 0.01770561
INFO:root:[38,   650] training loss: 0.00061524
INFO:root:[38,   700] training loss: 0.00043165
INFO:root:[38,   750] training loss: 0.00030680
INFO:root:[38,   800] training loss: 0.00024606
INFO:root:[38,   850] training loss: 0.00021351
INFO:root:[38,   900] training loss: 0.02689249
INFO:root:[38,   950] training loss: 0.02032465
INFO:root:[38,  1000] training loss: 0.00047100
INFO:root:[38,  1050] training loss: 0.00025358
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.3333    0.4000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.3500    0.7000    0.4667        10
          G2     0.2200    0.7432    0.3395        74
          G1     0.2827    0.9980    0.4406      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2802      3872
   macro avg     0.3361    0.5392    0.3781      3872
weighted avg     0.0814    0.2802    0.1254      3872

INFO:root:epoch38
INFO:root:[39,    50] training loss: 0.03644072
INFO:root:[39,   100] training loss: 0.01675057
INFO:root:[39,   150] training loss: 0.01475372
INFO:root:[39,   200] training loss: 0.01881093
INFO:root:[39,   250] training loss: 0.01424755
INFO:root:[39,   300] training loss: 0.01654574
INFO:root:[39,   350] training loss: 0.01409706
INFO:root:[39,   400] training loss: 0.00033948
INFO:root:[39,   450] training loss: 0.00013320
INFO:root:[39,   500] training loss: 0.00401590
INFO:root:[39,   550] training loss: 0.00265582
INFO:root:[39,   600] training loss: 0.01670441
INFO:root:[39,   650] training loss: 0.00063988
INFO:root:[39,   700] training loss: 0.00040777
INFO:root:[39,   750] training loss: 0.00028616
INFO:root:[39,   800] training loss: 0.00023137
INFO:root:[39,   850] training loss: 0.00019683
INFO:root:[39,   900] training loss: 0.03023048
INFO:root:[39,   950] training loss: 0.01948257
INFO:root:[39,  1000] training loss: 0.00050595
INFO:root:[39,  1050] training loss: 0.00026539
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.3333    0.3333         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.3333    0.3000    0.3158        10
          G2     0.2294    0.7162    0.3475        74
          G1     0.2788    0.9921    0.4353      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2771      3872
   macro avg     0.3107    0.4774    0.3474      3872
weighted avg     0.0803    0.2771    0.1237      3872

INFO:root:epoch39
INFO:root:[40,    50] training loss: 0.03177247
INFO:root:[40,   100] training loss: 0.01623012
INFO:root:[40,   150] training loss: 0.01473937
INFO:root:[40,   200] training loss: 0.01418610
INFO:root:[40,   250] training loss: 0.01403663
INFO:root:[40,   300] training loss: 0.01711417
INFO:root:[40,   350] training loss: 0.01619753
INFO:root:[40,   400] training loss: 0.00019382
INFO:root:[40,   450] training loss: 0.00013618
INFO:root:[40,   500] training loss: 0.00348648
INFO:root:[40,   550] training loss: 0.00209583
INFO:root:[40,   600] training loss: 0.01728291
INFO:root:[40,   650] training loss: 0.00060185
INFO:root:[40,   700] training loss: 0.00037905
INFO:root:[40,   750] training loss: 0.00035600
INFO:root:[40,   800] training loss: 0.00030326
INFO:root:[40,   850] training loss: 0.00021522
INFO:root:[40,   900] training loss: 0.03416614
INFO:root:[40,   950] training loss: 0.02131284
INFO:root:[40,  1000] training loss: 0.00060868
INFO:root:[40,  1050] training loss: 0.00030141
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     1.0000    0.0010    0.0019      1039
   Telophase     0.1667    0.4000    0.2353        10
          G2     0.3023    0.3514    0.3250        74
          G1     0.2713    1.0000    0.4268      1018
    Anaphase     1.0000    0.8333    0.9091         6

    accuracy                         0.2727      3872
   macro avg     0.4629    0.4646    0.3528      3872
weighted avg     0.3478    0.2727    0.1214      3872

INFO:root:epoch40
INFO:root:[41,    50] training loss: 0.03203633
INFO:root:[41,   100] training loss: 0.01578941
INFO:root:[41,   150] training loss: 0.01589363
INFO:root:[41,   200] training loss: 0.01541441
INFO:root:[41,   250] training loss: 0.01330842
INFO:root:[41,   300] training loss: 0.01519858
INFO:root:[41,   350] training loss: 0.01273984
INFO:root:[41,   400] training loss: 0.00024702
INFO:root:[41,   450] training loss: 0.00011464
INFO:root:[41,   500] training loss: 0.00328902
INFO:root:[41,   550] training loss: 0.00210114
INFO:root:[41,   600] training loss: 0.01639816
INFO:root:[41,   650] training loss: 0.00082257
INFO:root:[41,   700] training loss: 0.00048601
INFO:root:[41,   750] training loss: 0.00032838
INFO:root:[41,   800] training loss: 0.00030789
INFO:root:[41,   850] training loss: 0.00023055
INFO:root:[41,   900] training loss: 0.02976088
INFO:root:[41,   950] training loss: 0.01907571
INFO:root:[41,  1000] training loss: 0.00054550
INFO:root:[41,  1050] training loss: 0.00025683
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.3333    0.4000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     1.0000    0.0010    0.0019      1039
   Telophase     0.2500    0.6000    0.3529        10
          G2     0.1562    0.6757    0.2538        74
          G1     0.2834    0.9794    0.4396      1018
    Anaphase     0.8571    1.0000    0.9231         6

    accuracy                         0.2740      3872
   macro avg     0.4353    0.5128    0.3388      3872
weighted avg     0.3482    0.2740    0.1236      3872

INFO:root:epoch41
INFO:root:[42,    50] training loss: 0.03118150
INFO:root:[42,   100] training loss: 0.01485724
INFO:root:[42,   150] training loss: 0.01387234
INFO:root:[42,   200] training loss: 0.01586970
INFO:root:[42,   250] training loss: 0.01388734
INFO:root:[42,   300] training loss: 0.01493547
INFO:root:[42,   350] training loss: 0.01312584
INFO:root:[42,   400] training loss: 0.00011175
INFO:root:[42,   450] training loss: 0.00008296
INFO:root:[42,   500] training loss: 0.00378402
INFO:root:[42,   550] training loss: 0.00185488
INFO:root:[42,   600] training loss: 0.01700847
INFO:root:[42,   650] training loss: 0.00086881
INFO:root:[42,   700] training loss: 0.00049495
INFO:root:[42,   750] training loss: 0.00038840
INFO:root:[42,   800] training loss: 0.00032998
INFO:root:[42,   850] training loss: 0.00022161
INFO:root:[42,   900] training loss: 0.03040905
INFO:root:[42,   950] training loss: 0.01860253
INFO:root:[42,  1000] training loss: 0.00049341
INFO:root:[42,  1050] training loss: 0.00023754
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.3333    0.4000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.3889    0.7000    0.5000        10
          G2     0.1418    0.7838    0.2402        74
          G1     0.2900    0.9784    0.4473      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.2758      3872
   macro avg     0.2958    0.5422    0.3492      3872
weighted avg     0.0815    0.2758    0.1251      3872

INFO:root:epoch42
INFO:root:[43,    50] training loss: 0.02704242
INFO:root:[43,   100] training loss: 0.01582412
INFO:root:[43,   150] training loss: 0.01578683
INFO:root:[43,   200] training loss: 0.01618377
INFO:root:[43,   250] training loss: 0.01336166
INFO:root:[43,   300] training loss: 0.01397157
INFO:root:[43,   350] training loss: 0.01873038
INFO:root:[43,   400] training loss: 0.00010742
INFO:root:[43,   450] training loss: 0.00008903
INFO:root:[43,   500] training loss: 0.00338395
INFO:root:[43,   550] training loss: 0.00177501
INFO:root:[43,   600] training loss: 0.01542110
INFO:root:[43,   650] training loss: 0.00086524
INFO:root:[43,   700] training loss: 0.00050849
INFO:root:[43,   750] training loss: 0.00039341
INFO:root:[43,   800] training loss: 0.00033368
INFO:root:[43,   850] training loss: 0.00023999
INFO:root:[43,   900] training loss: 0.02885092
INFO:root:[43,   950] training loss: 0.01941354
INFO:root:[43,  1000] training loss: 0.00046209
INFO:root:[43,  1050] training loss: 0.00028731
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0769    0.3333    0.1250         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.1818    0.2000    0.1905        10
          G2     0.1501    0.8378    0.2546        74
          G1     0.2900    0.9784    0.4473      1018
    Anaphase     0.0000    0.0000    0.0000         6

    accuracy                         0.2740      3872
   macro avg     0.0998    0.3357    0.1453      3872
weighted avg     0.0796    0.2740    0.1231      3872

INFO:root:epoch43
INFO:root:[44,    50] training loss: 0.02694399
INFO:root:[44,   100] training loss: 0.01548685
INFO:root:[44,   150] training loss: 0.01568312
INFO:root:[44,   200] training loss: 0.01689380
INFO:root:[44,   250] training loss: 0.01436477
INFO:root:[44,   300] training loss: 0.01616645
INFO:root:[44,   350] training loss: 0.01323881
INFO:root:[44,   400] training loss: 0.00015141
INFO:root:[44,   450] training loss: 0.00008362
INFO:root:[44,   500] training loss: 0.00357647
INFO:root:[44,   550] training loss: 0.00153929
INFO:root:[44,   600] training loss: 0.01589134
INFO:root:[44,   650] training loss: 0.00084378
INFO:root:[44,   700] training loss: 0.00049399
INFO:root:[44,   750] training loss: 0.00037161
INFO:root:[44,   800] training loss: 0.00028298
INFO:root:[44,   850] training loss: 0.00023749
INFO:root:[44,   900] training loss: 0.02812006
INFO:root:[44,   950] training loss: 0.01832768
INFO:root:[44,  1000] training loss: 0.00053392
INFO:root:[44,  1050] training loss: 0.00026686
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.3333    0.3333         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     1.0000    0.0010    0.0019      1039
   Telophase     0.3636    0.8000    0.5000        10
          G2     0.1869    0.7297    0.2975        74
          G1     0.2828    0.9853    0.4394      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.2771      3872
   macro avg     0.3952    0.5499    0.3317      3872
weighted avg     0.3484    0.2771    0.1244      3872

INFO:root:epoch44
INFO:root:[45,    50] training loss: 0.02751819
INFO:root:[45,   100] training loss: 0.01661303
INFO:root:[45,   150] training loss: 0.01373556
INFO:root:[45,   200] training loss: 0.01785610
INFO:root:[45,   250] training loss: 0.01402358
INFO:root:[45,   300] training loss: 0.01482067
INFO:root:[45,   350] training loss: 0.01358078
INFO:root:[45,   400] training loss: 0.00016848
INFO:root:[45,   450] training loss: 0.00010877
INFO:root:[45,   500] training loss: 0.00331762
INFO:root:[45,   550] training loss: 0.00205396
INFO:root:[45,   600] training loss: 0.01598845
INFO:root:[45,   650] training loss: 0.00091642
INFO:root:[45,   700] training loss: 0.00048893
INFO:root:[45,   750] training loss: 0.00041209
INFO:root:[45,   800] training loss: 0.00030703
INFO:root:[45,   850] training loss: 0.00024996
INFO:root:[45,   900] training loss: 0.02771297
INFO:root:[45,   950] training loss: 0.01875776
INFO:root:[45,  1000] training loss: 0.00058224
INFO:root:[45,  1050] training loss: 0.00031624
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.2692    0.7000    0.3889        10
          G2     0.4242    0.5676    0.4855        74
          G1     0.2711    0.9951    0.4262      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.2763      3872
   macro avg     0.3402    0.5613    0.4035      3872
weighted avg     0.0818    0.2763    0.1242      3872

INFO:root:epoch45
INFO:root:[46,    50] training loss: 0.03152175
INFO:root:[46,   100] training loss: 0.01634911
INFO:root:[46,   150] training loss: 0.01475330
INFO:root:[46,   200] training loss: 0.01568860
INFO:root:[46,   250] training loss: 0.01344324
INFO:root:[46,   300] training loss: 0.01573814
INFO:root:[46,   350] training loss: 0.01303177
INFO:root:[46,   400] training loss: 0.00013323
INFO:root:[46,   450] training loss: 0.00009325
INFO:root:[46,   500] training loss: 0.00367562
INFO:root:[46,   550] training loss: 0.00166297
INFO:root:[46,   600] training loss: 0.01456106
INFO:root:[46,   650] training loss: 0.00098601
INFO:root:[46,   700] training loss: 0.00050268
INFO:root:[46,   750] training loss: 0.00040350
INFO:root:[46,   800] training loss: 0.00030771
INFO:root:[46,   850] training loss: 0.00023256
INFO:root:[46,   900] training loss: 0.02945978
INFO:root:[46,   950] training loss: 0.01727445
INFO:root:[46,  1000] training loss: 0.00057022
INFO:root:[46,  1050] training loss: 0.00027075
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.2400    0.6000    0.3429        10
          G2     0.2807    0.6486    0.3918        74
          G1     0.2754    0.9912    0.4310      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.2766      3872
   macro avg     0.2923    0.5581    0.3706      3872
weighted avg     0.0799    0.2766    0.1235      3872

INFO:root:epoch46
INFO:root:[47,    50] training loss: 0.02874242
INFO:root:[47,   100] training loss: 0.01463926
INFO:root:[47,   150] training loss: 0.01415880
INFO:root:[47,   200] training loss: 0.01697413
INFO:root:[47,   250] training loss: 0.01227599
INFO:root:[47,   300] training loss: 0.01414494
INFO:root:[47,   350] training loss: 0.01313061
INFO:root:[47,   400] training loss: 0.00016147
INFO:root:[47,   450] training loss: 0.00010140
INFO:root:[47,   500] training loss: 0.00314943
INFO:root:[47,   550] training loss: 0.00159747
INFO:root:[47,   600] training loss: 0.01526147
INFO:root:[47,   650] training loss: 0.00089112
INFO:root:[47,   700] training loss: 0.00049990
INFO:root:[47,   750] training loss: 0.00040195
INFO:root:[47,   800] training loss: 0.00028470
INFO:root:[47,   850] training loss: 0.00021887
INFO:root:[47,   900] training loss: 0.02540858
INFO:root:[47,   950] training loss: 0.02231979
INFO:root:[47,  1000] training loss: 0.00031959
INFO:root:[47,  1050] training loss: 0.00020664
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.2400    0.6000    0.3429        10
          G2     0.3684    0.6622    0.4734        74
          G1     0.2729    0.9931    0.4281      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2774      3872
   macro avg     0.3640    0.5603    0.4159      3872
weighted avg     0.0815    0.2774    0.1246      3872

INFO:root:epoch47
INFO:root:[48,    50] training loss: 0.03143221
INFO:root:[48,   100] training loss: 0.01641060
INFO:root:[48,   150] training loss: 0.01458345
INFO:root:[48,   200] training loss: 0.01574142
INFO:root:[48,   250] training loss: 0.01272498
INFO:root:[48,   300] training loss: 0.01461595
INFO:root:[48,   350] training loss: 0.01283277
INFO:root:[48,   400] training loss: 0.00014344
INFO:root:[48,   450] training loss: 0.00008412
INFO:root:[48,   500] training loss: 0.00386962
INFO:root:[48,   550] training loss: 0.00155933
INFO:root:[48,   600] training loss: 0.01347295
INFO:root:[48,   650] training loss: 0.00078976
INFO:root:[48,   700] training loss: 0.00045203
INFO:root:[48,   750] training loss: 0.00035384
INFO:root:[48,   800] training loss: 0.00030183
INFO:root:[48,   850] training loss: 0.00023461
INFO:root:[48,   900] training loss: 0.02617874
INFO:root:[48,   950] training loss: 0.01625861
INFO:root:[48,  1000] training loss: 0.00040778
INFO:root:[48,  1050] training loss: 0.00021779
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.3333    0.4000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.3684    0.7000    0.4828        10
          G2     0.2732    0.7568    0.4014        74
          G1     0.2780    0.9941    0.4345      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2794      3872
   macro avg     0.3457    0.5406    0.3884      3872
weighted avg     0.0812    0.2794    0.1250      3872

INFO:root:epoch48
INFO:root:[49,    50] training loss: 0.02814853
INFO:root:[49,   100] training loss: 0.01624409
INFO:root:[49,   150] training loss: 0.01674563
INFO:root:[49,   200] training loss: 0.01559347
INFO:root:[49,   250] training loss: 0.01270096
INFO:root:[49,   300] training loss: 0.01389575
INFO:root:[49,   350] training loss: 0.01203460
INFO:root:[49,   400] training loss: 0.00011597
INFO:root:[49,   450] training loss: 0.00008975
INFO:root:[49,   500] training loss: 0.00258513
INFO:root:[49,   550] training loss: 0.00138045
INFO:root:[49,   600] training loss: 0.01172069
INFO:root:[49,   650] training loss: 0.00080378
INFO:root:[49,   700] training loss: 0.00044467
INFO:root:[49,   750] training loss: 0.00043448
INFO:root:[49,   800] training loss: 0.00035811
INFO:root:[49,   850] training loss: 0.00024949
INFO:root:[49,   900] training loss: 0.02632925
INFO:root:[49,   950] training loss: 0.01600424
INFO:root:[49,  1000] training loss: 0.00039868
INFO:root:[49,  1050] training loss: 0.00021981
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.8889    0.0077    0.0153      1039
   Telophase     0.1509    0.8000    0.2540        10
          G2     0.2763    0.5676    0.3717        74
          G1     0.2765    0.9902    0.4322      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.2774      3872
   macro avg     0.4061    0.5760    0.3574      3872
weighted avg     0.3184    0.2774    0.1273      3872

INFO:root:epoch49
INFO:root:[50,    50] training loss: 0.02727346
INFO:root:[50,   100] training loss: 0.01366663
INFO:root:[50,   150] training loss: 0.01356807
INFO:root:[50,   200] training loss: 0.01527482
INFO:root:[50,   250] training loss: 0.01257884
INFO:root:[50,   300] training loss: 0.01391609
INFO:root:[50,   350] training loss: 0.01234575
INFO:root:[50,   400] training loss: 0.00013745
INFO:root:[50,   450] training loss: 0.00007412
INFO:root:[50,   500] training loss: 0.00290432
INFO:root:[50,   550] training loss: 0.00135626
INFO:root:[50,   600] training loss: 0.01056498
INFO:root:[50,   650] training loss: 0.00086217
INFO:root:[50,   700] training loss: 0.00044205
INFO:root:[50,   750] training loss: 0.00047408
INFO:root:[50,   800] training loss: 0.00033018
INFO:root:[50,   850] training loss: 0.00024498
INFO:root:[50,   900] training loss: 0.02420616
INFO:root:[50,   950] training loss: 0.01445895
INFO:root:[50,  1000] training loss: 0.00038526
INFO:root:[50,  1050] training loss: 0.00017862
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.9688    0.0298    0.0579      1039
   Telophase     0.3684    0.7000    0.4828        10
          G2     0.2162    0.6486    0.3243        74
          G1     0.2789    0.9843    0.4346      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2825      3872
   macro avg     0.4046    0.4804    0.3285      3872
weighted avg     0.3399    0.2825    0.1388      3872

INFO:root:epoch50
INFO:root:[51,    50] training loss: 0.02853246
INFO:root:[51,   100] training loss: 0.01508302
INFO:root:[51,   150] training loss: 0.01466793
INFO:root:[51,   200] training loss: 0.01425929
INFO:root:[51,   250] training loss: 0.01222759
INFO:root:[51,   300] training loss: 0.01419167
INFO:root:[51,   350] training loss: 0.01209784
INFO:root:[51,   400] training loss: 0.00042601
INFO:root:[51,   450] training loss: 0.00006642
INFO:root:[51,   500] training loss: 0.00243839
INFO:root:[51,   550] training loss: 0.00111872
INFO:root:[51,   600] training loss: 0.00959526
INFO:root:[51,   650] training loss: 0.00090130
INFO:root:[51,   700] training loss: 0.00045601
INFO:root:[51,   750] training loss: 0.00052312
INFO:root:[51,   800] training loss: 0.00038670
INFO:root:[51,   850] training loss: 0.00024929
INFO:root:[51,   900] training loss: 0.02268224
INFO:root:[51,   950] training loss: 0.01295189
INFO:root:[51,  1000] training loss: 0.00033343
INFO:root:[51,  1050] training loss: 0.00017688
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     1.0000    0.0212    0.0415      1039
   Telophase     0.2593    0.7000    0.3784        10
          G2     0.2103    0.7703    0.3304        74
          G1     0.2846    0.9902    0.4421      1018
    Anaphase     0.8571    1.0000    0.9231         6

    accuracy                         0.2846      3872
   macro avg     0.4683    0.5926    0.3974      3872
weighted avg     0.3497    0.2846    0.1366      3872

INFO:root:epoch51
INFO:root:[52,    50] training loss: 0.02391953
INFO:root:[52,   100] training loss: 0.01452898
INFO:root:[52,   150] training loss: 0.01304520
INFO:root:[52,   200] training loss: 0.01567196
INFO:root:[52,   250] training loss: 0.01273022
INFO:root:[52,   300] training loss: 0.01354889
INFO:root:[52,   350] training loss: 0.01562238
INFO:root:[52,   400] training loss: 0.00012494
INFO:root:[52,   450] training loss: 0.00010204
INFO:root:[52,   500] training loss: 0.00175666
INFO:root:[52,   550] training loss: 0.00113868
INFO:root:[52,   600] training loss: 0.00836518
INFO:root:[52,   650] training loss: 0.00087238
INFO:root:[52,   700] training loss: 0.00047276
INFO:root:[52,   750] training loss: 0.00052518
INFO:root:[52,   800] training loss: 0.00041675
INFO:root:[52,   850] training loss: 0.00025984
INFO:root:[52,   900] training loss: 0.02314693
INFO:root:[52,   950] training loss: 0.01260473
INFO:root:[52,  1000] training loss: 0.00035729
INFO:root:[52,  1050] training loss: 0.00017751
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.9302    0.0385    0.0739      1039
   Telophase     0.2286    0.8000    0.3556        10
          G2     0.1951    0.7568    0.3102        74
          G1     0.2835    0.9735    0.4392      1018
    Anaphase     0.5000    0.5000    0.5000         6

    accuracy                         0.2841      3872
   macro avg     0.3530    0.5336    0.3033      3872
weighted avg     0.3295    0.2841    0.1433      3872

INFO:root:epoch52
INFO:root:[53,    50] training loss: 0.02280104
INFO:root:[53,   100] training loss: 0.01429898
INFO:root:[53,   150] training loss: 0.01366298
INFO:root:[53,   200] training loss: 0.01386920
INFO:root:[53,   250] training loss: 0.01232381
INFO:root:[53,   300] training loss: 0.01581469
INFO:root:[53,   350] training loss: 0.01140350
INFO:root:[53,   400] training loss: 0.00012554
INFO:root:[53,   450] training loss: 0.00007170
INFO:root:[53,   500] training loss: 0.00182729
INFO:root:[53,   550] training loss: 0.00096754
INFO:root:[53,   600] training loss: 0.00756192
INFO:root:[53,   650] training loss: 0.00090148
INFO:root:[53,   700] training loss: 0.00049639
INFO:root:[53,   750] training loss: 0.00052771
INFO:root:[53,   800] training loss: 0.00036186
INFO:root:[53,   850] training loss: 0.00025450
INFO:root:[53,   900] training loss: 0.02327954
INFO:root:[53,   950] training loss: 0.01082737
INFO:root:[53,  1000] training loss: 0.00029110
INFO:root:[53,  1050] training loss: 0.00016723
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.5000    0.0006    0.0012      1722
           S     0.9259    0.0241    0.0469      1039
   Telophase     0.3043    0.7000    0.4242        10
          G2     0.1908    0.7297    0.3025        74
          G1     0.2828    0.9794    0.4389      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.2820      3872
   macro avg     0.5053    0.5858    0.3829      3872
weighted avg     0.5512    0.2820    0.1371      3872

INFO:root:epoch53
INFO:root:[54,    50] training loss: 0.02035742
INFO:root:[54,   100] training loss: 0.01334890
INFO:root:[54,   150] training loss: 0.01363859
INFO:root:[54,   200] training loss: 0.01292889
INFO:root:[54,   250] training loss: 0.01184093
INFO:root:[54,   300] training loss: 0.01231155
INFO:root:[54,   350] training loss: 0.01225741
INFO:root:[54,   400] training loss: 0.00012201
INFO:root:[54,   450] training loss: 0.00006658
INFO:root:[54,   500] training loss: 0.00151876
INFO:root:[54,   550] training loss: 0.00102969
INFO:root:[54,   600] training loss: 0.00605305
INFO:root:[54,   650] training loss: 0.00081037
INFO:root:[54,   700] training loss: 0.00045141
INFO:root:[54,   750] training loss: 0.00062984
INFO:root:[54,   800] training loss: 0.00039061
INFO:root:[54,   850] training loss: 0.00023759
INFO:root:[54,   900] training loss: 0.01973541
INFO:root:[54,   950] training loss: 0.00912725
INFO:root:[54,  1000] training loss: 0.00029024
INFO:root:[54,  1050] training loss: 0.00016511
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     1.0000    0.0012    0.0023      1722
           S     0.8462    0.0106    0.0209      1039
   Telophase     0.7143    0.5000    0.5882        10
          G2     0.1639    0.7973    0.2719        74
          G1     0.2853    0.9754    0.4415      1018
    Anaphase     0.8571    1.0000    0.9231         6

    accuracy                         0.2784      3872
   macro avg     0.6476    0.5645    0.4164      3872
weighted avg     0.7536    0.2784    0.1314      3872

INFO:root:epoch54
INFO:root:[55,    50] training loss: 0.02195723
INFO:root:[55,   100] training loss: 0.01477616
INFO:root:[55,   150] training loss: 0.01274451
INFO:root:[55,   200] training loss: 0.01465202
INFO:root:[55,   250] training loss: 0.01256562
INFO:root:[55,   300] training loss: 0.01469933
INFO:root:[55,   350] training loss: 0.01213165
INFO:root:[55,   400] training loss: 0.00012148
INFO:root:[55,   450] training loss: 0.00006308
INFO:root:[55,   500] training loss: 0.00146624
INFO:root:[55,   550] training loss: 0.00092665
INFO:root:[55,   600] training loss: 0.00573044
INFO:root:[55,   650] training loss: 0.00077962
INFO:root:[55,   700] training loss: 0.00042671
INFO:root:[55,   750] training loss: 0.00055738
INFO:root:[55,   800] training loss: 0.00039696
INFO:root:[55,   850] training loss: 0.00023030
INFO:root:[55,   900] training loss: 0.02375256
INFO:root:[55,   950] training loss: 0.00780983
INFO:root:[55,  1000] training loss: 0.00041779
INFO:root:[55,  1050] training loss: 0.00018111
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.8750    0.0081    0.0161      1722
           S     0.9027    0.0982    0.1771      1039
   Telophase     0.4211    0.8000    0.5517        10
          G2     0.1473    0.8919    0.2529        74
          G1     0.3004    0.9637    0.4580      1018
    Anaphase     0.8571    1.0000    0.9231         6

    accuracy                         0.3045      3872
   macro avg     0.5957    0.6326    0.4351      3872
weighted avg     0.7161    0.3045    0.1833      3872

INFO:root:epoch55
INFO:root:[56,    50] training loss: 0.01875660
INFO:root:[56,   100] training loss: 0.01379454
INFO:root:[56,   150] training loss: 0.01320996
INFO:root:[56,   200] training loss: 0.01332249
INFO:root:[56,   250] training loss: 0.00998735
INFO:root:[56,   300] training loss: 0.01282603
INFO:root:[56,   350] training loss: 0.01138540
INFO:root:[56,   400] training loss: 0.00007231
INFO:root:[56,   450] training loss: 0.00005012
INFO:root:[56,   500] training loss: 0.00118438
INFO:root:[56,   550] training loss: 0.00164196
INFO:root:[56,   600] training loss: 0.00880076
INFO:root:[56,   650] training loss: 0.00071789
INFO:root:[56,   700] training loss: 0.00037576
INFO:root:[56,   750] training loss: 0.00032824
INFO:root:[56,   800] training loss: 0.00023570
INFO:root:[56,   850] training loss: 0.00017742
INFO:root:[56,   900] training loss: 0.02867544
INFO:root:[56,   950] training loss: 0.01081537
INFO:root:[56,  1000] training loss: 0.00024980
INFO:root:[56,  1050] training loss: 0.00017822
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2000    0.3333    0.2500         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.9032    0.0269    0.0523      1039
   Telophase     0.3529    0.6000    0.4444        10
          G2     0.3121    0.5946    0.4093        74
          G1     0.2748    0.9921    0.4304      1018
    Anaphase     1.0000    0.5000    0.6667         6

    accuracy                         0.2820      3872
   macro avg     0.4347    0.4353    0.3219      3872
weighted avg     0.3232    0.2820    0.1374      3872

INFO:root:epoch56
INFO:root:[57,    50] training loss: 0.02543384
INFO:root:[57,   100] training loss: 0.01403289
INFO:root:[57,   150] training loss: 0.01419937
INFO:root:[57,   200] training loss: 0.01531529
INFO:root:[57,   250] training loss: 0.01486913
INFO:root:[57,   300] training loss: 0.01550659
INFO:root:[57,   350] training loss: 0.01569432
INFO:root:[57,   400] training loss: 0.00027486
INFO:root:[57,   450] training loss: 0.00008054
INFO:root:[57,   500] training loss: 0.00110142
INFO:root:[57,   550] training loss: 0.00077606
INFO:root:[57,   600] training loss: 0.00744455
INFO:root:[57,   650] training loss: 0.00081060
INFO:root:[57,   700] training loss: 0.00042188
INFO:root:[57,   750] training loss: 0.00054341
INFO:root:[57,   800] training loss: 0.00038704
INFO:root:[57,   850] training loss: 0.00024741
INFO:root:[57,   900] training loss: 0.02478860
INFO:root:[57,   950] training loss: 0.00891370
INFO:root:[57,  1000] training loss: 0.00031728
INFO:root:[57,  1050] training loss: 0.00015691
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.8636    0.0110    0.0218      1722
           S     0.8911    0.0866    0.1579      1039
   Telophase     0.3333    0.6000    0.4286        10
          G2     0.1858    0.8514    0.3051        74
          G1     0.2920    0.9695    0.4488      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.3029      3872
   macro avg     0.5570    0.5979    0.4041      3872
weighted avg     0.7059    0.3029    0.1788      3872

INFO:root:epoch57
INFO:root:[58,    50] training loss: 0.01854279
INFO:root:[58,   100] training loss: 0.01369685
INFO:root:[58,   150] training loss: 0.01253889
INFO:root:[58,   200] training loss: 0.01265387
INFO:root:[58,   250] training loss: 0.01291375
INFO:root:[58,   300] training loss: 0.01397767
INFO:root:[58,   350] training loss: 0.01475305
INFO:root:[58,   400] training loss: 0.00015742
INFO:root:[58,   450] training loss: 0.00007723
INFO:root:[58,   500] training loss: 0.00088816
INFO:root:[58,   550] training loss: 0.00074843
INFO:root:[58,   600] training loss: 0.00602872
INFO:root:[58,   650] training loss: 0.00080479
INFO:root:[58,   700] training loss: 0.00043010
INFO:root:[58,   750] training loss: 0.00056677
INFO:root:[58,   800] training loss: 0.00034447
INFO:root:[58,   850] training loss: 0.00024236
INFO:root:[58,   900] training loss: 0.02029790
INFO:root:[58,   950] training loss: 0.00661654
INFO:root:[58,  1000] training loss: 0.00033746
INFO:root:[58,  1050] training loss: 0.00017172
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.3333    0.3333         3
    Prophase     0.9529    0.0470    0.0897      1722
           S     0.8551    0.1761    0.2921      1039
   Telophase     0.2286    0.8000    0.3556        10
          G2     0.1555    0.8784    0.2642        74
          G1     0.3103    0.9470    0.4674      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.3378      3872
   macro avg     0.4908    0.5974    0.3646      3872
weighted avg     0.7396    0.3378    0.2485      3872

INFO:root:epoch58
INFO:root:[59,    50] training loss: 0.01742349
INFO:root:[59,   100] training loss: 0.01472145
INFO:root:[59,   150] training loss: 0.01372345
INFO:root:[59,   200] training loss: 0.01758388
INFO:root:[59,   250] training loss: 0.01243011
INFO:root:[59,   300] training loss: 0.01438395
INFO:root:[59,   350] training loss: 0.01282845
INFO:root:[59,   400] training loss: 0.00011594
INFO:root:[59,   450] training loss: 0.00006395
INFO:root:[59,   500] training loss: 0.00092229
INFO:root:[59,   550] training loss: 0.00066179
INFO:root:[59,   600] training loss: 0.00506262
INFO:root:[59,   650] training loss: 0.00070478
INFO:root:[59,   700] training loss: 0.00040499
INFO:root:[59,   750] training loss: 0.00059428
INFO:root:[59,   800] training loss: 0.00035054
INFO:root:[59,   850] training loss: 0.00023598
INFO:root:[59,   900] training loss: 0.01914662
INFO:root:[59,   950] training loss: 0.01854484
INFO:root:[59,  1000] training loss: 0.00010177
INFO:root:[59,  1050] training loss: 0.00006091
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.7500    1.0000    0.8571         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.0000    0.0000    0.0000      1039
   Telophase     0.1429    0.6000    0.2308        10
          G2     1.0000    0.0135    0.0267        74
          G1     0.2660    0.9980    0.4201      1018
    Anaphase     1.0000    1.0000    1.0000         6

    accuracy                         0.2665      3872
   macro avg     0.4513    0.5159    0.3621      3872
weighted avg     0.0916    0.2665    0.1138      3872

INFO:root:epoch59
INFO:root:[60,    50] training loss: 0.04142953
INFO:root:[60,   100] training loss: 0.01823663
INFO:root:[60,   150] training loss: 0.01674129
INFO:root:[60,   200] training loss: 0.01638028
INFO:root:[60,   250] training loss: 0.01670803
INFO:root:[60,   300] training loss: 0.01736581
INFO:root:[60,   350] training loss: 0.01304032
INFO:root:[60,   400] training loss: 0.00016557
INFO:root:[60,   450] training loss: 0.00009495
INFO:root:[60,   500] training loss: 0.00122637
INFO:root:[60,   550] training loss: 0.00079202
INFO:root:[60,   600] training loss: 0.00560304
INFO:root:[60,   650] training loss: 0.00084283
INFO:root:[60,   700] training loss: 0.00048764
INFO:root:[60,   750] training loss: 0.00046947
INFO:root:[60,   800] training loss: 0.00039209
INFO:root:[60,   850] training loss: 0.00027507
INFO:root:[60,   900] training loss: 0.02619198
INFO:root:[60,   950] training loss: 0.01145972
INFO:root:[60,  1000] training loss: 0.00033819
INFO:root:[60,  1050] training loss: 0.00017134
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     1.0000    0.0006    0.0012      1722
           S     0.8438    0.0520    0.0979      1039
   Telophase     0.3333    0.8000    0.4706        10
          G2     0.3125    0.6757    0.4274        74
          G1     0.2760    0.9784    0.4305      1018
    Anaphase     0.5455    1.0000    0.7059         6

    accuracy                         0.2885      3872
   macro avg     0.5682    0.5962    0.4000      3872
weighted avg     0.7519    0.2885    0.1510      3872

INFO:root:epoch60
INFO:root:[61,    50] training loss: 0.02296896
INFO:root:[61,   100] training loss: 0.01443911
INFO:root:[61,   150] training loss: 0.01330128
INFO:root:[61,   200] training loss: 0.01612110
INFO:root:[61,   250] training loss: 0.01420889
INFO:root:[61,   300] training loss: 0.01484415
INFO:root:[61,   350] training loss: 0.01229949
INFO:root:[61,   400] training loss: 0.00018289
INFO:root:[61,   450] training loss: 0.00009582
INFO:root:[61,   500] training loss: 0.00116149
INFO:root:[61,   550] training loss: 0.00068165
INFO:root:[61,   600] training loss: 0.00481154
INFO:root:[61,   650] training loss: 0.00087626
INFO:root:[61,   700] training loss: 0.00046959
INFO:root:[61,   750] training loss: 0.00060021
INFO:root:[61,   800] training loss: 0.00038889
INFO:root:[61,   850] training loss: 0.00026471
INFO:root:[61,   900] training loss: 0.02021085
INFO:root:[61,   950] training loss: 0.00904424
INFO:root:[61,  1000] training loss: 0.00034429
INFO:root:[61,  1050] training loss: 0.00016155
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.0000    0.0000    0.0000         3
    Prophase     0.0000    0.0000    0.0000      1722
           S     0.7308    0.0183    0.0357      1039
   Telophase     0.3182    0.7000    0.4375        10
          G2     0.1449    0.6892    0.2394        74
          G1     0.2854    0.9705    0.4411      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.2766      3872
   macro avg     0.2970    0.4826    0.2720      3872
weighted avg     0.2756    0.2766    0.1324      3872

INFO:root:epoch61
INFO:root:[62,    50] training loss: 0.02343922
INFO:root:[62,   100] training loss: 0.01319053
INFO:root:[62,   150] training loss: 0.01286743
INFO:root:[62,   200] training loss: 0.01252496
INFO:root:[62,   250] training loss: 0.01205606
INFO:root:[62,   300] training loss: 0.01266703
INFO:root:[62,   350] training loss: 0.01222024
INFO:root:[62,   400] training loss: 0.00013708
INFO:root:[62,   450] training loss: 0.00006232
INFO:root:[62,   500] training loss: 0.00079046
INFO:root:[62,   550] training loss: 0.00079309
INFO:root:[62,   600] training loss: 0.00394434
INFO:root:[62,   650] training loss: 0.00077578
INFO:root:[62,   700] training loss: 0.00041605
INFO:root:[62,   750] training loss: 0.00058122
INFO:root:[62,   800] training loss: 0.00038343
INFO:root:[62,   850] training loss: 0.00025164
INFO:root:[62,   900] training loss: 0.02056693
INFO:root:[62,   950] training loss: 0.00609845
INFO:root:[62,  1000] training loss: 0.00036941
INFO:root:[62,  1050] training loss: 0.00017176
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9764    0.0720    0.1341      1722
           S     0.8617    0.2098    0.3375      1039
   Telophase     0.3182    0.7000    0.4375        10
          G2     0.1419    0.8649    0.2438        74
          G1     0.3184    0.9401    0.4756      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.3559      3872
   macro avg     0.5405    0.6362    0.4286      3872
weighted avg     0.7541    0.3559    0.2827      3872

INFO:root:epoch62
INFO:root:[63,    50] training loss: 0.01633802
INFO:root:[63,   100] training loss: 0.01283745
INFO:root:[63,   150] training loss: 0.01251372
INFO:root:[63,   200] training loss: 0.01393760
INFO:root:[63,   250] training loss: 0.01095005
INFO:root:[63,   300] training loss: 0.01280626
INFO:root:[63,   350] training loss: 0.01132944
INFO:root:[63,   400] training loss: 0.00004874
INFO:root:[63,   450] training loss: 0.00004131
INFO:root:[63,   500] training loss: 0.00066662
INFO:root:[63,   550] training loss: 0.00055783
INFO:root:[63,   600] training loss: 0.00337036
INFO:root:[63,   650] training loss: 0.00061975
INFO:root:[63,   700] training loss: 0.00033471
INFO:root:[63,   750] training loss: 0.00056617
INFO:root:[63,   800] training loss: 0.00036947
INFO:root:[63,   850] training loss: 0.00020017
INFO:root:[63,   900] training loss: 0.02140251
INFO:root:[63,   950] training loss: 0.00506837
INFO:root:[63,  1000] training loss: 0.00035209
INFO:root:[63,  1050] training loss: 0.00016172
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.9931    0.0836    0.1543      1722
           S     0.8375    0.2281    0.3585      1039
   Telophase     0.2500    0.6000    0.3529        10
          G2     0.1111    0.8919    0.1976        74
          G1     0.3252    0.8988    0.4776      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.3554      3872
   macro avg     0.5500    0.6242    0.4297      3872
weighted avg     0.7562    0.3554    0.2968      3872

INFO:root:epoch63
INFO:root:[64,    50] training loss: 0.01656826
INFO:root:[64,   100] training loss: 0.01278584
INFO:root:[64,   150] training loss: 0.01333530
INFO:root:[64,   200] training loss: 0.01370207
INFO:root:[64,   250] training loss: 0.01048339
INFO:root:[64,   300] training loss: 0.01269337
INFO:root:[64,   350] training loss: 0.01285117
INFO:root:[64,   400] training loss: 0.00004972
INFO:root:[64,   450] training loss: 0.00008826
INFO:root:[64,   500] training loss: 0.00049029
INFO:root:[64,   550] training loss: 0.00071399
INFO:root:[64,   600] training loss: 0.00326900
INFO:root:[64,   650] training loss: 0.00063631
INFO:root:[64,   700] training loss: 0.00033906
INFO:root:[64,   750] training loss: 0.00072702
INFO:root:[64,   800] training loss: 0.00032234
INFO:root:[64,   850] training loss: 0.00022376
INFO:root:[64,   900] training loss: 0.01906597
INFO:root:[64,   950] training loss: 0.00428584
INFO:root:[64,  1000] training loss: 0.00033377
INFO:root:[64,  1050] training loss: 0.00018440
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9890    0.2091    0.3452      1722
           S     0.7802    0.4167    0.5433      1039
   Telophase     0.3333    0.6000    0.4286        10
          G2     0.1234    0.9054    0.2172        74
          G1     0.3587    0.8379    0.5024      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.4460      3872
   macro avg     0.5264    0.6623    0.4797      3872
weighted avg     0.7480    0.4460    0.4382      3872

INFO:root:epoch64
INFO:root:[65,    50] training loss: 0.01640417
INFO:root:[65,   100] training loss: 0.01239507
INFO:root:[65,   150] training loss: 0.01464255
INFO:root:[65,   200] training loss: 0.01209661
INFO:root:[65,   250] training loss: 0.01049488
INFO:root:[65,   300] training loss: 0.01247492
INFO:root:[65,   350] training loss: 0.01071661
INFO:root:[65,   400] training loss: 0.00006424
INFO:root:[65,   450] training loss: 0.00005245
INFO:root:[65,   500] training loss: 0.00051929
INFO:root:[65,   550] training loss: 0.00055938
INFO:root:[65,   600] training loss: 0.00273812
INFO:root:[65,   650] training loss: 0.00055230
INFO:root:[65,   700] training loss: 0.00030947
INFO:root:[65,   750] training loss: 0.00062092
INFO:root:[65,   800] training loss: 0.00031616
INFO:root:[65,   850] training loss: 0.00022295
INFO:root:[65,   900] training loss: 0.01682121
INFO:root:[65,   950] training loss: 0.00360994
INFO:root:[65,  1000] training loss: 0.00032068
INFO:root:[65,  1050] training loss: 0.00014367
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6000    1.0000    0.7500         3
    Prophase     0.9859    0.2439    0.3911      1722
           S     0.7287    0.5351    0.6171      1039
   Telophase     0.2963    0.8000    0.4324        10
          G2     0.1262    0.8649    0.2203        74
          G1     0.3560    0.7466    0.4821      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.4693      3872
   macro avg     0.5371    0.7415    0.5276      3872
weighted avg     0.7323    0.4693    0.4734      3872

INFO:root:epoch65
INFO:root:[66,    50] training loss: 0.01455483
INFO:root:[66,   100] training loss: 0.01589482
INFO:root:[66,   150] training loss: 0.01455928
INFO:root:[66,   200] training loss: 0.01217610
INFO:root:[66,   250] training loss: 0.01225497
INFO:root:[66,   300] training loss: 0.01261877
INFO:root:[66,   350] training loss: 0.01132727
INFO:root:[66,   400] training loss: 0.00005929
INFO:root:[66,   450] training loss: 0.00004413
INFO:root:[66,   500] training loss: 0.00043156
INFO:root:[66,   550] training loss: 0.00065570
INFO:root:[66,   600] training loss: 0.00267833
INFO:root:[66,   650] training loss: 0.00054327
INFO:root:[66,   700] training loss: 0.00029933
INFO:root:[66,   750] training loss: 0.00066074
INFO:root:[66,   800] training loss: 0.00035975
INFO:root:[66,   850] training loss: 0.00018444
INFO:root:[66,   900] training loss: 0.01784982
INFO:root:[66,   950] training loss: 0.00369238
INFO:root:[66,  1000] training loss: 0.00026515
INFO:root:[66,  1050] training loss: 0.00010951
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3750    1.0000    0.5455         3
    Prophase     1.0000    0.0871    0.1603      1722
           S     0.7448    0.5842    0.6548      1039
   Telophase     0.2400    0.6000    0.3429        10
          G2     0.1422    0.8784    0.2448        74
          G1     0.3155    0.7466    0.4435      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.4124      3872
   macro avg     0.5096    0.6995    0.4641      3872
weighted avg     0.7323    0.4124    0.3709      3872

INFO:root:epoch66
INFO:root:[67,    50] training loss: 0.01532310
INFO:root:[67,   100] training loss: 0.01166109
INFO:root:[67,   150] training loss: 0.01292235
INFO:root:[67,   200] training loss: 0.01247386
INFO:root:[67,   250] training loss: 0.01065929
INFO:root:[67,   300] training loss: 0.01362138
INFO:root:[67,   350] training loss: 0.01142742
INFO:root:[67,   400] training loss: 0.00005477
INFO:root:[67,   450] training loss: 0.00003247
INFO:root:[67,   500] training loss: 0.00037568
INFO:root:[67,   550] training loss: 0.00061416
INFO:root:[67,   600] training loss: 0.00283611
INFO:root:[67,   650] training loss: 0.00051414
INFO:root:[67,   700] training loss: 0.00029166
INFO:root:[67,   750] training loss: 0.00060573
INFO:root:[67,   800] training loss: 0.00034123
INFO:root:[67,   850] training loss: 0.00020388
INFO:root:[67,   900] training loss: 0.01799972
INFO:root:[67,   950] training loss: 0.00334139
INFO:root:[67,  1000] training loss: 0.00025699
INFO:root:[67,  1050] training loss: 0.00012332
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9854    0.2346    0.3790      1722
           S     0.6969    0.5178    0.5941      1039
   Telophase     0.2963    0.8000    0.4324        10
          G2     0.1225    0.9189    0.2162        74
          G1     0.3470    0.7141    0.4671      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.4527      3872
   macro avg     0.5140    0.6932    0.4923      3872
weighted avg     0.7210    0.4527    0.4577      3872

INFO:root:epoch67
INFO:root:[68,    50] training loss: 0.01425432
INFO:root:[68,   100] training loss: 0.01134113
INFO:root:[68,   150] training loss: 0.01174642
INFO:root:[68,   200] training loss: 0.01258421
INFO:root:[68,   250] training loss: 0.01005381
INFO:root:[68,   300] training loss: 0.01177118
INFO:root:[68,   350] training loss: 0.01064229
INFO:root:[68,   400] training loss: 0.00005747
INFO:root:[68,   450] training loss: 0.00004582
INFO:root:[68,   500] training loss: 0.00029192
INFO:root:[68,   550] training loss: 0.00056019
INFO:root:[68,   600] training loss: 0.00184336
INFO:root:[68,   650] training loss: 0.00046793
INFO:root:[68,   700] training loss: 0.00023642
INFO:root:[68,   750] training loss: 0.00068530
INFO:root:[68,   800] training loss: 0.00028898
INFO:root:[68,   850] training loss: 0.00016110
INFO:root:[68,   900] training loss: 0.01545700
INFO:root:[68,   950] training loss: 0.00321736
INFO:root:[68,  1000] training loss: 0.00021753
INFO:root:[68,  1050] training loss: 0.00011695
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9776    0.3798    0.5471      1722
           S     0.7487    0.5361    0.6248      1039
   Telophase     0.4000    0.6000    0.4800        10
          G2     0.1285    0.8784    0.2241        74
          G1     0.3964    0.7495    0.5185      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.5302      3872
   macro avg     0.5454    0.6872    0.5380      3872
weighted avg     0.7448    0.5302    0.5545      3872

INFO:root:epoch68
INFO:root:[69,    50] training loss: 0.01512765
INFO:root:[69,   100] training loss: 0.01128715
INFO:root:[69,   150] training loss: 0.01308886
INFO:root:[69,   200] training loss: 0.01151691
INFO:root:[69,   250] training loss: 0.00983378
INFO:root:[69,   300] training loss: 0.01191197
INFO:root:[69,   350] training loss: 0.01066118
INFO:root:[69,   400] training loss: 0.00007884
INFO:root:[69,   450] training loss: 0.00003832
INFO:root:[69,   500] training loss: 0.00030958
INFO:root:[69,   550] training loss: 0.00057468
INFO:root:[69,   600] training loss: 0.00188037
INFO:root:[69,   650] training loss: 0.00039449
INFO:root:[69,   700] training loss: 0.00022029
INFO:root:[69,   750] training loss: 0.00062082
INFO:root:[69,   800] training loss: 0.00026346
INFO:root:[69,   850] training loss: 0.00016912
INFO:root:[69,   900] training loss: 0.01641693
INFO:root:[69,   950] training loss: 0.00296776
INFO:root:[69,  1000] training loss: 0.00022592
INFO:root:[69,  1050] training loss: 0.00012054
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9861    0.4123    0.5815      1722
           S     0.7059    0.5428    0.6137      1039
   Telophase     0.3571    0.5000    0.4167        10
          G2     0.1259    0.9324    0.2219        74
          G1     0.4032    0.7033    0.5125      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.5351      3872
   macro avg     0.5112    0.6797    0.5138      3872
weighted avg     0.7385    0.5351    0.5649      3872

INFO:root:epoch69
INFO:root:[70,    50] training loss: 0.01355001
INFO:root:[70,   100] training loss: 0.01188526
INFO:root:[70,   150] training loss: 0.01256110
INFO:root:[70,   200] training loss: 0.01324169
INFO:root:[70,   250] training loss: 0.01103823
INFO:root:[70,   300] training loss: 0.01424626
INFO:root:[70,   350] training loss: 0.01130303
INFO:root:[70,   400] training loss: 0.00005725
INFO:root:[70,   450] training loss: 0.00003431
INFO:root:[70,   500] training loss: 0.00035157
INFO:root:[70,   550] training loss: 0.00058547
INFO:root:[70,   600] training loss: 0.00186459
INFO:root:[70,   650] training loss: 0.00040688
INFO:root:[70,   700] training loss: 0.00024737
INFO:root:[70,   750] training loss: 0.00058770
INFO:root:[70,   800] training loss: 0.00029027
INFO:root:[70,   850] training loss: 0.00017836
INFO:root:[70,   900] training loss: 0.01384751
INFO:root:[70,   950] training loss: 0.00379808
INFO:root:[70,  1000] training loss: 0.00012807
INFO:root:[70,  1050] training loss: 0.00005277
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9692    0.0366    0.0705      1722
           S     0.8609    0.0953    0.1716      1039
   Telophase     0.5833    0.7000    0.6364        10
          G2     0.2308    0.5270    0.3210        74
          G1     0.2837    0.9754    0.4396      1018
    Anaphase     0.7143    0.8333    0.7692         6

    accuracy                         0.3120      3872
   macro avg     0.5917    0.5478    0.4257      3872
weighted avg     0.7441    0.3120    0.2024      3872

INFO:root:epoch70
INFO:root:[71,    50] training loss: 0.02258950
INFO:root:[71,   100] training loss: 0.01646298
INFO:root:[71,   150] training loss: 0.01610804
INFO:root:[71,   200] training loss: 0.01602805
INFO:root:[71,   250] training loss: 0.01442423
INFO:root:[71,   300] training loss: 0.01580048
INFO:root:[71,   350] training loss: 0.01376219
INFO:root:[71,   400] training loss: 0.00008929
INFO:root:[71,   450] training loss: 0.00006468
INFO:root:[71,   500] training loss: 0.00070324
INFO:root:[71,   550] training loss: 0.00069892
INFO:root:[71,   600] training loss: 0.00324097
INFO:root:[71,   650] training loss: 0.00032040
INFO:root:[71,   700] training loss: 0.00021327
INFO:root:[71,   750] training loss: 0.00057009
INFO:root:[71,   800] training loss: 0.00029595
INFO:root:[71,   850] training loss: 0.00020036
INFO:root:[71,   900] training loss: 0.01744829
INFO:root:[71,   950] training loss: 0.00479064
INFO:root:[71,  1000] training loss: 0.00011823
INFO:root:[71,  1050] training loss: 0.00007139
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     1.0000    0.0012    0.0023      1722
           S     0.8316    0.3946    0.5352      1039
   Telophase     0.3478    0.8000    0.4848        10
          G2     0.1292    0.8919    0.2256        74
          G1     0.3066    0.8517    0.4509      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.3515      3872
   macro avg     0.5165    0.6580    0.4205      3872
weighted avg     0.7532    0.3515    0.2703      3872

INFO:root:epoch71
INFO:root:[72,    50] training loss: 0.01813991
INFO:root:[72,   100] training loss: 0.01368894
INFO:root:[72,   150] training loss: 0.01385739
INFO:root:[72,   200] training loss: 0.01177114
INFO:root:[72,   250] training loss: 0.01122165
INFO:root:[72,   300] training loss: 0.01228643
INFO:root:[72,   350] training loss: 0.01131814
INFO:root:[72,   400] training loss: 0.00007982
INFO:root:[72,   450] training loss: 0.00005049
INFO:root:[72,   500] training loss: 0.00032740
INFO:root:[72,   550] training loss: 0.00046369
INFO:root:[72,   600] training loss: 0.00182494
INFO:root:[72,   650] training loss: 0.00042572
INFO:root:[72,   700] training loss: 0.00022174
INFO:root:[72,   750] training loss: 0.00063101
INFO:root:[72,   800] training loss: 0.00031377
INFO:root:[72,   850] training loss: 0.00016208
INFO:root:[72,   900] training loss: 0.01433448
INFO:root:[72,   950] training loss: 0.00272408
INFO:root:[72,  1000] training loss: 0.00022466
INFO:root:[72,  1050] training loss: 0.00008895
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6000    1.0000    0.7500         3
    Prophase     0.9671    0.4605    0.6239      1722
           S     0.8114    0.5467    0.6532      1039
   Telophase     0.5385    0.7000    0.6087        10
          G2     0.1487    0.9324    0.2565        74
          G1     0.4390    0.8026    0.5676      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.5845      3872
   macro avg     0.5959    0.7775    0.6086      3872
weighted avg     0.7690    0.5845    0.6103      3872

INFO:root:epoch72
INFO:root:[73,    50] training loss: 0.01307469
INFO:root:[73,   100] training loss: 0.01111222
INFO:root:[73,   150] training loss: 0.01309118
INFO:root:[73,   200] training loss: 0.01096169
INFO:root:[73,   250] training loss: 0.00946615
INFO:root:[73,   300] training loss: 0.01224568
INFO:root:[73,   350] training loss: 0.01176106
INFO:root:[73,   400] training loss: 0.00005794
INFO:root:[73,   450] training loss: 0.00007637
INFO:root:[73,   500] training loss: 0.00029522
INFO:root:[73,   550] training loss: 0.00049314
INFO:root:[73,   600] training loss: 0.00187660
INFO:root:[73,   650] training loss: 0.00034434
INFO:root:[73,   700] training loss: 0.00020462
INFO:root:[73,   750] training loss: 0.00060780
INFO:root:[73,   800] training loss: 0.00029412
INFO:root:[73,   850] training loss: 0.00014540
INFO:root:[73,   900] training loss: 0.01600726
INFO:root:[73,   950] training loss: 0.00358810
INFO:root:[73,  1000] training loss: 0.00014709
INFO:root:[73,  1050] training loss: 0.00006823
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.7500    1.0000    0.8571         3
    Prophase     0.9646    0.4901    0.6500      1722
           S     0.8393    0.5428    0.6593      1039
   Telophase     0.4211    0.8000    0.5517        10
          G2     0.1556    0.8514    0.2630        74
          G1     0.4516    0.8379    0.5869      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.6046      3872
   macro avg     0.6189    0.7889    0.6322      3872
weighted avg     0.7787    0.6046    0.6287      3872

INFO:root:epoch73
INFO:root:[74,    50] training loss: 0.01233517
INFO:root:[74,   100] training loss: 0.01134350
INFO:root:[74,   150] training loss: 0.01159862
INFO:root:[74,   200] training loss: 0.01055224
INFO:root:[74,   250] training loss: 0.00939493
INFO:root:[74,   300] training loss: 0.01212312
INFO:root:[74,   350] training loss: 0.01218077
INFO:root:[74,   400] training loss: 0.00005683
INFO:root:[74,   450] training loss: 0.00004388
INFO:root:[74,   500] training loss: 0.00025539
INFO:root:[74,   550] training loss: 0.00042631
INFO:root:[74,   600] training loss: 0.00143350
INFO:root:[74,   650] training loss: 0.00042429
INFO:root:[74,   700] training loss: 0.00022100
INFO:root:[74,   750] training loss: 0.00063408
INFO:root:[74,   800] training loss: 0.00023065
INFO:root:[74,   850] training loss: 0.00015328
INFO:root:[74,   900] training loss: 0.01284281
INFO:root:[74,   950] training loss: 0.00292869
INFO:root:[74,  1000] training loss: 0.00016529
INFO:root:[74,  1050] training loss: 0.00008141
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9742    0.4826    0.6454      1722
           S     0.8126    0.6593    0.7279      1039
   Telophase     0.3810    0.8000    0.5161        10
          G2     0.1815    0.8243    0.2976        74
          G1     0.4546    0.8065    0.5814      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.6235      3872
   macro avg     0.5672    0.7485    0.5914      3872
weighted avg     0.7767    0.6235    0.6440      3872

INFO:root:epoch74
INFO:root:[75,    50] training loss: 0.01169701
INFO:root:[75,   100] training loss: 0.01088415
INFO:root:[75,   150] training loss: 0.01186146
INFO:root:[75,   200] training loss: 0.01105691
INFO:root:[75,   250] training loss: 0.00999857
INFO:root:[75,   300] training loss: 0.01168698
INFO:root:[75,   350] training loss: 0.01788676
INFO:root:[75,   400] training loss: 0.00039579
INFO:root:[75,   450] training loss: 0.00008961
INFO:root:[75,   500] training loss: 0.00025154
INFO:root:[75,   550] training loss: 0.00057621
INFO:root:[75,   600] training loss: 0.00167899
INFO:root:[75,   650] training loss: 0.00026667
INFO:root:[75,   700] training loss: 0.00017356
INFO:root:[75,   750] training loss: 0.00063066
INFO:root:[75,   800] training loss: 0.00023436
INFO:root:[75,   850] training loss: 0.00013880
INFO:root:[75,   900] training loss: 0.01338317
INFO:root:[75,   950] training loss: 0.00276833
INFO:root:[75,  1000] training loss: 0.00013236
INFO:root:[75,  1050] training loss: 0.00006408
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2500    0.6667    0.3636         3
    Prophase     0.9771    0.3966    0.5642      1722
           S     0.8003    0.5900    0.6792      1039
   Telophase     0.3333    0.9000    0.4865        10
          G2     0.1560    0.8243    0.2624        74
          G1     0.4179    0.8104    0.5515      1018
    Anaphase     0.8571    1.0000    0.9231         6

    accuracy                         0.5679      3872
   macro avg     0.5417    0.7411    0.5472      3872
weighted avg     0.7645    0.5679    0.5862      3872

INFO:root:epoch75
INFO:root:[76,    50] training loss: 0.01248362
INFO:root:[76,   100] training loss: 0.01130868
INFO:root:[76,   150] training loss: 0.01083957
INFO:root:[76,   200] training loss: 0.01423802
INFO:root:[76,   250] training loss: 0.01141104
INFO:root:[76,   300] training loss: 0.01176407
INFO:root:[76,   350] training loss: 0.01058411
INFO:root:[76,   400] training loss: 0.00005454
INFO:root:[76,   450] training loss: 0.00003709
INFO:root:[76,   500] training loss: 0.00044171
INFO:root:[76,   550] training loss: 0.00042485
INFO:root:[76,   600] training loss: 0.00120745
INFO:root:[76,   650] training loss: 0.00026750
INFO:root:[76,   700] training loss: 0.00016801
INFO:root:[76,   750] training loss: 0.00064223
INFO:root:[76,   800] training loss: 0.00018378
INFO:root:[76,   850] training loss: 0.00012010
INFO:root:[76,   900] training loss: 0.01242580
INFO:root:[76,   950] training loss: 0.00269383
INFO:root:[76,  1000] training loss: 0.00020052
INFO:root:[76,  1050] training loss: 0.00008365
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2500    0.3333    0.2857         3
    Prophase     0.9535    0.5476    0.6957      1722
           S     0.8275    0.6323    0.7169      1039
   Telophase     0.3529    0.6000    0.4444        10
          G2     0.1813    0.8649    0.2998        74
          G1     0.4725    0.7917    0.5918      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.6413      3872
   macro avg     0.5292    0.6814    0.5477      3872
weighted avg     0.7759    0.6413    0.6657      3872

INFO:root:epoch76
INFO:root:[77,    50] training loss: 0.01193620
INFO:root:[77,   100] training loss: 0.01211592
INFO:root:[77,   150] training loss: 0.01273626
INFO:root:[77,   200] training loss: 0.01092673
INFO:root:[77,   250] training loss: 0.00955335
INFO:root:[77,   300] training loss: 0.01341139
INFO:root:[77,   350] training loss: 0.01144838
INFO:root:[77,   400] training loss: 0.00002832
INFO:root:[77,   450] training loss: 0.00002696
INFO:root:[77,   500] training loss: 0.00016015
INFO:root:[77,   550] training loss: 0.00041129
INFO:root:[77,   600] training loss: 0.00101825
INFO:root:[77,   650] training loss: 0.00026309
INFO:root:[77,   700] training loss: 0.00014857
INFO:root:[77,   750] training loss: 0.00068588
INFO:root:[77,   800] training loss: 0.00020777
INFO:root:[77,   850] training loss: 0.00014102
INFO:root:[77,   900] training loss: 0.01165579
INFO:root:[77,   950] training loss: 0.00294598
INFO:root:[77,  1000] training loss: 0.00012531
INFO:root:[77,  1050] training loss: 0.00006282
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9658    0.5575    0.7069      1722
           S     0.8226    0.5890    0.6865      1039
   Telophase     0.5556    0.5000    0.5263        10
          G2     0.1641    0.8784    0.2766        74
          G1     0.4848    0.8173    0.6086      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.6410      3872
   macro avg     0.5918    0.7156    0.5946      3872
weighted avg     0.7838    0.6410    0.6670      3872

INFO:root:epoch77
INFO:root:[78,    50] training loss: 0.01293977
INFO:root:[78,   100] training loss: 0.01105804
INFO:root:[78,   150] training loss: 0.01368718
INFO:root:[78,   200] training loss: 0.00971614
INFO:root:[78,   250] training loss: 0.01036906
INFO:root:[78,   300] training loss: 0.01188606
INFO:root:[78,   350] training loss: 0.01011741
INFO:root:[78,   400] training loss: 0.00002967
INFO:root:[78,   450] training loss: 0.00002878
INFO:root:[78,   500] training loss: 0.00016423
INFO:root:[78,   550] training loss: 0.00047017
INFO:root:[78,   600] training loss: 0.00090968
INFO:root:[78,   650] training loss: 0.00023139
INFO:root:[78,   700] training loss: 0.00013313
INFO:root:[78,   750] training loss: 0.00063516
INFO:root:[78,   800] training loss: 0.00020815
INFO:root:[78,   850] training loss: 0.00012606
INFO:root:[78,   900] training loss: 0.01179472
INFO:root:[78,   950] training loss: 0.00252543
INFO:root:[78,  1000] training loss: 0.00014584
INFO:root:[78,  1050] training loss: 0.00005493
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.9511    0.6783    0.7919      1722
           S     0.7444    0.6756    0.7084      1039
   Telophase     0.5000    0.6000    0.5455        10
          G2     0.1769    0.9324    0.2974        74
          G1     0.5373    0.6798    0.6002      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.6831      3872
   macro avg     0.6181    0.7475    0.6382      3872
weighted avg     0.7704    0.6831    0.7090      3872

INFO:root:epoch78
INFO:root:[79,    50] training loss: 0.01199230
INFO:root:[79,   100] training loss: 0.00980228
INFO:root:[79,   150] training loss: 0.01074674
INFO:root:[79,   200] training loss: 0.01570635
INFO:root:[79,   250] training loss: 0.01171825
INFO:root:[79,   300] training loss: 0.01209996
INFO:root:[79,   350] training loss: 0.01129279
INFO:root:[79,   400] training loss: 0.00002309
INFO:root:[79,   450] training loss: 0.00002769
INFO:root:[79,   500] training loss: 0.00022312
INFO:root:[79,   550] training loss: 0.00040912
INFO:root:[79,   600] training loss: 0.00137388
INFO:root:[79,   650] training loss: 0.00021216
INFO:root:[79,   700] training loss: 0.00016065
INFO:root:[79,   750] training loss: 0.00064258
INFO:root:[79,   800] training loss: 0.00018098
INFO:root:[79,   850] training loss: 0.00014200
INFO:root:[79,   900] training loss: 0.01141793
INFO:root:[79,   950] training loss: 0.00226057
INFO:root:[79,  1000] training loss: 0.00012300
INFO:root:[79,  1050] training loss: 0.00006333
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2500    0.6667    0.3636         3
    Prophase     0.9456    0.6864    0.7954      1722
           S     0.8024    0.6487    0.7174      1039
   Telophase     0.5000    0.4000    0.4444        10
          G2     0.1965    0.9054    0.3229        74
          G1     0.5445    0.7574    0.6335      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.6989      3872
   macro avg     0.5579    0.7235    0.5825      3872
weighted avg     0.7853    0.6989    0.7217      3872

INFO:root:epoch79
INFO:root:[80,    50] training loss: 0.01347128
INFO:root:[80,   100] training loss: 0.01262761
INFO:root:[80,   150] training loss: 0.01170030
INFO:root:[80,   200] training loss: 0.01075175
INFO:root:[80,   250] training loss: 0.01013118
INFO:root:[80,   300] training loss: 0.01043843
INFO:root:[80,   350] training loss: 0.01035310
INFO:root:[80,   400] training loss: 0.00003607
INFO:root:[80,   450] training loss: 0.00003226
INFO:root:[80,   500] training loss: 0.00015469
INFO:root:[80,   550] training loss: 0.00033339
INFO:root:[80,   600] training loss: 0.00082005
INFO:root:[80,   650] training loss: 0.00020000
INFO:root:[80,   700] training loss: 0.00012225
INFO:root:[80,   750] training loss: 0.00076534
INFO:root:[80,   800] training loss: 0.00017470
INFO:root:[80,   850] training loss: 0.00009766
INFO:root:[80,   900] training loss: 0.01068914
INFO:root:[80,   950] training loss: 0.00296532
INFO:root:[80,  1000] training loss: 0.00014374
INFO:root:[80,  1050] training loss: 0.00005628
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9377    0.6998    0.8015      1722
           S     0.8185    0.6295    0.7116      1039
   Telophase     0.3913    0.9000    0.5455        10
          G2     0.1818    0.7568    0.2932        74
          G1     0.5475    0.7760    0.6420      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7030      3872
   macro avg     0.5681    0.7755    0.6165      3872
weighted avg     0.7864    0.7030    0.7248      3872

INFO:root:epoch80
INFO:root:[81,    50] training loss: 0.01319499
INFO:root:[81,   100] training loss: 0.01083551
INFO:root:[81,   150] training loss: 0.01156397
INFO:root:[81,   200] training loss: 0.00984214
INFO:root:[81,   250] training loss: 0.00988259
INFO:root:[81,   300] training loss: 0.01141073
INFO:root:[81,   350] training loss: 0.00936035
INFO:root:[81,   400] training loss: 0.00002951
INFO:root:[81,   450] training loss: 0.00002901
INFO:root:[81,   500] training loss: 0.00017548
INFO:root:[81,   550] training loss: 0.00031997
INFO:root:[81,   600] training loss: 0.00094277
INFO:root:[81,   650] training loss: 0.00017039
INFO:root:[81,   700] training loss: 0.00012989
INFO:root:[81,   750] training loss: 0.00060433
INFO:root:[81,   800] training loss: 0.00021631
INFO:root:[81,   850] training loss: 0.00009361
INFO:root:[81,   900] training loss: 0.01291166
INFO:root:[81,   950] training loss: 0.00320939
INFO:root:[81,  1000] training loss: 0.00009096
INFO:root:[81,  1050] training loss: 0.00005439
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9425    0.6475    0.7676      1722
           S     0.8079    0.7084    0.7549      1039
   Telophase     0.4167    0.5000    0.4545        10
          G2     0.2148    0.7838    0.3372        74
          G1     0.5236    0.7623    0.6208      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.6968      3872
   macro avg     0.5675    0.7241    0.6050      3872
weighted avg     0.7801    0.6968    0.7164      3872

INFO:root:epoch81
INFO:root:[82,    50] training loss: 0.01215361
INFO:root:[82,   100] training loss: 0.01012608
INFO:root:[82,   150] training loss: 0.01339898
INFO:root:[82,   200] training loss: 0.01234818
INFO:root:[82,   250] training loss: 0.01315960
INFO:root:[82,   300] training loss: 0.01266412
INFO:root:[82,   350] training loss: 0.01182214
INFO:root:[82,   400] training loss: 0.00006974
INFO:root:[82,   450] training loss: 0.00005151
INFO:root:[82,   500] training loss: 0.00013823
INFO:root:[82,   550] training loss: 0.00033960
INFO:root:[82,   600] training loss: 0.00092639
INFO:root:[82,   650] training loss: 0.00024625
INFO:root:[82,   700] training loss: 0.00011391
INFO:root:[82,   750] training loss: 0.00066679
INFO:root:[82,   800] training loss: 0.00020488
INFO:root:[82,   850] training loss: 0.00011052
INFO:root:[82,   900] training loss: 0.01314294
INFO:root:[82,   950] training loss: 0.00263081
INFO:root:[82,  1000] training loss: 0.00015260
INFO:root:[82,  1050] training loss: 0.00005121
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6000    1.0000    0.7500         3
    Prophase     0.9568    0.6690    0.7874      1722
           S     0.7716    0.7218    0.7459      1039
   Telophase     0.5000    0.5000    0.5000        10
          G2     0.2105    0.8649    0.3386        74
          G1     0.5340    0.7181    0.6125      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.7002      3872
   macro avg     0.6176    0.7820    0.6559      3872
weighted avg     0.7799    0.7002    0.7210      3872

INFO:root:epoch82
INFO:root:[83,    50] training loss: 0.01309445
INFO:root:[83,   100] training loss: 0.01023506
INFO:root:[83,   150] training loss: 0.01141556
INFO:root:[83,   200] training loss: 0.01104150
INFO:root:[83,   250] training loss: 0.01091308
INFO:root:[83,   300] training loss: 0.01126372
INFO:root:[83,   350] training loss: 0.01111442
INFO:root:[83,   400] training loss: 0.00002411
INFO:root:[83,   450] training loss: 0.00003515
INFO:root:[83,   500] training loss: 0.00009569
INFO:root:[83,   550] training loss: 0.00035599
INFO:root:[83,   600] training loss: 0.00072063
INFO:root:[83,   650] training loss: 0.00020301
INFO:root:[83,   700] training loss: 0.00011088
INFO:root:[83,   750] training loss: 0.00061588
INFO:root:[83,   800] training loss: 0.00016178
INFO:root:[83,   850] training loss: 0.00010130
INFO:root:[83,   900] training loss: 0.00972809
INFO:root:[83,   950] training loss: 0.00215318
INFO:root:[83,  1000] training loss: 0.00008916
INFO:root:[83,  1050] training loss: 0.00004816
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9366    0.7207    0.8146      1722
           S     0.7931    0.7045    0.7462      1039
   Telophase     0.4545    0.5000    0.4762        10
          G2     0.2237    0.8919    0.3577        74
          G1     0.5739    0.7358    0.6449      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.7234      3872
   macro avg     0.5903    0.7456    0.6281      3872
weighted avg     0.7872    0.7234    0.7418      3872

INFO:root:epoch83
INFO:root:[84,    50] training loss: 0.01047190
INFO:root:[84,   100] training loss: 0.01020337
INFO:root:[84,   150] training loss: 0.01405926
INFO:root:[84,   200] training loss: 0.01220039
INFO:root:[84,   250] training loss: 0.00966579
INFO:root:[84,   300] training loss: 0.01072633
INFO:root:[84,   350] training loss: 0.01040882
INFO:root:[84,   400] training loss: 0.00003199
INFO:root:[84,   450] training loss: 0.00004517
INFO:root:[84,   500] training loss: 0.00011248
INFO:root:[84,   550] training loss: 0.00032760
INFO:root:[84,   600] training loss: 0.00060899
INFO:root:[84,   650] training loss: 0.00017903
INFO:root:[84,   700] training loss: 0.00008306
INFO:root:[84,   750] training loss: 0.00058044
INFO:root:[84,   800] training loss: 0.00013027
INFO:root:[84,   850] training loss: 0.00009005
INFO:root:[84,   900] training loss: 0.01139269
INFO:root:[84,   950] training loss: 0.00240844
INFO:root:[84,  1000] training loss: 0.00007899
INFO:root:[84,  1050] training loss: 0.00004418
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9382    0.7410    0.8280      1722
           S     0.8231    0.6314    0.7146      1039
   Telophase     0.3750    0.6000    0.4615        10
          G2     0.1700    0.8108    0.2810        74
          G1     0.5848    0.7652    0.6630      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7193      3872
   macro avg     0.5654    0.7450    0.6069      3872
weighted avg     0.7974    0.7193    0.7425      3872

INFO:root:epoch84
INFO:root:[85,    50] training loss: 0.01112211
INFO:root:[85,   100] training loss: 0.00993799
INFO:root:[85,   150] training loss: 0.01173716
INFO:root:[85,   200] training loss: 0.00984274
INFO:root:[85,   250] training loss: 0.01014078
INFO:root:[85,   300] training loss: 0.01054821
INFO:root:[85,   350] training loss: 0.00962404
INFO:root:[85,   400] training loss: 0.00001943
INFO:root:[85,   450] training loss: 0.00002861
INFO:root:[85,   500] training loss: 0.00013050
INFO:root:[85,   550] training loss: 0.00029076
INFO:root:[85,   600] training loss: 0.00058753
INFO:root:[85,   650] training loss: 0.00014367
INFO:root:[85,   700] training loss: 0.00007700
INFO:root:[85,   750] training loss: 0.00053275
INFO:root:[85,   800] training loss: 0.00018030
INFO:root:[85,   850] training loss: 0.00009315
INFO:root:[85,   900] training loss: 0.00923833
INFO:root:[85,   950] training loss: 0.00278406
INFO:root:[85,  1000] training loss: 0.00008696
INFO:root:[85,  1050] training loss: 0.00004390
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2500    0.6667    0.3636         3
    Prophase     0.9419    0.7149    0.8128      1722
           S     0.8319    0.7671    0.7982      1039
   Telophase     0.7143    0.5000    0.5882        10
          G2     0.2762    0.7838    0.4085        74
          G1     0.5681    0.7662    0.6524      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7435      3872
   macro avg     0.6070    0.7427    0.6320      3872
weighted avg     0.7998    0.7435    0.7581      3872

INFO:root:epoch85
INFO:root:[86,    50] training loss: 0.01092855
INFO:root:[86,   100] training loss: 0.00943384
INFO:root:[86,   150] training loss: 0.01163452
INFO:root:[86,   200] training loss: 0.01328993
INFO:root:[86,   250] training loss: 0.01030037
INFO:root:[86,   300] training loss: 0.01123395
INFO:root:[86,   350] training loss: 0.00950526
INFO:root:[86,   400] training loss: 0.00004062
INFO:root:[86,   450] training loss: 0.00003150
INFO:root:[86,   500] training loss: 0.00013584
INFO:root:[86,   550] training loss: 0.00030930
INFO:root:[86,   600] training loss: 0.00063203
INFO:root:[86,   650] training loss: 0.00014062
INFO:root:[86,   700] training loss: 0.00008065
INFO:root:[86,   750] training loss: 0.00052953
INFO:root:[86,   800] training loss: 0.00013483
INFO:root:[86,   850] training loss: 0.00011315
INFO:root:[86,   900] training loss: 0.00951901
INFO:root:[86,   950] training loss: 0.00209411
INFO:root:[86,  1000] training loss: 0.00007222
INFO:root:[86,  1050] training loss: 0.00003758
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.6667    0.6667    0.6667         3
    Prophase     0.9582    0.6521    0.7761      1722
           S     0.8182    0.6843    0.7453      1039
   Telophase     0.5000    0.6000    0.5455        10
          G2     0.2234    0.8243    0.3516        74
          G1     0.5264    0.7937    0.6330      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.7017      3872
   macro avg     0.6347    0.7459    0.6536      3872
weighted avg     0.7913    0.7017    0.7215      3872

INFO:root:epoch86
INFO:root:[87,    50] training loss: 0.01125811
INFO:root:[87,   100] training loss: 0.01100276
INFO:root:[87,   150] training loss: 0.01261310
INFO:root:[87,   200] training loss: 0.01061669
INFO:root:[87,   250] training loss: 0.01305856
INFO:root:[87,   300] training loss: 0.01131150
INFO:root:[87,   350] training loss: 0.00872154
INFO:root:[87,   400] training loss: 0.00003633
INFO:root:[87,   450] training loss: 0.00007158
INFO:root:[87,   500] training loss: 0.00013078
INFO:root:[87,   550] training loss: 0.00036781
INFO:root:[87,   600] training loss: 0.00054339
INFO:root:[87,   650] training loss: 0.00013500
INFO:root:[87,   700] training loss: 0.00008920
INFO:root:[87,   750] training loss: 0.00063831
INFO:root:[87,   800] training loss: 0.00015026
INFO:root:[87,   850] training loss: 0.00008544
INFO:root:[87,   900] training loss: 0.00921900
INFO:root:[87,   950] training loss: 0.00200351
INFO:root:[87,  1000] training loss: 0.00008805
INFO:root:[87,  1050] training loss: 0.00005393
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9221    0.7352    0.8181      1722
           S     0.8217    0.7584    0.7888      1039
   Telophase     0.6667    0.6000    0.6316        10
          G2     0.2596    0.8243    0.3948        74
          G1     0.5802    0.7318    0.6473      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.7423      3872
   macro avg     0.6429    0.7595    0.6727      3872
weighted avg     0.7913    0.7423    0.7566      3872

INFO:root:epoch87
INFO:root:[88,    50] training loss: 0.01108782
INFO:root:[88,   100] training loss: 0.01050269
INFO:root:[88,   150] training loss: 0.01367325
INFO:root:[88,   200] training loss: 0.00975627
INFO:root:[88,   250] training loss: 0.00880436
INFO:root:[88,   300] training loss: 0.01092520
INFO:root:[88,   350] training loss: 0.00992812
INFO:root:[88,   400] training loss: 0.00002127
INFO:root:[88,   450] training loss: 0.00003649
INFO:root:[88,   500] training loss: 0.00010414
INFO:root:[88,   550] training loss: 0.00039211
INFO:root:[88,   600] training loss: 0.00070219
INFO:root:[88,   650] training loss: 0.00017700
INFO:root:[88,   700] training loss: 0.00007178
INFO:root:[88,   750] training loss: 0.00063569
INFO:root:[88,   800] training loss: 0.00012172
INFO:root:[88,   850] training loss: 0.00007596
INFO:root:[88,   900] training loss: 0.00937584
INFO:root:[88,   950] training loss: 0.00221070
INFO:root:[88,  1000] training loss: 0.00007451
INFO:root:[88,  1050] training loss: 0.00004010
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9486    0.7079    0.8108      1722
           S     0.7830    0.7257    0.7532      1039
   Telophase     0.4667    0.7000    0.5600        10
          G2     0.2138    0.8378    0.3407        74
          G1     0.5669    0.7279    0.6374      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.7208      3872
   macro avg     0.6041    0.7666    0.6472      3872
weighted avg     0.7879    0.7208    0.7400      3872

INFO:root:epoch88
INFO:root:[89,    50] training loss: 0.01076902
INFO:root:[89,   100] training loss: 0.01078780
INFO:root:[89,   150] training loss: 0.01082116
INFO:root:[89,   200] training loss: 0.00985896
INFO:root:[89,   250] training loss: 0.00948100
INFO:root:[89,   300] training loss: 0.01176711
INFO:root:[89,   350] training loss: 0.00996976
INFO:root:[89,   400] training loss: 0.00002069
INFO:root:[89,   450] training loss: 0.00002225
INFO:root:[89,   500] training loss: 0.00016778
INFO:root:[89,   550] training loss: 0.00026476
INFO:root:[89,   600] training loss: 0.00055674
INFO:root:[89,   650] training loss: 0.00015650
INFO:root:[89,   700] training loss: 0.00007701
INFO:root:[89,   750] training loss: 0.00062901
INFO:root:[89,   800] training loss: 0.00012492
INFO:root:[89,   850] training loss: 0.00009122
INFO:root:[89,   900] training loss: 0.01022804
INFO:root:[89,   950] training loss: 0.00246874
INFO:root:[89,  1000] training loss: 0.00010240
INFO:root:[89,  1050] training loss: 0.00004004
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    1.0000    0.6667         3
    Prophase     0.9353    0.7642    0.8412      1722
           S     0.8305    0.7401    0.7827      1039
   Telophase     0.3684    0.7000    0.4828        10
          G2     0.2748    0.8243    0.4122        74
          G1     0.6016    0.7564    0.6701      1018
    Anaphase     0.5000    1.0000    0.6667         6

    accuracy                         0.7572      3872
   macro avg     0.5729    0.8264    0.6460      3872
weighted avg     0.8043    0.7572    0.7710      3872

INFO:root:epoch89
INFO:root:[90,    50] training loss: 0.01083016
INFO:root:[90,   100] training loss: 0.00989385
INFO:root:[90,   150] training loss: 0.01048521
INFO:root:[90,   200] training loss: 0.01028476
INFO:root:[90,   250] training loss: 0.00874912
INFO:root:[90,   300] training loss: 0.01003373
INFO:root:[90,   350] training loss: 0.00931011
INFO:root:[90,   400] training loss: 0.00003516
INFO:root:[90,   450] training loss: 0.00001642
INFO:root:[90,   500] training loss: 0.00007563
INFO:root:[90,   550] training loss: 0.00023179
INFO:root:[90,   600] training loss: 0.00045000
INFO:root:[90,   650] training loss: 0.00011562
INFO:root:[90,   700] training loss: 0.00008637
INFO:root:[90,   750] training loss: 0.00070845
INFO:root:[90,   800] training loss: 0.00011988
INFO:root:[90,   850] training loss: 0.00010714
INFO:root:[90,   900] training loss: 0.00930853
INFO:root:[90,   950] training loss: 0.00212933
INFO:root:[90,  1000] training loss: 0.00007489
INFO:root:[90,  1050] training loss: 0.00003815
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9566    0.6527    0.7760      1722
           S     0.7724    0.7969    0.7845      1039
   Telophase     0.6000    0.6000    0.6000        10
          G2     0.2593    0.7568    0.3862        74
          G1     0.5256    0.7151    0.6059      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7102      3872
   macro avg     0.6020    0.7412    0.6391      3872
weighted avg     0.7787    0.7102    0.7254      3872

INFO:root:epoch90
INFO:root:[91,    50] training loss: 0.01067333
INFO:root:[91,   100] training loss: 0.00949118
INFO:root:[91,   150] training loss: 0.01026044
INFO:root:[91,   200] training loss: 0.00962408
INFO:root:[91,   250] training loss: 0.00906475
INFO:root:[91,   300] training loss: 0.00971437
INFO:root:[91,   350] training loss: 0.01011397
INFO:root:[91,   400] training loss: 0.00002117
INFO:root:[91,   450] training loss: 0.00002148
INFO:root:[91,   500] training loss: 0.00009954
INFO:root:[91,   550] training loss: 0.00027673
INFO:root:[91,   600] training loss: 0.00058793
INFO:root:[91,   650] training loss: 0.00009163
INFO:root:[91,   700] training loss: 0.00006821
INFO:root:[91,   750] training loss: 0.00063750
INFO:root:[91,   800] training loss: 0.00012371
INFO:root:[91,   850] training loss: 0.00008989
INFO:root:[91,   900] training loss: 0.00809838
INFO:root:[91,   950] training loss: 0.00169969
INFO:root:[91,  1000] training loss: 0.00005977
INFO:root:[91,  1050] training loss: 0.00003025
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2500    0.3333    0.2857         3
    Prophase     0.9414    0.7364    0.8263      1722
           S     0.7914    0.7632    0.7771      1039
   Telophase     0.5556    0.5000    0.5263        10
          G2     0.2458    0.7973    0.3758        74
          G1     0.5825    0.7210    0.6444      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7402      3872
   macro avg     0.5667    0.6930    0.5979      3872
weighted avg     0.7914    0.7402    0.7554      3872

INFO:root:epoch91
INFO:root:[92,    50] training loss: 0.00974154
INFO:root:[92,   100] training loss: 0.01050415
INFO:root:[92,   150] training loss: 0.01288985
INFO:root:[92,   200] training loss: 0.01876450
INFO:root:[92,   250] training loss: 0.01294315
INFO:root:[92,   300] training loss: 0.01205856
INFO:root:[92,   350] training loss: 0.01039842
INFO:root:[92,   400] training loss: 0.00007586
INFO:root:[92,   450] training loss: 0.00003108
INFO:root:[92,   500] training loss: 0.00008025
INFO:root:[92,   550] training loss: 0.00034607
INFO:root:[92,   600] training loss: 0.00046925
INFO:root:[92,   650] training loss: 0.00017093
INFO:root:[92,   700] training loss: 0.00006360
INFO:root:[92,   750] training loss: 0.00057604
INFO:root:[92,   800] training loss: 0.00012330
INFO:root:[92,   850] training loss: 0.00009004
INFO:root:[92,   900] training loss: 0.00912109
INFO:root:[92,   950] training loss: 0.00219119
INFO:root:[92,  1000] training loss: 0.00009496
INFO:root:[92,  1050] training loss: 0.00002998
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9498    0.6266    0.7551      1722
           S     0.8627    0.6593    0.7474      1039
   Telophase     0.4000    0.4000    0.4000        10
          G2     0.2410    0.8108    0.3715        74
          G1     0.5144    0.8428    0.6389      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.6958      3872
   macro avg     0.5668    0.7152    0.5947      3872
weighted avg     0.7960    0.6958    0.7140      3872

INFO:root:epoch92
INFO:root:[93,    50] training loss: 0.01435034
INFO:root:[93,   100] training loss: 0.01023344
INFO:root:[93,   150] training loss: 0.01314826
INFO:root:[93,   200] training loss: 0.01045259
INFO:root:[93,   250] training loss: 0.00971651
INFO:root:[93,   300] training loss: 0.01129598
INFO:root:[93,   350] training loss: 0.00904907
INFO:root:[93,   400] training loss: 0.00004802
INFO:root:[93,   450] training loss: 0.00004120
INFO:root:[93,   500] training loss: 0.00013023
INFO:root:[93,   550] training loss: 0.00033301
INFO:root:[93,   600] training loss: 0.00036019
INFO:root:[93,   650] training loss: 0.00012134
INFO:root:[93,   700] training loss: 0.00010013
INFO:root:[93,   750] training loss: 0.00061106
INFO:root:[93,   800] training loss: 0.00009160
INFO:root:[93,   850] training loss: 0.00010048
INFO:root:[93,   900] training loss: 0.00881336
INFO:root:[93,   950] training loss: 0.00222061
INFO:root:[93,  1000] training loss: 0.00007684
INFO:root:[93,  1050] training loss: 0.00003295
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9255    0.7642    0.8372      1722
           S     0.8040    0.7421    0.7718      1039
   Telophase     0.5333    0.8000    0.6400        10
          G2     0.2407    0.7838    0.3683        74
          G1     0.5947    0.7122    0.6482      1018
    Anaphase     0.5455    1.0000    0.7059         6

    accuracy                         0.7454      3872
   macro avg     0.5777    0.7813    0.6387      3872
weighted avg     0.7908    0.7454    0.7600      3872

INFO:root:epoch93
INFO:root:[94,    50] training loss: 0.01027891
INFO:root:[94,   100] training loss: 0.00990241
INFO:root:[94,   150] training loss: 0.01048796
INFO:root:[94,   200] training loss: 0.00888670
INFO:root:[94,   250] training loss: 0.00839483
INFO:root:[94,   300] training loss: 0.00948564
INFO:root:[94,   350] training loss: 0.00931931
INFO:root:[94,   400] training loss: 0.00003333
INFO:root:[94,   450] training loss: 0.00002854
INFO:root:[94,   500] training loss: 0.00009240
INFO:root:[94,   550] training loss: 0.00037848
INFO:root:[94,   600] training loss: 0.00050058
INFO:root:[94,   650] training loss: 0.00008738
INFO:root:[94,   700] training loss: 0.00004963
INFO:root:[94,   750] training loss: 0.00075683
INFO:root:[94,   800] training loss: 0.00011734
INFO:root:[94,   850] training loss: 0.00012751
INFO:root:[94,   900] training loss: 0.00975572
INFO:root:[94,   950] training loss: 0.00276655
INFO:root:[94,  1000] training loss: 0.00008960
INFO:root:[94,  1050] training loss: 0.00003388
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9419    0.7538    0.8374      1722
           S     0.7638    0.6814    0.7202      1039
   Telophase     0.4375    0.7000    0.5385        10
          G2     0.1826    0.8243    0.2990        74
          G1     0.5885    0.6955    0.6376      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7206      3872
   macro avg     0.5735    0.7602    0.6220      3872
weighted avg     0.7845    0.7206    0.7420      3872

INFO:root:epoch94
INFO:root:[95,    50] training loss: 0.01146328
INFO:root:[95,   100] training loss: 0.01084039
INFO:root:[95,   150] training loss: 0.01112316
INFO:root:[95,   200] training loss: 0.01095099
INFO:root:[95,   250] training loss: 0.00904807
INFO:root:[95,   300] training loss: 0.01140431
INFO:root:[95,   350] training loss: 0.00921113
INFO:root:[95,   400] training loss: 0.00002359
INFO:root:[95,   450] training loss: 0.00001657
INFO:root:[95,   500] training loss: 0.00013538
INFO:root:[95,   550] training loss: 0.00023890
INFO:root:[95,   600] training loss: 0.00043337
INFO:root:[95,   650] training loss: 0.00013118
INFO:root:[95,   700] training loss: 0.00007174
INFO:root:[95,   750] training loss: 0.00053698
INFO:root:[95,   800] training loss: 0.00011205
INFO:root:[95,   850] training loss: 0.00005413
INFO:root:[95,   900] training loss: 0.00779153
INFO:root:[95,   950] training loss: 0.00193924
INFO:root:[95,  1000] training loss: 0.00006605
INFO:root:[95,  1050] training loss: 0.00002882
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9297    0.8060    0.8635      1722
           S     0.8213    0.7036    0.7579      1039
   Telophase     0.5385    0.7000    0.6087        10
          G2     0.2302    0.7838    0.3558        74
          G1     0.6286    0.7466    0.6825      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7624      3872
   macro avg     0.5926    0.7724    0.6455      3872
weighted avg     0.8062    0.7624    0.7767      3872

INFO:root:epoch95
INFO:root:[96,    50] training loss: 0.00992557
INFO:root:[96,   100] training loss: 0.00928317
INFO:root:[96,   150] training loss: 0.00993234
INFO:root:[96,   200] training loss: 0.00797312
INFO:root:[96,   250] training loss: 0.00873723
INFO:root:[96,   300] training loss: 0.01038559
INFO:root:[96,   350] training loss: 0.01093118
INFO:root:[96,   400] training loss: 0.00001484
INFO:root:[96,   450] training loss: 0.00001823
INFO:root:[96,   500] training loss: 0.00021531
INFO:root:[96,   550] training loss: 0.00028342
INFO:root:[96,   600] training loss: 0.00030652
INFO:root:[96,   650] training loss: 0.00008535
INFO:root:[96,   700] training loss: 0.00005243
INFO:root:[96,   750] training loss: 0.00070199
INFO:root:[96,   800] training loss: 0.00007280
INFO:root:[96,   850] training loss: 0.00007354
INFO:root:[96,   900] training loss: 0.00786250
INFO:root:[96,   950] training loss: 0.00198358
INFO:root:[96,  1000] training loss: 0.00009395
INFO:root:[96,  1050] training loss: 0.00005415
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9474    0.7323    0.8261      1722
           S     0.7864    0.8114    0.7987      1039
   Telophase     0.4000    0.4000    0.4000        10
          G2     0.3418    0.7297    0.4655        74
          G1     0.5785    0.7308    0.6458      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7526      3872
   macro avg     0.5792    0.7244    0.6266      3872
weighted avg     0.7933    0.7526    0.7630      3872

INFO:root:epoch96
INFO:root:[97,    50] training loss: 0.01093653
INFO:root:[97,   100] training loss: 0.00961315
INFO:root:[97,   150] training loss: 0.01167230
INFO:root:[97,   200] training loss: 0.00902954
INFO:root:[97,   250] training loss: 0.00843756
INFO:root:[97,   300] training loss: 0.01049045
INFO:root:[97,   350] training loss: 0.00854014
INFO:root:[97,   400] training loss: 0.00002448
INFO:root:[97,   450] training loss: 0.00004225
INFO:root:[97,   500] training loss: 0.00006244
INFO:root:[97,   550] training loss: 0.00021612
INFO:root:[97,   600] training loss: 0.00030371
INFO:root:[97,   650] training loss: 0.00007702
INFO:root:[97,   700] training loss: 0.00004880
INFO:root:[97,   750] training loss: 0.00065621
INFO:root:[97,   800] training loss: 0.00008087
INFO:root:[97,   850] training loss: 0.00004599
INFO:root:[97,   900] training loss: 0.01124238
INFO:root:[97,   950] training loss: 0.00205656
INFO:root:[97,  1000] training loss: 0.00006920
INFO:root:[97,  1050] training loss: 0.00003658
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    1.0000    0.6667         3
    Prophase     0.9393    0.7096    0.8085      1722
           S     0.8123    0.7247    0.7660      1039
   Telophase     0.4286    0.6000    0.5000        10
          G2     0.2231    0.7297    0.3418        74
          G1     0.5615    0.7574    0.6449      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7270      3872
   macro avg     0.5902    0.7888    0.6468      3872
weighted avg     0.7901    0.7270    0.7442      3872

INFO:root:epoch97
INFO:root:[98,    50] training loss: 0.01051697
INFO:root:[98,   100] training loss: 0.00906086
INFO:root:[98,   150] training loss: 0.01040986
INFO:root:[98,   200] training loss: 0.00854702
INFO:root:[98,   250] training loss: 0.00870917
INFO:root:[98,   300] training loss: 0.01003139
INFO:root:[98,   350] training loss: 0.00795304
INFO:root:[98,   400] training loss: 0.00001267
INFO:root:[98,   450] training loss: 0.00001308
INFO:root:[98,   500] training loss: 0.00005746
INFO:root:[98,   550] training loss: 0.00022330
INFO:root:[98,   600] training loss: 0.00026894
INFO:root:[98,   650] training loss: 0.00010417
INFO:root:[98,   700] training loss: 0.00007200
INFO:root:[98,   750] training loss: 0.00061260
INFO:root:[98,   800] training loss: 0.00012709
INFO:root:[98,   850] training loss: 0.00009276
INFO:root:[98,   900] training loss: 0.00688479
INFO:root:[98,   950] training loss: 0.00173693
INFO:root:[98,  1000] training loss: 0.00004930
INFO:root:[98,  1050] training loss: 0.00002502
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.8846    0.8368    0.8600      1722
           S     0.8114    0.7950    0.8031      1039
   Telophase     0.6667    0.6000    0.6316        10
          G2     0.3146    0.7568    0.4444        74
          G1     0.6360    0.6385    0.6373      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7714      3872
   macro avg     0.6067    0.7562    0.6530      3872
weighted avg     0.7873    0.7714    0.7772      3872

INFO:root:epoch98
INFO:root:[99,    50] training loss: 0.00983400
INFO:root:[99,   100] training loss: 0.01066545
INFO:root:[99,   150] training loss: 0.01223756
INFO:root:[99,   200] training loss: 0.01130938
INFO:root:[99,   250] training loss: 0.01072224
INFO:root:[99,   300] training loss: 0.01034170
INFO:root:[99,   350] training loss: 0.00860636
INFO:root:[99,   400] training loss: 0.00001497
INFO:root:[99,   450] training loss: 0.00002058
INFO:root:[99,   500] training loss: 0.00006979
INFO:root:[99,   550] training loss: 0.00029281
INFO:root:[99,   600] training loss: 0.00057594
INFO:root:[99,   650] training loss: 0.00008085
INFO:root:[99,   700] training loss: 0.00003795
INFO:root:[99,   750] training loss: 0.00072720
INFO:root:[99,   800] training loss: 0.00010059
INFO:root:[99,   850] training loss: 0.00005667
INFO:root:[99,   900] training loss: 0.00777154
INFO:root:[99,   950] training loss: 0.00199547
INFO:root:[99,  1000] training loss: 0.00006950
INFO:root:[99,  1050] training loss: 0.00003079
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2000    0.3333    0.2500         3
    Prophase     0.8952    0.8089    0.8499      1722
           S     0.7802    0.7825    0.7814      1039
   Telophase     0.6667    0.6000    0.6316        10
          G2     0.2589    0.7838    0.3893        74
          G1     0.6209    0.6257    0.6233      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7526      3872
   macro avg     0.5746    0.7049    0.6108      3872
weighted avg     0.7785    0.7526    0.7619      3872

INFO:root:epoch99
INFO:root:[100,    50] training loss: 0.01060244
INFO:root:[100,   100] training loss: 0.00934716
INFO:root:[100,   150] training loss: 0.00968725
INFO:root:[100,   200] training loss: 0.00840489
INFO:root:[100,   250] training loss: 0.00816406
INFO:root:[100,   300] training loss: 0.00932947
INFO:root:[100,   350] training loss: 0.00825769
INFO:root:[100,   400] training loss: 0.00001093
INFO:root:[100,   450] training loss: 0.00001911
INFO:root:[100,   500] training loss: 0.00013655
INFO:root:[100,   550] training loss: 0.00020767
INFO:root:[100,   600] training loss: 0.00035066
INFO:root:[100,   650] training loss: 0.00007718
INFO:root:[100,   700] training loss: 0.00006990
INFO:root:[100,   750] training loss: 0.00052535
INFO:root:[100,   800] training loss: 0.00009883
INFO:root:[100,   850] training loss: 0.00010553
INFO:root:[100,   900] training loss: 0.00824950
INFO:root:[100,   950] training loss: 0.00182104
INFO:root:[100,  1000] training loss: 0.00006429
INFO:root:[100,  1050] training loss: 0.00002894
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2857    0.6667    0.4000         3
    Prophase     0.9240    0.6568    0.7678      1722
           S     0.8232    0.7392    0.7789      1039
   Telophase     0.5556    0.5000    0.5263        10
          G2     0.2423    0.7432    0.3654        74
          G1     0.5236    0.7525    0.6175      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7058      3872
   macro avg     0.5744    0.7226    0.6080      3872
weighted avg     0.7768    0.7058    0.7227      3872

INFO:root:epoch100
INFO:root:[101,    50] training loss: 0.01113194
INFO:root:[101,   100] training loss: 0.00896835
INFO:root:[101,   150] training loss: 0.00992918
INFO:root:[101,   200] training loss: 0.00797201
INFO:root:[101,   250] training loss: 0.00756601
INFO:root:[101,   300] training loss: 0.00929311
INFO:root:[101,   350] training loss: 0.00803258
INFO:root:[101,   400] training loss: 0.00002308
INFO:root:[101,   450] training loss: 0.00003673
INFO:root:[101,   500] training loss: 0.00004281
INFO:root:[101,   550] training loss: 0.00045776
INFO:root:[101,   600] training loss: 0.00029099
INFO:root:[101,   650] training loss: 0.00016012
INFO:root:[101,   700] training loss: 0.00013929
INFO:root:[101,   750] training loss: 0.00192305
INFO:root:[101,   800] training loss: 0.00132079
INFO:root:[101,   850] training loss: 0.00065073
INFO:root:[101,   900] training loss: 0.00750256
INFO:root:[101,   950] training loss: 0.00167105
INFO:root:[101,  1000] training loss: 0.00009540
INFO:root:[101,  1050] training loss: 0.00005852
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.8470    0.8711    0.8589      1722
           S     0.8333    0.7507    0.7899      1039
   Telophase     0.5556    0.5000    0.5263        10
          G2     0.3630    0.7162    0.4818        74
          G1     0.6209    0.6081    0.6144      1018
    Anaphase     0.7500    1.0000    0.8571         6

    accuracy                         0.7658      3872
   macro avg     0.6242    0.7304    0.6612      3872
weighted avg     0.7734    0.7658    0.7677      3872

INFO:root:epoch101
INFO:root:[102,    50] training loss: 0.01101201
INFO:root:[102,   100] training loss: 0.00860894
INFO:root:[102,   150] training loss: 0.00904458
INFO:root:[102,   200] training loss: 0.00740486
INFO:root:[102,   250] training loss: 0.00777853
INFO:root:[102,   300] training loss: 0.00873103
INFO:root:[102,   350] training loss: 0.00986598
INFO:root:[102,   400] training loss: 0.00001737
INFO:root:[102,   450] training loss: 0.00001922
INFO:root:[102,   500] training loss: 0.00003692
INFO:root:[102,   550] training loss: 0.00037564
INFO:root:[102,   600] training loss: 0.00026658
INFO:root:[102,   650] training loss: 0.00011003
INFO:root:[102,   700] training loss: 0.00005481
INFO:root:[102,   750] training loss: 0.00133384
INFO:root:[102,   800] training loss: 0.00090988
INFO:root:[102,   850] training loss: 0.00068770
INFO:root:[102,   900] training loss: 0.00906154
INFO:root:[102,   950] training loss: 0.00154495
INFO:root:[102,  1000] training loss: 0.00007810
INFO:root:[102,  1050] training loss: 0.00005157
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.1429    0.3333    0.2000         3
    Prophase     0.8605    0.8630    0.8617      1722
           S     0.8377    0.7652    0.7998      1039
   Telophase     0.6000    0.6000    0.6000        10
          G2     0.3926    0.7162    0.5072        74
          G1     0.6219    0.6316    0.6267      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7722      3872
   macro avg     0.5794    0.7013    0.6208      3872
weighted avg     0.7810    0.7722    0.7752      3872

INFO:root:epoch102
INFO:root:[103,    50] training loss: 0.01017897
INFO:root:[103,   100] training loss: 0.00842739
INFO:root:[103,   150] training loss: 0.00911480
INFO:root:[103,   200] training loss: 0.00769118
INFO:root:[103,   250] training loss: 0.00787851
INFO:root:[103,   300] training loss: 0.00872923
INFO:root:[103,   350] training loss: 0.00704806
INFO:root:[103,   400] training loss: 0.00004363
INFO:root:[103,   450] training loss: 0.00002484
INFO:root:[103,   500] training loss: 0.00004635
INFO:root:[103,   550] training loss: 0.00045223
INFO:root:[103,   600] training loss: 0.00019438
INFO:root:[103,   650] training loss: 0.00009669
INFO:root:[103,   700] training loss: 0.00005361
INFO:root:[103,   750] training loss: 0.00116963
INFO:root:[103,   800] training loss: 0.00079093
INFO:root:[103,   850] training loss: 0.00045014
INFO:root:[103,   900] training loss: 0.00672100
INFO:root:[103,   950] training loss: 0.00148121
INFO:root:[103,  1000] training loss: 0.00007108
INFO:root:[103,  1050] training loss: 0.00005144
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.2000    0.3333    0.2500         3
    Prophase     0.8696    0.8595    0.8645      1722
           S     0.8296    0.7777    0.8028      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.3985    0.7162    0.5121        74
          G1     0.6287    0.6405    0.6345      1018
    Anaphase     0.6000    1.0000    0.7500         6

    accuracy                         0.7763      3872
   macro avg     0.5817    0.7039    0.6265      3872
weighted avg     0.7847    0.7763    0.7793      3872

INFO:root:epoch103
INFO:root:[104,    50] training loss: 0.01010711
INFO:root:[104,   100] training loss: 0.00808235
INFO:root:[104,   150] training loss: 0.00844164
INFO:root:[104,   200] training loss: 0.00724207
INFO:root:[104,   250] training loss: 0.00673767
INFO:root:[104,   300] training loss: 0.00827696
INFO:root:[104,   350] training loss: 0.00707021
INFO:root:[104,   400] training loss: 0.00002582
INFO:root:[104,   450] training loss: 0.00002189
INFO:root:[104,   500] training loss: 0.00003454
INFO:root:[104,   550] training loss: 0.00042864
INFO:root:[104,   600] training loss: 0.00017993
INFO:root:[104,   650] training loss: 0.00004740
INFO:root:[104,   700] training loss: 0.00007548
INFO:root:[104,   750] training loss: 0.00095215
INFO:root:[104,   800] training loss: 0.00064547
INFO:root:[104,   850] training loss: 0.00045483
INFO:root:[104,   900] training loss: 0.00646319
INFO:root:[104,   950] training loss: 0.00135668
INFO:root:[104,  1000] training loss: 0.00007968
INFO:root:[104,  1050] training loss: 0.00004778
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.8599    0.8659    0.8628      1722
           S     0.8417    0.7575    0.7974      1039
   Telophase     0.5556    0.5000    0.5263        10
          G2     0.3885    0.7297    0.5070        74
          G1     0.6250    0.6385    0.6317      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7735      3872
   macro avg     0.6101    0.7369    0.6528      3872
weighted avg     0.7827    0.7735    0.7764      3872

INFO:root:epoch104
INFO:root:[105,    50] training loss: 0.01137155
INFO:root:[105,   100] training loss: 0.00827986
INFO:root:[105,   150] training loss: 0.00872663
INFO:root:[105,   200] training loss: 0.00703206
INFO:root:[105,   250] training loss: 0.00648373
INFO:root:[105,   300] training loss: 0.00898837
INFO:root:[105,   350] training loss: 0.00702562
INFO:root:[105,   400] training loss: 0.00002404
INFO:root:[105,   450] training loss: 0.00001723
INFO:root:[105,   500] training loss: 0.00003381
INFO:root:[105,   550] training loss: 0.00037283
INFO:root:[105,   600] training loss: 0.00016785
INFO:root:[105,   650] training loss: 0.00004840
INFO:root:[105,   700] training loss: 0.00003391
INFO:root:[105,   750] training loss: 0.00084843
INFO:root:[105,   800] training loss: 0.00062251
INFO:root:[105,   850] training loss: 0.00043619
INFO:root:[105,   900] training loss: 0.00672307
INFO:root:[105,   950] training loss: 0.00161366
INFO:root:[105,  1000] training loss: 0.00006467
INFO:root:[105,  1050] training loss: 0.00003476
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.8744    0.8734    0.8739      1722
           S     0.8435    0.7883    0.8149      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4167    0.7432    0.5340        74
          G1     0.6520    0.6552    0.6536      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7900      3872
   macro avg     0.6189    0.7610    0.6703      3872
weighted avg     0.7973    0.7900    0.7924      3872

INFO:root:epoch105
INFO:root:[106,    50] training loss: 0.00926071
INFO:root:[106,   100] training loss: 0.00779773
INFO:root:[106,   150] training loss: 0.00770952
INFO:root:[106,   200] training loss: 0.00772768
INFO:root:[106,   250] training loss: 0.00697938
INFO:root:[106,   300] training loss: 0.00812659
INFO:root:[106,   350] training loss: 0.00694565
INFO:root:[106,   400] training loss: 0.00001681
INFO:root:[106,   450] training loss: 0.00007405
INFO:root:[106,   500] training loss: 0.00003267
INFO:root:[106,   550] training loss: 0.00030098
INFO:root:[106,   600] training loss: 0.00011488
INFO:root:[106,   650] training loss: 0.00013006
INFO:root:[106,   700] training loss: 0.00003840
INFO:root:[106,   750] training loss: 0.00084982
INFO:root:[106,   800] training loss: 0.00061150
INFO:root:[106,   850] training loss: 0.00046865
INFO:root:[106,   900] training loss: 0.00613314
INFO:root:[106,   950] training loss: 0.00131004
INFO:root:[106,  1000] training loss: 0.00005893
INFO:root:[106,  1050] training loss: 0.00005151
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.8821    0.8426    0.8619      1722
           S     0.8413    0.7652    0.8014      1039
   Telophase     0.6667    0.6000    0.6316        10
          G2     0.3873    0.7432    0.5093        74
          G1     0.6210    0.6807    0.6495      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7769      3872
   macro avg     0.6283    0.7569    0.6712      3872
weighted avg     0.7917    0.7769    0.7821      3872

INFO:root:epoch106
INFO:root:[107,    50] training loss: 0.01027155
INFO:root:[107,   100] training loss: 0.00819580
INFO:root:[107,   150] training loss: 0.00798889
INFO:root:[107,   200] training loss: 0.00794990
INFO:root:[107,   250] training loss: 0.00719159
INFO:root:[107,   300] training loss: 0.00813018
INFO:root:[107,   350] training loss: 0.00696556
INFO:root:[107,   400] training loss: 0.00001915
INFO:root:[107,   450] training loss: 0.00001931
INFO:root:[107,   500] training loss: 0.00003267
INFO:root:[107,   550] training loss: 0.00044447
INFO:root:[107,   600] training loss: 0.00017509
INFO:root:[107,   650] training loss: 0.00005201
INFO:root:[107,   700] training loss: 0.00003651
INFO:root:[107,   750] training loss: 0.00083356
INFO:root:[107,   800] training loss: 0.00049955
INFO:root:[107,   850] training loss: 0.00043170
INFO:root:[107,   900] training loss: 0.00621288
INFO:root:[107,   950] training loss: 0.00195863
INFO:root:[107,  1000] training loss: 0.00005318
INFO:root:[107,  1050] training loss: 0.00003681
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.8821    0.8560    0.8688      1722
           S     0.8383    0.7834    0.8100      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.3885    0.7297    0.5070        74
          G1     0.6413    0.6709    0.6558      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7849      3872
   macro avg     0.6137    0.7581    0.6654      3872
weighted avg     0.7960    0.7849    0.7889      3872

INFO:root:epoch107
INFO:root:[108,    50] training loss: 0.00931667
INFO:root:[108,   100] training loss: 0.00776211
INFO:root:[108,   150] training loss: 0.00849931
INFO:root:[108,   200] training loss: 0.00742118
INFO:root:[108,   250] training loss: 0.00728069
INFO:root:[108,   300] training loss: 0.00852387
INFO:root:[108,   350] training loss: 0.00702946
INFO:root:[108,   400] training loss: 0.00012071
INFO:root:[108,   450] training loss: 0.00001366
INFO:root:[108,   500] training loss: 0.00003638
INFO:root:[108,   550] training loss: 0.00033559
INFO:root:[108,   600] training loss: 0.00015240
INFO:root:[108,   650] training loss: 0.00002752
INFO:root:[108,   700] training loss: 0.00003080
INFO:root:[108,   750] training loss: 0.00055851
INFO:root:[108,   800] training loss: 0.00052600
INFO:root:[108,   850] training loss: 0.00033539
INFO:root:[108,   900] training loss: 0.00556279
INFO:root:[108,   950] training loss: 0.00155311
INFO:root:[108,  1000] training loss: 0.00005728
INFO:root:[108,  1050] training loss: 0.00005834
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.8743    0.8600    0.8671      1722
           S     0.8471    0.7680    0.8057      1039
   Telophase     0.5556    0.5000    0.5263        10
          G2     0.3901    0.7432    0.5116        74
          G1     0.6359    0.6690    0.6520      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7820      3872
   macro avg     0.6147    0.7439    0.6582      3872
weighted avg     0.7935    0.7820    0.7859      3872

INFO:root:epoch108
INFO:root:[109,    50] training loss: 0.01001657
INFO:root:[109,   100] training loss: 0.00899174
INFO:root:[109,   150] training loss: 0.00771474
INFO:root:[109,   200] training loss: 0.00705320
INFO:root:[109,   250] training loss: 0.00646403
INFO:root:[109,   300] training loss: 0.00839314
INFO:root:[109,   350] training loss: 0.00692436
INFO:root:[109,   400] training loss: 0.00001621
INFO:root:[109,   450] training loss: 0.00001582
INFO:root:[109,   500] training loss: 0.00003669
INFO:root:[109,   550] training loss: 0.00033051
INFO:root:[109,   600] training loss: 0.00016018
INFO:root:[109,   650] training loss: 0.00002943
INFO:root:[109,   700] training loss: 0.00002443
INFO:root:[109,   750] training loss: 0.00056573
INFO:root:[109,   800] training loss: 0.00035634
INFO:root:[109,   850] training loss: 0.00036353
INFO:root:[109,   900] training loss: 0.00586475
INFO:root:[109,   950] training loss: 0.00152533
INFO:root:[109,  1000] training loss: 0.00004745
INFO:root:[109,  1050] training loss: 0.00003557
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.8851    0.8548    0.8697      1722
           S     0.8499    0.7796    0.8133      1039
   Telophase     0.5556    0.5000    0.5263        10
          G2     0.4161    0.7703    0.5403        74
          G1     0.6384    0.6866    0.6616      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7880      3872
   macro avg     0.6207    0.7511    0.6651      3872
weighted avg     0.8002    0.7880    0.7922      3872

INFO:root:epoch109
INFO:root:[110,    50] training loss: 0.00897505
INFO:root:[110,   100] training loss: 0.00835762
INFO:root:[110,   150] training loss: 0.00852222
INFO:root:[110,   200] training loss: 0.00698126
INFO:root:[110,   250] training loss: 0.00651504
INFO:root:[110,   300] training loss: 0.00802308
INFO:root:[110,   350] training loss: 0.00637778
INFO:root:[110,   400] training loss: 0.00001901
INFO:root:[110,   450] training loss: 0.00001970
INFO:root:[110,   500] training loss: 0.00003602
INFO:root:[110,   550] training loss: 0.00043031
INFO:root:[110,   600] training loss: 0.00014350
INFO:root:[110,   650] training loss: 0.00003034
INFO:root:[110,   700] training loss: 0.00002726
INFO:root:[110,   750] training loss: 0.00058041
INFO:root:[110,   800] training loss: 0.00044453
INFO:root:[110,   850] training loss: 0.00039938
INFO:root:[110,   900] training loss: 0.00804692
INFO:root:[110,   950] training loss: 0.00144195
INFO:root:[110,  1000] training loss: 0.00008218
INFO:root:[110,  1050] training loss: 0.00003765
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.3333    0.6667    0.4444         3
    Prophase     0.8741    0.8705    0.8723      1722
           S     0.8541    0.7834    0.8173      1039
   Telophase     0.6000    0.6000    0.6000        10
          G2     0.4576    0.7297    0.5625        74
          G1     0.6428    0.6699    0.6561      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7911      3872
   macro avg     0.6327    0.7600    0.6789      3872
weighted avg     0.7985    0.7911    0.7936      3872

INFO:root:epoch110
INFO:root:[111,    50] training loss: 0.01068250
INFO:root:[111,   100] training loss: 0.00771760
INFO:root:[111,   150] training loss: 0.00804132
INFO:root:[111,   200] training loss: 0.00696059
INFO:root:[111,   250] training loss: 0.00657866
INFO:root:[111,   300] training loss: 0.00808106
INFO:root:[111,   350] training loss: 0.00690628
INFO:root:[111,   400] training loss: 0.00004236
INFO:root:[111,   450] training loss: 0.00001667
INFO:root:[111,   500] training loss: 0.00002770
INFO:root:[111,   550] training loss: 0.00031314
INFO:root:[111,   600] training loss: 0.00016466
INFO:root:[111,   650] training loss: 0.00003825
INFO:root:[111,   700] training loss: 0.00002747
INFO:root:[111,   750] training loss: 0.00066479
INFO:root:[111,   800] training loss: 0.00057937
INFO:root:[111,   850] training loss: 0.00035425
INFO:root:[111,   900] training loss: 0.00551916
INFO:root:[111,   950] training loss: 0.00137452
INFO:root:[111,  1000] training loss: 0.00005015
INFO:root:[111,  1050] training loss: 0.00002762
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.8820    0.8635    0.8727      1722
           S     0.8459    0.7873    0.8156      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4179    0.7568    0.5385        74
          G1     0.6462    0.6729    0.6593      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.7903      3872
   macro avg     0.6292    0.7639    0.6796      3872
weighted avg     0.7999    0.7903    0.7937      3872

INFO:root:epoch111
INFO:root:[112,    50] training loss: 0.00975890
INFO:root:[112,   100] training loss: 0.00876349
INFO:root:[112,   150] training loss: 0.00848370
INFO:root:[112,   200] training loss: 0.00854032
INFO:root:[112,   250] training loss: 0.00772917
INFO:root:[112,   300] training loss: 0.00841391
INFO:root:[112,   350] training loss: 0.00654096
INFO:root:[112,   400] training loss: 0.00001446
INFO:root:[112,   450] training loss: 0.00001818
INFO:root:[112,   500] training loss: 0.00002512
INFO:root:[112,   550] training loss: 0.00025688
INFO:root:[112,   600] training loss: 0.00022241
INFO:root:[112,   650] training loss: 0.00003116
INFO:root:[112,   700] training loss: 0.00004766
INFO:root:[112,   750] training loss: 0.00066776
INFO:root:[112,   800] training loss: 0.00079290
INFO:root:[112,   850] training loss: 0.00077075
INFO:root:[112,   900] training loss: 0.00497296
INFO:root:[112,   950] training loss: 0.00209896
INFO:root:[112,  1000] training loss: 0.00003930
INFO:root:[112,  1050] training loss: 0.00002395
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9138    0.8374    0.8739      1722
           S     0.8411    0.8200    0.8304      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4240    0.7162    0.5327        74
          G1     0.6519    0.7250    0.6865      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8004      3872
   macro avg     0.6490    0.7665    0.6952      3872
weighted avg     0.8144    0.8004    0.8053      3872

INFO:root:epoch112
INFO:root:[113,    50] training loss: 0.00960763
INFO:root:[113,   100] training loss: 0.00784208
INFO:root:[113,   150] training loss: 0.00831091
INFO:root:[113,   200] training loss: 0.00706151
INFO:root:[113,   250] training loss: 0.00643969
INFO:root:[113,   300] training loss: 0.00838336
INFO:root:[113,   350] training loss: 0.00664344
INFO:root:[113,   400] training loss: 0.00001622
INFO:root:[113,   450] training loss: 0.00001699
INFO:root:[113,   500] training loss: 0.00002868
INFO:root:[113,   550] training loss: 0.00027315
INFO:root:[113,   600] training loss: 0.00015934
INFO:root:[113,   650] training loss: 0.00004147
INFO:root:[113,   700] training loss: 0.00003439
INFO:root:[113,   750] training loss: 0.00083023
INFO:root:[113,   800] training loss: 0.00080109
INFO:root:[113,   850] training loss: 0.00071652
INFO:root:[113,   900] training loss: 0.00525669
INFO:root:[113,   950] training loss: 0.00149790
INFO:root:[113,  1000] training loss: 0.00002981
INFO:root:[113,  1050] training loss: 0.00002406
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9183    0.8351    0.8747      1722
           S     0.8371    0.8306    0.8338      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4370    0.7027    0.5389        74
          G1     0.6537    0.7269    0.6884      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8024      3872
   macro avg     0.6512    0.7660    0.6969      3872
weighted avg     0.8160    0.8024    0.8072      3872

INFO:root:epoch113
INFO:root:[114,    50] training loss: 0.00847980
INFO:root:[114,   100] training loss: 0.00708670
INFO:root:[114,   150] training loss: 0.00785603
INFO:root:[114,   200] training loss: 0.00713418
INFO:root:[114,   250] training loss: 0.00625440
INFO:root:[114,   300] training loss: 0.00819788
INFO:root:[114,   350] training loss: 0.00617457
INFO:root:[114,   400] training loss: 0.00001909
INFO:root:[114,   450] training loss: 0.00001770
INFO:root:[114,   500] training loss: 0.00002312
INFO:root:[114,   550] training loss: 0.00032512
INFO:root:[114,   600] training loss: 0.00016513
INFO:root:[114,   650] training loss: 0.00002898
INFO:root:[114,   700] training loss: 0.00003282
INFO:root:[114,   750] training loss: 0.00058252
INFO:root:[114,   800] training loss: 0.00075081
INFO:root:[114,   850] training loss: 0.00068862
INFO:root:[114,   900] training loss: 0.00577447
INFO:root:[114,   950] training loss: 0.00152513
INFO:root:[114,  1000] training loss: 0.00005766
INFO:root:[114,  1050] training loss: 0.00002086
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9178    0.8362    0.8751      1722
           S     0.8377    0.8393    0.8385      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4636    0.6892    0.5543        74
          G1     0.6551    0.7259    0.6887      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8048      3872
   macro avg     0.6552    0.7653    0.6999      3872
weighted avg     0.8169    0.8048    0.8090      3872

INFO:root:epoch114
INFO:root:[115,    50] training loss: 0.00923344
INFO:root:[115,   100] training loss: 0.00763755
INFO:root:[115,   150] training loss: 0.00773107
INFO:root:[115,   200] training loss: 0.00692333
INFO:root:[115,   250] training loss: 0.00695615
INFO:root:[115,   300] training loss: 0.00813506
INFO:root:[115,   350] training loss: 0.00655086
INFO:root:[115,   400] training loss: 0.00001727
INFO:root:[115,   450] training loss: 0.00001886
INFO:root:[115,   500] training loss: 0.00002687
INFO:root:[115,   550] training loss: 0.00031325
INFO:root:[115,   600] training loss: 0.00014534
INFO:root:[115,   650] training loss: 0.00002502
INFO:root:[115,   700] training loss: 0.00002416
INFO:root:[115,   750] training loss: 0.00056587
INFO:root:[115,   800] training loss: 0.00066933
INFO:root:[115,   850] training loss: 0.00095018
INFO:root:[115,   900] training loss: 0.00531676
INFO:root:[115,   950] training loss: 0.00162808
INFO:root:[115,  1000] training loss: 0.00005539
INFO:root:[115,  1050] training loss: 0.00002444
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9161    0.8374    0.8750      1722
           S     0.8433    0.8393    0.8413      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4679    0.6892    0.5574        74
          G1     0.6569    0.7299    0.6915      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8063      3872
   macro avg     0.6566    0.7661    0.7011      3872
weighted avg     0.8182    0.8063    0.8105      3872

INFO:root:epoch115
INFO:root:[116,    50] training loss: 0.00886043
INFO:root:[116,   100] training loss: 0.00753210
INFO:root:[116,   150] training loss: 0.00771553
INFO:root:[116,   200] training loss: 0.00685033
INFO:root:[116,   250] training loss: 0.00676245
INFO:root:[116,   300] training loss: 0.00808958
INFO:root:[116,   350] training loss: 0.00635836
INFO:root:[116,   400] training loss: 0.00001920
INFO:root:[116,   450] training loss: 0.00001682
INFO:root:[116,   500] training loss: 0.00002745
INFO:root:[116,   550] training loss: 0.00029992
INFO:root:[116,   600] training loss: 0.00017599
INFO:root:[116,   650] training loss: 0.00002367
INFO:root:[116,   700] training loss: 0.00003261
INFO:root:[116,   750] training loss: 0.00046571
INFO:root:[116,   800] training loss: 0.00050692
INFO:root:[116,   850] training loss: 0.00055311
INFO:root:[116,   900] training loss: 0.00531986
INFO:root:[116,   950] training loss: 0.00134002
INFO:root:[116,  1000] training loss: 0.00003797
INFO:root:[116,  1050] training loss: 0.00002341
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9167    0.8374    0.8753      1722
           S     0.8439    0.8325    0.8382      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4595    0.6892    0.5514        74
          G1     0.6550    0.7328    0.6917      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8053      3872
   macro avg     0.6553    0.7655    0.6999      3872
weighted avg     0.8180    0.8053    0.8097      3872

INFO:root:epoch116
INFO:root:[117,    50] training loss: 0.00831908
INFO:root:[117,   100] training loss: 0.00725430
INFO:root:[117,   150] training loss: 0.00770076
INFO:root:[117,   200] training loss: 0.00714150
INFO:root:[117,   250] training loss: 0.00623342
INFO:root:[117,   300] training loss: 0.00772882
INFO:root:[117,   350] training loss: 0.00641418
INFO:root:[117,   400] training loss: 0.00001774
INFO:root:[117,   450] training loss: 0.00001901
INFO:root:[117,   500] training loss: 0.00002975
INFO:root:[117,   550] training loss: 0.00026378
INFO:root:[117,   600] training loss: 0.00017832
INFO:root:[117,   650] training loss: 0.00003103
INFO:root:[117,   700] training loss: 0.00001970
INFO:root:[117,   750] training loss: 0.00061621
INFO:root:[117,   800] training loss: 0.00044826
INFO:root:[117,   850] training loss: 0.00062231
INFO:root:[117,   900] training loss: 0.00484829
INFO:root:[117,   950] training loss: 0.00177598
INFO:root:[117,  1000] training loss: 0.00002927
INFO:root:[117,  1050] training loss: 0.00002277
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9161    0.8374    0.8750      1722
           S     0.8427    0.8354    0.8391      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4636    0.6892    0.5543        74
          G1     0.6552    0.7299    0.6905      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8053      3872
   macro avg     0.6557    0.7655    0.7003      3872
weighted avg     0.8175    0.8053    0.8096      3872

INFO:root:epoch117
INFO:root:[118,    50] training loss: 0.00822174
INFO:root:[118,   100] training loss: 0.00856207
INFO:root:[118,   150] training loss: 0.00750664
INFO:root:[118,   200] training loss: 0.00705674
INFO:root:[118,   250] training loss: 0.00619044
INFO:root:[118,   300] training loss: 0.00785679
INFO:root:[118,   350] training loss: 0.00663517
INFO:root:[118,   400] training loss: 0.00001661
INFO:root:[118,   450] training loss: 0.00001672
INFO:root:[118,   500] training loss: 0.00002880
INFO:root:[118,   550] training loss: 0.00032430
INFO:root:[118,   600] training loss: 0.00016795
INFO:root:[118,   650] training loss: 0.00003029
INFO:root:[118,   700] training loss: 0.00002121
INFO:root:[118,   750] training loss: 0.00047122
INFO:root:[118,   800] training loss: 0.00042608
INFO:root:[118,   850] training loss: 0.00058496
INFO:root:[118,   900] training loss: 0.00545157
INFO:root:[118,   950] training loss: 0.00158869
INFO:root:[118,  1000] training loss: 0.00002889
INFO:root:[118,  1050] training loss: 0.00002440
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9151    0.8386    0.8752      1722
           S     0.8450    0.8345    0.8397      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4630    0.6757    0.5495        74
          G1     0.6558    0.7318    0.6917      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8058      3872
   macro avg     0.6559    0.7639    0.6998      3872
weighted avg     0.8178    0.8058    0.8101      3872

INFO:root:epoch118
INFO:root:[119,    50] training loss: 0.00869757
INFO:root:[119,   100] training loss: 0.00784838
INFO:root:[119,   150] training loss: 0.00761431
INFO:root:[119,   200] training loss: 0.00687325
INFO:root:[119,   250] training loss: 0.00637974
INFO:root:[119,   300] training loss: 0.00792677
INFO:root:[119,   350] training loss: 0.00620408
INFO:root:[119,   400] training loss: 0.00001612
INFO:root:[119,   450] training loss: 0.00001713
INFO:root:[119,   500] training loss: 0.00002553
INFO:root:[119,   550] training loss: 0.00027691
INFO:root:[119,   600] training loss: 0.00015082
INFO:root:[119,   650] training loss: 0.00002191
INFO:root:[119,   700] training loss: 0.00002384
INFO:root:[119,   750] training loss: 0.00036357
INFO:root:[119,   800] training loss: 0.00064140
INFO:root:[119,   850] training loss: 0.00070614
INFO:root:[119,   900] training loss: 0.00516114
INFO:root:[119,   950] training loss: 0.00167230
INFO:root:[119,  1000] training loss: 0.00003088
INFO:root:[119,  1050] training loss: 0.00002928
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9157    0.8391    0.8758      1722
           S     0.8436    0.8306    0.8371      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4679    0.6892    0.5574        74
          G1     0.6547    0.7318    0.6911      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8053      3872
   macro avg     0.6563    0.7653    0.7006      3872
weighted avg     0.8175    0.8053    0.8096      3872

INFO:root:epoch119
INFO:root:[120,    50] training loss: 0.00802588
INFO:root:[120,   100] training loss: 0.00811293
INFO:root:[120,   150] training loss: 0.00777476
INFO:root:[120,   200] training loss: 0.00784527
INFO:root:[120,   250] training loss: 0.00780191
INFO:root:[120,   300] training loss: 0.00882174
INFO:root:[120,   350] training loss: 0.00636686
INFO:root:[120,   400] training loss: 0.00001711
INFO:root:[120,   450] training loss: 0.00002019
INFO:root:[120,   500] training loss: 0.00004282
INFO:root:[120,   550] training loss: 0.00026917
INFO:root:[120,   600] training loss: 0.00016623
INFO:root:[120,   650] training loss: 0.00005050
INFO:root:[120,   700] training loss: 0.00002284
INFO:root:[120,   750] training loss: 0.00061738
INFO:root:[120,   800] training loss: 0.00044554
INFO:root:[120,   850] training loss: 0.00047112
INFO:root:[120,   900] training loss: 0.00502144
INFO:root:[120,   950] training loss: 0.00140603
INFO:root:[120,  1000] training loss: 0.00003069
INFO:root:[120,  1050] training loss: 0.00002511
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.5000    0.6667    0.5714         3
    Prophase     0.9161    0.8374    0.8750      1722
           S     0.8447    0.8325    0.8386      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4636    0.6892    0.5543        74
          G1     0.6544    0.7328    0.6914      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8053      3872
   macro avg     0.6559    0.7655    0.7003      3872
weighted avg     0.8178    0.8053    0.8097      3872

INFO:root:epoch120
INFO:root:[121,    50] training loss: 0.00835798
INFO:root:[121,   100] training loss: 0.00751395
INFO:root:[121,   150] training loss: 0.00832445
INFO:root:[121,   200] training loss: 0.00672930
INFO:root:[121,   250] training loss: 0.00633720
INFO:root:[121,   300] training loss: 0.00790217
INFO:root:[121,   350] training loss: 0.00653319
INFO:root:[121,   400] training loss: 0.00002035
INFO:root:[121,   450] training loss: 0.00001757
INFO:root:[121,   500] training loss: 0.00003310
INFO:root:[121,   550] training loss: 0.00035854
INFO:root:[121,   600] training loss: 0.00012881
INFO:root:[121,   650] training loss: 0.00003207
INFO:root:[121,   700] training loss: 0.00002195
INFO:root:[121,   750] training loss: 0.00050932
INFO:root:[121,   800] training loss: 0.00059659
INFO:root:[121,   850] training loss: 0.00048644
INFO:root:[121,   900] training loss: 0.00475317
INFO:root:[121,   950] training loss: 0.00142105
INFO:root:[121,  1000] training loss: 0.00003024
INFO:root:[121,  1050] training loss: 0.00001880
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9157    0.8386    0.8754      1722
           S     0.8456    0.8325    0.8390      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4679    0.6892    0.5574        74
          G1     0.6555    0.7328    0.6920      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8058      3872
   macro avg     0.6424    0.7657    0.6907      3872
weighted avg     0.8182    0.8058    0.8102      3872

INFO:root:epoch121
INFO:root:[122,    50] training loss: 0.00971313
INFO:root:[122,   100] training loss: 0.00840742
INFO:root:[122,   150] training loss: 0.00770785
INFO:root:[122,   200] training loss: 0.00688782
INFO:root:[122,   250] training loss: 0.00606326
INFO:root:[122,   300] training loss: 0.00789217
INFO:root:[122,   350] training loss: 0.00668557
INFO:root:[122,   400] training loss: 0.00001824
INFO:root:[122,   450] training loss: 0.00001715
INFO:root:[122,   500] training loss: 0.00002657
INFO:root:[122,   550] training loss: 0.00027174
INFO:root:[122,   600] training loss: 0.00013617
INFO:root:[122,   650] training loss: 0.00002213
INFO:root:[122,   700] training loss: 0.00002218
INFO:root:[122,   750] training loss: 0.00030420
INFO:root:[122,   800] training loss: 0.00040874
INFO:root:[122,   850] training loss: 0.00044077
INFO:root:[122,   900] training loss: 0.00515352
INFO:root:[122,   950] training loss: 0.00133688
INFO:root:[122,  1000] training loss: 0.00002619
INFO:root:[122,  1050] training loss: 0.00002165
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9167    0.8368    0.8749      1722
           S     0.8459    0.8345    0.8401      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6550    0.7348    0.6926      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8060      3872
   macro avg     0.6431    0.7660    0.6914      3872
weighted avg     0.8186    0.8060    0.8104      3872

INFO:root:epoch122
INFO:root:[123,    50] training loss: 0.00819323
INFO:root:[123,   100] training loss: 0.00782561
INFO:root:[123,   150] training loss: 0.00760595
INFO:root:[123,   200] training loss: 0.00689058
INFO:root:[123,   250] training loss: 0.00615934
INFO:root:[123,   300] training loss: 0.00754979
INFO:root:[123,   350] training loss: 0.00644371
INFO:root:[123,   400] training loss: 0.00001633
INFO:root:[123,   450] training loss: 0.00001745
INFO:root:[123,   500] training loss: 0.00002479
INFO:root:[123,   550] training loss: 0.00031425
INFO:root:[123,   600] training loss: 0.00011680
INFO:root:[123,   650] training loss: 0.00003388
INFO:root:[123,   700] training loss: 0.00001766
INFO:root:[123,   750] training loss: 0.00033908
INFO:root:[123,   800] training loss: 0.00038516
INFO:root:[123,   850] training loss: 0.00058796
INFO:root:[123,   900] training loss: 0.00496716
INFO:root:[123,   950] training loss: 0.00166981
INFO:root:[123,  1000] training loss: 0.00003340
INFO:root:[123,  1050] training loss: 0.00001973
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9161    0.8368    0.8747      1722
           S     0.8467    0.8345    0.8405      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6550    0.7348    0.6926      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8060      3872
   macro avg     0.6432    0.7660    0.6914      3872
weighted avg     0.8186    0.8060    0.8104      3872

INFO:root:epoch123
INFO:root:[124,    50] training loss: 0.00848111
INFO:root:[124,   100] training loss: 0.00755038
INFO:root:[124,   150] training loss: 0.00777688
INFO:root:[124,   200] training loss: 0.00740799
INFO:root:[124,   250] training loss: 0.00639507
INFO:root:[124,   300] training loss: 0.00853853
INFO:root:[124,   350] training loss: 0.00652177
INFO:root:[124,   400] training loss: 0.00003851
INFO:root:[124,   450] training loss: 0.00001851
INFO:root:[124,   500] training loss: 0.00003585
INFO:root:[124,   550] training loss: 0.00027052
INFO:root:[124,   600] training loss: 0.00017397
INFO:root:[124,   650] training loss: 0.00002217
INFO:root:[124,   700] training loss: 0.00001962
INFO:root:[124,   750] training loss: 0.00038916
INFO:root:[124,   800] training loss: 0.00034709
INFO:root:[124,   850] training loss: 0.00048723
INFO:root:[124,   900] training loss: 0.00574430
INFO:root:[124,   950] training loss: 0.00135772
INFO:root:[124,  1000] training loss: 0.00003091
INFO:root:[124,  1050] training loss: 0.00002748
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9161    0.8368    0.8747      1722
           S     0.8475    0.8345    0.8409      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6553    0.7358    0.6932      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8063      3872
   macro avg     0.6433    0.7661    0.6915      3872
weighted avg     0.8189    0.8063    0.8107      3872

INFO:root:epoch124
INFO:root:[125,    50] training loss: 0.00828795
INFO:root:[125,   100] training loss: 0.00760639
INFO:root:[125,   150] training loss: 0.00734704
INFO:root:[125,   200] training loss: 0.00711854
INFO:root:[125,   250] training loss: 0.00647367
INFO:root:[125,   300] training loss: 0.00790001
INFO:root:[125,   350] training loss: 0.00636737
INFO:root:[125,   400] training loss: 0.00001504
INFO:root:[125,   450] training loss: 0.00001695
INFO:root:[125,   500] training loss: 0.00003118
INFO:root:[125,   550] training loss: 0.00027001
INFO:root:[125,   600] training loss: 0.00023838
INFO:root:[125,   650] training loss: 0.00001979
INFO:root:[125,   700] training loss: 0.00002326
INFO:root:[125,   750] training loss: 0.00036143
INFO:root:[125,   800] training loss: 0.00037972
INFO:root:[125,   850] training loss: 0.00053946
INFO:root:[125,   900] training loss: 0.00480254
INFO:root:[125,   950] training loss: 0.00145106
INFO:root:[125,  1000] training loss: 0.00003802
INFO:root:[125,  1050] training loss: 0.00002019
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9156    0.8380    0.8751      1722
           S     0.8474    0.8335    0.8404      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6564    0.7358    0.6938      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8066      3872
   macro avg     0.6434    0.7662    0.6916      3872
weighted avg     0.8189    0.8066    0.8109      3872

INFO:root:epoch125
INFO:root:[126,    50] training loss: 0.00817566
INFO:root:[126,   100] training loss: 0.00784334
INFO:root:[126,   150] training loss: 0.00781818
INFO:root:[126,   200] training loss: 0.00679056
INFO:root:[126,   250] training loss: 0.00688947
INFO:root:[126,   300] training loss: 0.00874812
INFO:root:[126,   350] training loss: 0.00635604
INFO:root:[126,   400] training loss: 0.00001614
INFO:root:[126,   450] training loss: 0.00001975
INFO:root:[126,   500] training loss: 0.00002676
INFO:root:[126,   550] training loss: 0.00030000
INFO:root:[126,   600] training loss: 0.00020329
INFO:root:[126,   650] training loss: 0.00002947
INFO:root:[126,   700] training loss: 0.00001910
INFO:root:[126,   750] training loss: 0.00034728
INFO:root:[126,   800] training loss: 0.00062328
INFO:root:[126,   850] training loss: 0.00044001
INFO:root:[126,   900] training loss: 0.00554936
INFO:root:[126,   950] training loss: 0.00185717
INFO:root:[126,  1000] training loss: 0.00002761
INFO:root:[126,  1050] training loss: 0.00002627
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9156    0.8374    0.8747      1722
           S     0.8474    0.8335    0.8404      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6559    0.7358    0.6935      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8063      3872
   macro avg     0.6433    0.7661    0.6915      3872
weighted avg     0.8188    0.8063    0.8107      3872

INFO:root:epoch126
INFO:root:[127,    50] training loss: 0.00831717
INFO:root:[127,   100] training loss: 0.00770740
INFO:root:[127,   150] training loss: 0.00806186
INFO:root:[127,   200] training loss: 0.00704970
INFO:root:[127,   250] training loss: 0.00674440
INFO:root:[127,   300] training loss: 0.00812073
INFO:root:[127,   350] training loss: 0.00636282
INFO:root:[127,   400] training loss: 0.00001520
INFO:root:[127,   450] training loss: 0.00006931
INFO:root:[127,   500] training loss: 0.00003773
INFO:root:[127,   550] training loss: 0.00023686
INFO:root:[127,   600] training loss: 0.00014416
INFO:root:[127,   650] training loss: 0.00002210
INFO:root:[127,   700] training loss: 0.00002884
INFO:root:[127,   750] training loss: 0.00031201
INFO:root:[127,   800] training loss: 0.00049822
INFO:root:[127,   850] training loss: 0.00043376
INFO:root:[127,   900] training loss: 0.00513594
INFO:root:[127,   950] training loss: 0.00225717
INFO:root:[127,  1000] training loss: 0.00003480
INFO:root:[127,  1050] training loss: 0.00004764
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8465    0.8335    0.8400      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6564    0.7358    0.6938      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8066      3872
   macro avg     0.6434    0.7662    0.6916      3872
weighted avg     0.8190    0.8066    0.8109      3872

INFO:root:epoch127
INFO:root:[128,    50] training loss: 0.00880532
INFO:root:[128,   100] training loss: 0.00835010
INFO:root:[128,   150] training loss: 0.00788538
INFO:root:[128,   200] training loss: 0.00707089
INFO:root:[128,   250] training loss: 0.00762793
INFO:root:[128,   300] training loss: 0.00798769
INFO:root:[128,   350] training loss: 0.00684883
INFO:root:[128,   400] training loss: 0.00006027
INFO:root:[128,   450] training loss: 0.00001921
INFO:root:[128,   500] training loss: 0.00002710
INFO:root:[128,   550] training loss: 0.00030127
INFO:root:[128,   600] training loss: 0.00014265
INFO:root:[128,   650] training loss: 0.00002614
INFO:root:[128,   700] training loss: 0.00003585
INFO:root:[128,   750] training loss: 0.00034453
INFO:root:[128,   800] training loss: 0.00045772
INFO:root:[128,   850] training loss: 0.00057203
INFO:root:[128,   900] training loss: 0.00461850
INFO:root:[128,   950] training loss: 0.00161855
INFO:root:[128,  1000] training loss: 0.00004012
INFO:root:[128,  1050] training loss: 0.00006988
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8482    0.8335    0.8408      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6570    0.7377    0.6950      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8071      3872
   macro avg     0.6437    0.7664    0.6919      3872
weighted avg     0.8196    0.8071    0.8115      3872

INFO:root:epoch128
INFO:root:[129,    50] training loss: 0.00839985
INFO:root:[129,   100] training loss: 0.00759932
INFO:root:[129,   150] training loss: 0.00775910
INFO:root:[129,   200] training loss: 0.00674227
INFO:root:[129,   250] training loss: 0.00686594
INFO:root:[129,   300] training loss: 0.00791080
INFO:root:[129,   350] training loss: 0.00679934
INFO:root:[129,   400] training loss: 0.00001783
INFO:root:[129,   450] training loss: 0.00002027
INFO:root:[129,   500] training loss: 0.00002713
INFO:root:[129,   550] training loss: 0.00029047
INFO:root:[129,   600] training loss: 0.00017346
INFO:root:[129,   650] training loss: 0.00002275
INFO:root:[129,   700] training loss: 0.00002679
INFO:root:[129,   750] training loss: 0.00036265
INFO:root:[129,   800] training loss: 0.00051928
INFO:root:[129,   850] training loss: 0.00050577
INFO:root:[129,   900] training loss: 0.00690282
INFO:root:[129,   950] training loss: 0.00129175
INFO:root:[129,  1000] training loss: 0.00003357
INFO:root:[129,  1050] training loss: 0.00002222
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9168    0.8380    0.8756      1722
           S     0.8490    0.8335    0.8412      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8076      3872
   macro avg     0.6440    0.7667    0.6921      3872
weighted avg     0.8202    0.8076    0.8120      3872

INFO:root:epoch129
INFO:root:[130,    50] training loss: 0.00825226
INFO:root:[130,   100] training loss: 0.00755635
INFO:root:[130,   150] training loss: 0.00789366
INFO:root:[130,   200] training loss: 0.00676152
INFO:root:[130,   250] training loss: 0.00640828
INFO:root:[130,   300] training loss: 0.00803534
INFO:root:[130,   350] training loss: 0.00618664
INFO:root:[130,   400] training loss: 0.00001599
INFO:root:[130,   450] training loss: 0.00001831
INFO:root:[130,   500] training loss: 0.00003277
INFO:root:[130,   550] training loss: 0.00029912
INFO:root:[130,   600] training loss: 0.00020886
INFO:root:[130,   650] training loss: 0.00002483
INFO:root:[130,   700] training loss: 0.00002000
INFO:root:[130,   750] training loss: 0.00032699
INFO:root:[130,   800] training loss: 0.00038576
INFO:root:[130,   850] training loss: 0.00065429
INFO:root:[130,   900] training loss: 0.00752027
INFO:root:[130,   950] training loss: 0.00174046
INFO:root:[130,  1000] training loss: 0.00003730
INFO:root:[130,  1050] training loss: 0.00002114
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9168    0.8380    0.8756      1722
           S     0.8483    0.8345    0.8413      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6579    0.7387    0.6960      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8076      3872
   macro avg     0.6439    0.7667    0.6921      3872
weighted avg     0.8201    0.8076    0.8120      3872

INFO:root:epoch130
INFO:root:[131,    50] training loss: 0.00801296
INFO:root:[131,   100] training loss: 0.00749877
INFO:root:[131,   150] training loss: 0.00784925
INFO:root:[131,   200] training loss: 0.00723493
INFO:root:[131,   250] training loss: 0.00590243
INFO:root:[131,   300] training loss: 0.00767439
INFO:root:[131,   350] training loss: 0.00651338
INFO:root:[131,   400] training loss: 0.00001651
INFO:root:[131,   450] training loss: 0.00001676
INFO:root:[131,   500] training loss: 0.00002620
INFO:root:[131,   550] training loss: 0.00032142
INFO:root:[131,   600] training loss: 0.00017405
INFO:root:[131,   650] training loss: 0.00002985
INFO:root:[131,   700] training loss: 0.00001931
INFO:root:[131,   750] training loss: 0.00031093
INFO:root:[131,   800] training loss: 0.00041121
INFO:root:[131,   850] training loss: 0.00058661
INFO:root:[131,   900] training loss: 0.00588039
INFO:root:[131,   950] training loss: 0.00132523
INFO:root:[131,  1000] training loss: 0.00002917
INFO:root:[131,  1050] training loss: 0.00001921
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8386    0.8757      1722
           S     0.8499    0.8335    0.8416      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4722    0.6892    0.5604        74
          G1     0.6582    0.7397    0.6966      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6441    0.7668    0.6922      3872
weighted avg     0.8204    0.8079    0.8122      3872

INFO:root:epoch131
INFO:root:[132,    50] training loss: 0.00850451
INFO:root:[132,   100] training loss: 0.00760028
INFO:root:[132,   150] training loss: 0.00786377
INFO:root:[132,   200] training loss: 0.00710957
INFO:root:[132,   250] training loss: 0.00660288
INFO:root:[132,   300] training loss: 0.00797836
INFO:root:[132,   350] training loss: 0.00625483
INFO:root:[132,   400] training loss: 0.00003732
INFO:root:[132,   450] training loss: 0.00001630
INFO:root:[132,   500] training loss: 0.00002743
INFO:root:[132,   550] training loss: 0.00025956
INFO:root:[132,   600] training loss: 0.00013749
INFO:root:[132,   650] training loss: 0.00002197
INFO:root:[132,   700] training loss: 0.00002234
INFO:root:[132,   750] training loss: 0.00044654
INFO:root:[132,   800] training loss: 0.00051906
INFO:root:[132,   850] training loss: 0.00056722
INFO:root:[132,   900] training loss: 0.00575039
INFO:root:[132,   950] training loss: 0.00159367
INFO:root:[132,  1000] training loss: 0.00002776
INFO:root:[132,  1050] training loss: 0.00004264
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9168    0.8380    0.8756      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6579    0.7407    0.6969      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8081      3872
   macro avg     0.6448    0.7670    0.6928      3872
weighted avg     0.8206    0.8081    0.8125      3872

INFO:root:epoch132
INFO:root:[133,    50] training loss: 0.00914973
INFO:root:[133,   100] training loss: 0.00765845
INFO:root:[133,   150] training loss: 0.00771298
INFO:root:[133,   200] training loss: 0.00701401
INFO:root:[133,   250] training loss: 0.00650069
INFO:root:[133,   300] training loss: 0.00809223
INFO:root:[133,   350] training loss: 0.00637113
INFO:root:[133,   400] training loss: 0.00001747
INFO:root:[133,   450] training loss: 0.00001708
INFO:root:[133,   500] training loss: 0.00002965
INFO:root:[133,   550] training loss: 0.00035839
INFO:root:[133,   600] training loss: 0.00016038
INFO:root:[133,   650] training loss: 0.00002516
INFO:root:[133,   700] training loss: 0.00002202
INFO:root:[133,   750] training loss: 0.00033999
INFO:root:[133,   800] training loss: 0.00052697
INFO:root:[133,   850] training loss: 0.00058058
INFO:root:[133,   900] training loss: 0.00566601
INFO:root:[133,   950] training loss: 0.00135296
INFO:root:[133,  1000] training loss: 0.00003146
INFO:root:[133,  1050] training loss: 0.00002284
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch133
INFO:root:[134,    50] training loss: 0.00869682
INFO:root:[134,   100] training loss: 0.00856558
INFO:root:[134,   150] training loss: 0.00751813
INFO:root:[134,   200] training loss: 0.00682343
INFO:root:[134,   250] training loss: 0.00625135
INFO:root:[134,   300] training loss: 0.00844258
INFO:root:[134,   350] training loss: 0.00759086
INFO:root:[134,   400] training loss: 0.00001652
INFO:root:[134,   450] training loss: 0.00001931
INFO:root:[134,   500] training loss: 0.00002828
INFO:root:[134,   550] training loss: 0.00026141
INFO:root:[134,   600] training loss: 0.00016055
INFO:root:[134,   650] training loss: 0.00002521
INFO:root:[134,   700] training loss: 0.00002389
INFO:root:[134,   750] training loss: 0.00032607
INFO:root:[134,   800] training loss: 0.00040051
INFO:root:[134,   850] training loss: 0.00046929
INFO:root:[134,   900] training loss: 0.00509279
INFO:root:[134,   950] training loss: 0.00162137
INFO:root:[134,  1000] training loss: 0.00004784
INFO:root:[134,  1050] training loss: 0.00002525
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch134
INFO:root:[135,    50] training loss: 0.00850788
INFO:root:[135,   100] training loss: 0.00770219
INFO:root:[135,   150] training loss: 0.00749438
INFO:root:[135,   200] training loss: 0.00690608
INFO:root:[135,   250] training loss: 0.00650633
INFO:root:[135,   300] training loss: 0.00765749
INFO:root:[135,   350] training loss: 0.00690795
INFO:root:[135,   400] training loss: 0.00001741
INFO:root:[135,   450] training loss: 0.00001706
INFO:root:[135,   500] training loss: 0.00002582
INFO:root:[135,   550] training loss: 0.00034492
INFO:root:[135,   600] training loss: 0.00015214
INFO:root:[135,   650] training loss: 0.00002235
INFO:root:[135,   700] training loss: 0.00001854
INFO:root:[135,   750] training loss: 0.00030317
INFO:root:[135,   800] training loss: 0.00055662
INFO:root:[135,   850] training loss: 0.00041512
INFO:root:[135,   900] training loss: 0.00501108
INFO:root:[135,   950] training loss: 0.00150125
INFO:root:[135,  1000] training loss: 0.00002935
INFO:root:[135,  1050] training loss: 0.00001937
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch135
INFO:root:[136,    50] training loss: 0.00784744
INFO:root:[136,   100] training loss: 0.00732300
INFO:root:[136,   150] training loss: 0.00795694
INFO:root:[136,   200] training loss: 0.00696729
INFO:root:[136,   250] training loss: 0.00692997
INFO:root:[136,   300] training loss: 0.00775462
INFO:root:[136,   350] training loss: 0.00610075
INFO:root:[136,   400] training loss: 0.00001755
INFO:root:[136,   450] training loss: 0.00001789
INFO:root:[136,   500] training loss: 0.00002762
INFO:root:[136,   550] training loss: 0.00032335
INFO:root:[136,   600] training loss: 0.00018326
INFO:root:[136,   650] training loss: 0.00002153
INFO:root:[136,   700] training loss: 0.00002052
INFO:root:[136,   750] training loss: 0.00022928
INFO:root:[136,   800] training loss: 0.00043451
INFO:root:[136,   850] training loss: 0.00044113
INFO:root:[136,   900] training loss: 0.00567289
INFO:root:[136,   950] training loss: 0.00166893
INFO:root:[136,  1000] training loss: 0.00002793
INFO:root:[136,  1050] training loss: 0.00002876
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch136
INFO:root:[137,    50] training loss: 0.00851431
INFO:root:[137,   100] training loss: 0.00758267
INFO:root:[137,   150] training loss: 0.00759514
INFO:root:[137,   200] training loss: 0.00665612
INFO:root:[137,   250] training loss: 0.00636153
INFO:root:[137,   300] training loss: 0.00786533
INFO:root:[137,   350] training loss: 0.00646149
INFO:root:[137,   400] training loss: 0.00001502
INFO:root:[137,   450] training loss: 0.00001719
INFO:root:[137,   500] training loss: 0.00002866
INFO:root:[137,   550] training loss: 0.00025796
INFO:root:[137,   600] training loss: 0.00014300
INFO:root:[137,   650] training loss: 0.00002064
INFO:root:[137,   700] training loss: 0.00002121
INFO:root:[137,   750] training loss: 0.00042203
INFO:root:[137,   800] training loss: 0.00040807
INFO:root:[137,   850] training loss: 0.00067534
INFO:root:[137,   900] training loss: 0.00503273
INFO:root:[137,   950] training loss: 0.00145222
INFO:root:[137,  1000] training loss: 0.00003235
INFO:root:[137,  1050] training loss: 0.00002068
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch137
INFO:root:[138,    50] training loss: 0.00788578
INFO:root:[138,   100] training loss: 0.00735100
INFO:root:[138,   150] training loss: 0.00971157
INFO:root:[138,   200] training loss: 0.00833858
INFO:root:[138,   250] training loss: 0.00734827
INFO:root:[138,   300] training loss: 0.00792422
INFO:root:[138,   350] training loss: 0.00666246
INFO:root:[138,   400] training loss: 0.00001808
INFO:root:[138,   450] training loss: 0.00001778
INFO:root:[138,   500] training loss: 0.00002703
INFO:root:[138,   550] training loss: 0.00028300
INFO:root:[138,   600] training loss: 0.00012803
INFO:root:[138,   650] training loss: 0.00003755
INFO:root:[138,   700] training loss: 0.00002135
INFO:root:[138,   750] training loss: 0.00025515
INFO:root:[138,   800] training loss: 0.00041375
INFO:root:[138,   850] training loss: 0.00064284
INFO:root:[138,   900] training loss: 0.00625939
INFO:root:[138,   950] training loss: 0.00142298
INFO:root:[138,  1000] training loss: 0.00002625
INFO:root:[138,  1050] training loss: 0.00002297
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch138
INFO:root:[139,    50] training loss: 0.00806486
INFO:root:[139,   100] training loss: 0.00770888
INFO:root:[139,   150] training loss: 0.00861192
INFO:root:[139,   200] training loss: 0.00738262
INFO:root:[139,   250] training loss: 0.00644656
INFO:root:[139,   300] training loss: 0.00806094
INFO:root:[139,   350] training loss: 0.00609207
INFO:root:[139,   400] training loss: 0.00001467
INFO:root:[139,   450] training loss: 0.00006328
INFO:root:[139,   500] training loss: 0.00002466
INFO:root:[139,   550] training loss: 0.00028752
INFO:root:[139,   600] training loss: 0.00011670
INFO:root:[139,   650] training loss: 0.00003293
INFO:root:[139,   700] training loss: 0.00002168
INFO:root:[139,   750] training loss: 0.00037125
INFO:root:[139,   800] training loss: 0.00045880
INFO:root:[139,   850] training loss: 0.00044580
INFO:root:[139,   900] training loss: 0.00614572
INFO:root:[139,   950] training loss: 0.00132672
INFO:root:[139,  1000] training loss: 0.00002712
INFO:root:[139,  1050] training loss: 0.00001993
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch139
INFO:root:[140,    50] training loss: 0.00834334
INFO:root:[140,   100] training loss: 0.00826672
INFO:root:[140,   150] training loss: 0.00863734
INFO:root:[140,   200] training loss: 0.00688721
INFO:root:[140,   250] training loss: 0.00644006
INFO:root:[140,   300] training loss: 0.00842939
INFO:root:[140,   350] training loss: 0.00700731
INFO:root:[140,   400] training loss: 0.00001644
INFO:root:[140,   450] training loss: 0.00001721
INFO:root:[140,   500] training loss: 0.00002734
INFO:root:[140,   550] training loss: 0.00028948
INFO:root:[140,   600] training loss: 0.00016164
INFO:root:[140,   650] training loss: 0.00002723
INFO:root:[140,   700] training loss: 0.00001724
INFO:root:[140,   750] training loss: 0.00033178
INFO:root:[140,   800] training loss: 0.00049177
INFO:root:[140,   850] training loss: 0.00051190
INFO:root:[140,   900] training loss: 0.00728691
INFO:root:[140,   950] training loss: 0.00145293
INFO:root:[140,  1000] training loss: 0.00003628
INFO:root:[140,  1050] training loss: 0.00002217
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch140
INFO:root:[141,    50] training loss: 0.00803707
INFO:root:[141,   100] training loss: 0.00747200
INFO:root:[141,   150] training loss: 0.00790085
INFO:root:[141,   200] training loss: 0.00680185
INFO:root:[141,   250] training loss: 0.00632257
INFO:root:[141,   300] training loss: 0.00777690
INFO:root:[141,   350] training loss: 0.00680279
INFO:root:[141,   400] training loss: 0.00001676
INFO:root:[141,   450] training loss: 0.00001883
INFO:root:[141,   500] training loss: 0.00002750
INFO:root:[141,   550] training loss: 0.00028492
INFO:root:[141,   600] training loss: 0.00015963
INFO:root:[141,   650] training loss: 0.00002015
INFO:root:[141,   700] training loss: 0.00002045
INFO:root:[141,   750] training loss: 0.00037206
INFO:root:[141,   800] training loss: 0.00049753
INFO:root:[141,   850] training loss: 0.00042611
INFO:root:[141,   900] training loss: 0.00616106
INFO:root:[141,   950] training loss: 0.00161625
INFO:root:[141,  1000] training loss: 0.00003431
INFO:root:[141,  1050] training loss: 0.00003264
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch141
INFO:root:[142,    50] training loss: 0.00835363
INFO:root:[142,   100] training loss: 0.00728126
INFO:root:[142,   150] training loss: 0.00766605
INFO:root:[142,   200] training loss: 0.00688217
INFO:root:[142,   250] training loss: 0.00634665
INFO:root:[142,   300] training loss: 0.00881712
INFO:root:[142,   350] training loss: 0.00665191
INFO:root:[142,   400] training loss: 0.00001651
INFO:root:[142,   450] training loss: 0.00001761
INFO:root:[142,   500] training loss: 0.00002469
INFO:root:[142,   550] training loss: 0.00025064
INFO:root:[142,   600] training loss: 0.00015645
INFO:root:[142,   650] training loss: 0.00002841
INFO:root:[142,   700] training loss: 0.00003091
INFO:root:[142,   750] training loss: 0.00035524
INFO:root:[142,   800] training loss: 0.00060483
INFO:root:[142,   850] training loss: 0.00051044
INFO:root:[142,   900] training loss: 0.00532883
INFO:root:[142,   950] training loss: 0.00141927
INFO:root:[142,  1000] training loss: 0.00003134
INFO:root:[142,  1050] training loss: 0.00002853
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch142
INFO:root:[143,    50] training loss: 0.00819028
INFO:root:[143,   100] training loss: 0.00730711
INFO:root:[143,   150] training loss: 0.00784781
INFO:root:[143,   200] training loss: 0.00697367
INFO:root:[143,   250] training loss: 0.00661080
INFO:root:[143,   300] training loss: 0.00808486
INFO:root:[143,   350] training loss: 0.00629412
INFO:root:[143,   400] training loss: 0.00001658
INFO:root:[143,   450] training loss: 0.00001829
INFO:root:[143,   500] training loss: 0.00002939
INFO:root:[143,   550] training loss: 0.00026643
INFO:root:[143,   600] training loss: 0.00013989
INFO:root:[143,   650] training loss: 0.00002663
INFO:root:[143,   700] training loss: 0.00001863
INFO:root:[143,   750] training loss: 0.00032337
INFO:root:[143,   800] training loss: 0.00049347
INFO:root:[143,   850] training loss: 0.00054722
INFO:root:[143,   900] training loss: 0.00456847
INFO:root:[143,   950] training loss: 0.00129670
INFO:root:[143,  1000] training loss: 0.00004083
INFO:root:[143,  1050] training loss: 0.00002061
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch143
INFO:root:[144,    50] training loss: 0.00802643
INFO:root:[144,   100] training loss: 0.00724629
INFO:root:[144,   150] training loss: 0.00852026
INFO:root:[144,   200] training loss: 0.00704569
INFO:root:[144,   250] training loss: 0.00660856
INFO:root:[144,   300] training loss: 0.00797853
INFO:root:[144,   350] training loss: 0.00662345
INFO:root:[144,   400] training loss: 0.00001541
INFO:root:[144,   450] training loss: 0.00003980
INFO:root:[144,   500] training loss: 0.00002448
INFO:root:[144,   550] training loss: 0.00028510
INFO:root:[144,   600] training loss: 0.00013770
INFO:root:[144,   650] training loss: 0.00004480
INFO:root:[144,   700] training loss: 0.00001744
INFO:root:[144,   750] training loss: 0.00023728
INFO:root:[144,   800] training loss: 0.00040292
INFO:root:[144,   850] training loss: 0.00050926
INFO:root:[144,   900] training loss: 0.00731805
INFO:root:[144,   950] training loss: 0.00143993
INFO:root:[144,  1000] training loss: 0.00003527
INFO:root:[144,  1050] training loss: 0.00001868
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch144
INFO:root:[145,    50] training loss: 0.00884593
INFO:root:[145,   100] training loss: 0.00799930
INFO:root:[145,   150] training loss: 0.00806388
INFO:root:[145,   200] training loss: 0.00696778
INFO:root:[145,   250] training loss: 0.00646171
INFO:root:[145,   300] training loss: 0.00788541
INFO:root:[145,   350] training loss: 0.00677637
INFO:root:[145,   400] training loss: 0.00001571
INFO:root:[145,   450] training loss: 0.00001863
INFO:root:[145,   500] training loss: 0.00004084
INFO:root:[145,   550] training loss: 0.00027344
INFO:root:[145,   600] training loss: 0.00023137
INFO:root:[145,   650] training loss: 0.00002172
INFO:root:[145,   700] training loss: 0.00002177
INFO:root:[145,   750] training loss: 0.00048433
INFO:root:[145,   800] training loss: 0.00063261
INFO:root:[145,   850] training loss: 0.00060677
INFO:root:[145,   900] training loss: 0.00562496
INFO:root:[145,   950] training loss: 0.00166595
INFO:root:[145,  1000] training loss: 0.00003428
INFO:root:[145,  1050] training loss: 0.00002197
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch145
INFO:root:[146,    50] training loss: 0.00793939
INFO:root:[146,   100] training loss: 0.00762556
INFO:root:[146,   150] training loss: 0.00754655
INFO:root:[146,   200] training loss: 0.00695335
INFO:root:[146,   250] training loss: 0.00605241
INFO:root:[146,   300] training loss: 0.00791134
INFO:root:[146,   350] training loss: 0.00621726
INFO:root:[146,   400] training loss: 0.00001519
INFO:root:[146,   450] training loss: 0.00001949
INFO:root:[146,   500] training loss: 0.00003609
INFO:root:[146,   550] training loss: 0.00026683
INFO:root:[146,   600] training loss: 0.00014306
INFO:root:[146,   650] training loss: 0.00002565
INFO:root:[146,   700] training loss: 0.00001952
INFO:root:[146,   750] training loss: 0.00036836
INFO:root:[146,   800] training loss: 0.00050527
INFO:root:[146,   850] training loss: 0.00049649
INFO:root:[146,   900] training loss: 0.00594368
INFO:root:[146,   950] training loss: 0.00137889
INFO:root:[146,  1000] training loss: 0.00003095
INFO:root:[146,  1050] training loss: 0.00002351
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch146
INFO:root:[147,    50] training loss: 0.00839502
INFO:root:[147,   100] training loss: 0.00747730
INFO:root:[147,   150] training loss: 0.00790713
INFO:root:[147,   200] training loss: 0.00700947
INFO:root:[147,   250] training loss: 0.00603741
INFO:root:[147,   300] training loss: 0.00823333
INFO:root:[147,   350] training loss: 0.00632471
INFO:root:[147,   400] training loss: 0.00003685
INFO:root:[147,   450] training loss: 0.00001871
INFO:root:[147,   500] training loss: 0.00002700
INFO:root:[147,   550] training loss: 0.00026642
INFO:root:[147,   600] training loss: 0.00012614
INFO:root:[147,   650] training loss: 0.00002963
INFO:root:[147,   700] training loss: 0.00002428
INFO:root:[147,   750] training loss: 0.00040018
INFO:root:[147,   800] training loss: 0.00044899
INFO:root:[147,   850] training loss: 0.00047637
INFO:root:[147,   900] training loss: 0.00524254
INFO:root:[147,   950] training loss: 0.00163497
INFO:root:[147,  1000] training loss: 0.00003542
INFO:root:[147,  1050] training loss: 0.00009452
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch147
INFO:root:[148,    50] training loss: 0.00803341
INFO:root:[148,   100] training loss: 0.00756133
INFO:root:[148,   150] training loss: 0.00748586
INFO:root:[148,   200] training loss: 0.00672852
INFO:root:[148,   250] training loss: 0.00657639
INFO:root:[148,   300] training loss: 0.00824473
INFO:root:[148,   350] training loss: 0.00686315
INFO:root:[148,   400] training loss: 0.00002185
INFO:root:[148,   450] training loss: 0.00001990
INFO:root:[148,   500] training loss: 0.00002444
INFO:root:[148,   550] training loss: 0.00024739
INFO:root:[148,   600] training loss: 0.00015148
INFO:root:[148,   650] training loss: 0.00001996
INFO:root:[148,   700] training loss: 0.00001859
INFO:root:[148,   750] training loss: 0.00028410
INFO:root:[148,   800] training loss: 0.00039418
INFO:root:[148,   850] training loss: 0.00043913
INFO:root:[148,   900] training loss: 0.00493688
INFO:root:[148,   950] training loss: 0.00140128
INFO:root:[148,  1000] training loss: 0.00002846
INFO:root:[148,  1050] training loss: 0.00002045
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch148
INFO:root:[149,    50] training loss: 0.00787795
INFO:root:[149,   100] training loss: 0.00744724
INFO:root:[149,   150] training loss: 0.00732743
INFO:root:[149,   200] training loss: 0.00684204
INFO:root:[149,   250] training loss: 0.00588784
INFO:root:[149,   300] training loss: 0.00804866
INFO:root:[149,   350] training loss: 0.00675085
INFO:root:[149,   400] training loss: 0.00001837
INFO:root:[149,   450] training loss: 0.00001793
INFO:root:[149,   500] training loss: 0.00002860
INFO:root:[149,   550] training loss: 0.00034295
INFO:root:[149,   600] training loss: 0.00018110
INFO:root:[149,   650] training loss: 0.00002464
INFO:root:[149,   700] training loss: 0.00002103
INFO:root:[149,   750] training loss: 0.00041687
INFO:root:[149,   800] training loss: 0.00042408
INFO:root:[149,   850] training loss: 0.00048911
INFO:root:[149,   900] training loss: 0.00524187
INFO:root:[149,   950] training loss: 0.00142763
INFO:root:[149,  1000] training loss: 0.00003603
INFO:root:[149,  1050] training loss: 0.00002612
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch149
INFO:root:[150,    50] training loss: 0.00773815
INFO:root:[150,   100] training loss: 0.00741373
INFO:root:[150,   150] training loss: 0.00910093
INFO:root:[150,   200] training loss: 0.00718924
INFO:root:[150,   250] training loss: 0.00711803
INFO:root:[150,   300] training loss: 0.00790362
INFO:root:[150,   350] training loss: 0.00612701
INFO:root:[150,   400] training loss: 0.00001556
INFO:root:[150,   450] training loss: 0.00001853
INFO:root:[150,   500] training loss: 0.00002523
INFO:root:[150,   550] training loss: 0.00025182
INFO:root:[150,   600] training loss: 0.00017747
INFO:root:[150,   650] training loss: 0.00002291
INFO:root:[150,   700] training loss: 0.00001779
INFO:root:[150,   750] training loss: 0.00027167
INFO:root:[150,   800] training loss: 0.00049404
INFO:root:[150,   850] training loss: 0.00040613
INFO:root:[150,   900] training loss: 0.00581661
INFO:root:[150,   950] training loss: 0.00150828
INFO:root:[150,  1000] training loss: 0.00003669
INFO:root:[150,  1050] training loss: 0.00002313
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch150
INFO:root:[151,    50] training loss: 0.00810342
INFO:root:[151,   100] training loss: 0.00712751
INFO:root:[151,   150] training loss: 0.00760214
INFO:root:[151,   200] training loss: 0.00779389
INFO:root:[151,   250] training loss: 0.00597620
INFO:root:[151,   300] training loss: 0.00795489
INFO:root:[151,   350] training loss: 0.00698445
INFO:root:[151,   400] training loss: 0.00001578
INFO:root:[151,   450] training loss: 0.00003318
INFO:root:[151,   500] training loss: 0.00003042
INFO:root:[151,   550] training loss: 0.00024060
INFO:root:[151,   600] training loss: 0.00018116
INFO:root:[151,   650] training loss: 0.00002594
INFO:root:[151,   700] training loss: 0.00002607
INFO:root:[151,   750] training loss: 0.00032252
INFO:root:[151,   800] training loss: 0.00050049
INFO:root:[151,   850] training loss: 0.00054954
INFO:root:[151,   900] training loss: 0.00626012
INFO:root:[151,   950] training loss: 0.00196433
INFO:root:[151,  1000] training loss: 0.00008445
INFO:root:[151,  1050] training loss: 0.00004135
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch151
INFO:root:[152,    50] training loss: 0.00846695
INFO:root:[152,   100] training loss: 0.00794314
INFO:root:[152,   150] training loss: 0.00765431
INFO:root:[152,   200] training loss: 0.00706777
INFO:root:[152,   250] training loss: 0.00640324
INFO:root:[152,   300] training loss: 0.00796510
INFO:root:[152,   350] training loss: 0.00683253
INFO:root:[152,   400] training loss: 0.00001496
INFO:root:[152,   450] training loss: 0.00001828
INFO:root:[152,   500] training loss: 0.00002343
INFO:root:[152,   550] training loss: 0.00027295
INFO:root:[152,   600] training loss: 0.00014572
INFO:root:[152,   650] training loss: 0.00002584
INFO:root:[152,   700] training loss: 0.00003619
INFO:root:[152,   750] training loss: 0.00045653
INFO:root:[152,   800] training loss: 0.00049935
INFO:root:[152,   850] training loss: 0.00041192
INFO:root:[152,   900] training loss: 0.00560430
INFO:root:[152,   950] training loss: 0.00180687
INFO:root:[152,  1000] training loss: 0.00003684
INFO:root:[152,  1050] training loss: 0.00002016
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch152
INFO:root:[153,    50] training loss: 0.00821497
INFO:root:[153,   100] training loss: 0.00804062
INFO:root:[153,   150] training loss: 0.00779037
INFO:root:[153,   200] training loss: 0.00669564
INFO:root:[153,   250] training loss: 0.00612448
INFO:root:[153,   300] training loss: 0.00773223
INFO:root:[153,   350] training loss: 0.00682437
INFO:root:[153,   400] training loss: 0.00002516
INFO:root:[153,   450] training loss: 0.00001846
INFO:root:[153,   500] training loss: 0.00002623
INFO:root:[153,   550] training loss: 0.00031862
INFO:root:[153,   600] training loss: 0.00014551
INFO:root:[153,   650] training loss: 0.00002978
INFO:root:[153,   700] training loss: 0.00002044
INFO:root:[153,   750] training loss: 0.00041283
INFO:root:[153,   800] training loss: 0.00046538
INFO:root:[153,   850] training loss: 0.00043274
INFO:root:[153,   900] training loss: 0.00515403
INFO:root:[153,   950] training loss: 0.00186584
INFO:root:[153,  1000] training loss: 0.00002929
INFO:root:[153,  1050] training loss: 0.00002686
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch153
INFO:root:[154,    50] training loss: 0.00774832
INFO:root:[154,   100] training loss: 0.00705113
INFO:root:[154,   150] training loss: 0.00764124
INFO:root:[154,   200] training loss: 0.00677429
INFO:root:[154,   250] training loss: 0.00620448
INFO:root:[154,   300] training loss: 0.00824862
INFO:root:[154,   350] training loss: 0.00663468
INFO:root:[154,   400] training loss: 0.00001998
INFO:root:[154,   450] training loss: 0.00002066
INFO:root:[154,   500] training loss: 0.00002882
INFO:root:[154,   550] training loss: 0.00033366
INFO:root:[154,   600] training loss: 0.00021922
INFO:root:[154,   650] training loss: 0.00002630
INFO:root:[154,   700] training loss: 0.00001837
INFO:root:[154,   750] training loss: 0.00033455
INFO:root:[154,   800] training loss: 0.00058571
INFO:root:[154,   850] training loss: 0.00050912
INFO:root:[154,   900] training loss: 0.00695125
INFO:root:[154,   950] training loss: 0.00165818
INFO:root:[154,  1000] training loss: 0.00003015
INFO:root:[154,  1050] training loss: 0.00002440
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch154
INFO:root:[155,    50] training loss: 0.00774665
INFO:root:[155,   100] training loss: 0.00816856
INFO:root:[155,   150] training loss: 0.00980103
INFO:root:[155,   200] training loss: 0.00700475
INFO:root:[155,   250] training loss: 0.00701992
INFO:root:[155,   300] training loss: 0.00861869
INFO:root:[155,   350] training loss: 0.00625023
INFO:root:[155,   400] training loss: 0.00001740
INFO:root:[155,   450] training loss: 0.00002254
INFO:root:[155,   500] training loss: 0.00002853
INFO:root:[155,   550] training loss: 0.00035069
INFO:root:[155,   600] training loss: 0.00015754
INFO:root:[155,   650] training loss: 0.00002208
INFO:root:[155,   700] training loss: 0.00001737
INFO:root:[155,   750] training loss: 0.00024848
INFO:root:[155,   800] training loss: 0.00041224
INFO:root:[155,   850] training loss: 0.00072045
INFO:root:[155,   900] training loss: 0.00588349
INFO:root:[155,   950] training loss: 0.00175792
INFO:root:[155,  1000] training loss: 0.00004043
INFO:root:[155,  1050] training loss: 0.00001776
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch155
INFO:root:[156,    50] training loss: 0.00833141
INFO:root:[156,   100] training loss: 0.00845721
INFO:root:[156,   150] training loss: 0.00810119
INFO:root:[156,   200] training loss: 0.00665643
INFO:root:[156,   250] training loss: 0.00670050
INFO:root:[156,   300] training loss: 0.00789221
INFO:root:[156,   350] training loss: 0.00636465
INFO:root:[156,   400] training loss: 0.00001798
INFO:root:[156,   450] training loss: 0.00001889
INFO:root:[156,   500] training loss: 0.00002860
INFO:root:[156,   550] training loss: 0.00033756
INFO:root:[156,   600] training loss: 0.00013769
INFO:root:[156,   650] training loss: 0.00002335
INFO:root:[156,   700] training loss: 0.00001834
INFO:root:[156,   750] training loss: 0.00027305
INFO:root:[156,   800] training loss: 0.00042197
INFO:root:[156,   850] training loss: 0.00049152
INFO:root:[156,   900] training loss: 0.00491480
INFO:root:[156,   950] training loss: 0.00134014
INFO:root:[156,  1000] training loss: 0.00002637
INFO:root:[156,  1050] training loss: 0.00001950
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch156
INFO:root:[157,    50] training loss: 0.00819949
INFO:root:[157,   100] training loss: 0.00738224
INFO:root:[157,   150] training loss: 0.00783380
INFO:root:[157,   200] training loss: 0.00912003
INFO:root:[157,   250] training loss: 0.00637751
INFO:root:[157,   300] training loss: 0.00742940
INFO:root:[157,   350] training loss: 0.00655430
INFO:root:[157,   400] training loss: 0.00001738
INFO:root:[157,   450] training loss: 0.00001613
INFO:root:[157,   500] training loss: 0.00002515
INFO:root:[157,   550] training loss: 0.00037087
INFO:root:[157,   600] training loss: 0.00013000
INFO:root:[157,   650] training loss: 0.00002257
INFO:root:[157,   700] training loss: 0.00003073
INFO:root:[157,   750] training loss: 0.00030955
INFO:root:[157,   800] training loss: 0.00054779
INFO:root:[157,   850] training loss: 0.00048769
INFO:root:[157,   900] training loss: 0.00553375
INFO:root:[157,   950] training loss: 0.00189121
INFO:root:[157,  1000] training loss: 0.00005424
INFO:root:[157,  1050] training loss: 0.00002385
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch157
INFO:root:[158,    50] training loss: 0.00794574
INFO:root:[158,   100] training loss: 0.00785386
INFO:root:[158,   150] training loss: 0.00778157
INFO:root:[158,   200] training loss: 0.00666536
INFO:root:[158,   250] training loss: 0.00644144
INFO:root:[158,   300] training loss: 0.00773068
INFO:root:[158,   350] training loss: 0.00613711
INFO:root:[158,   400] training loss: 0.00001633
INFO:root:[158,   450] training loss: 0.00001693
INFO:root:[158,   500] training loss: 0.00002899
INFO:root:[158,   550] training loss: 0.00031023
INFO:root:[158,   600] training loss: 0.00018377
INFO:root:[158,   650] training loss: 0.00002414
INFO:root:[158,   700] training loss: 0.00002770
INFO:root:[158,   750] training loss: 0.00038705
INFO:root:[158,   800] training loss: 0.00045820
INFO:root:[158,   850] training loss: 0.00049262
INFO:root:[158,   900] training loss: 0.00763582
INFO:root:[158,   950] training loss: 0.00143330
INFO:root:[158,  1000] training loss: 0.00002734
INFO:root:[158,  1050] training loss: 0.00002261
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch158
INFO:root:[159,    50] training loss: 0.00814721
INFO:root:[159,   100] training loss: 0.00764439
INFO:root:[159,   150] training loss: 0.00845757
INFO:root:[159,   200] training loss: 0.00785259
INFO:root:[159,   250] training loss: 0.00630286
INFO:root:[159,   300] training loss: 0.00781134
INFO:root:[159,   350] training loss: 0.00681562
INFO:root:[159,   400] training loss: 0.00001525
INFO:root:[159,   450] training loss: 0.00007582
INFO:root:[159,   500] training loss: 0.00003047
INFO:root:[159,   550] training loss: 0.00027129
INFO:root:[159,   600] training loss: 0.00015953
INFO:root:[159,   650] training loss: 0.00003309
INFO:root:[159,   700] training loss: 0.00001704
INFO:root:[159,   750] training loss: 0.00033532
INFO:root:[159,   800] training loss: 0.00044502
INFO:root:[159,   850] training loss: 0.00056867
INFO:root:[159,   900] training loss: 0.00561827
INFO:root:[159,   950] training loss: 0.00150061
INFO:root:[159,  1000] training loss: 0.00004604
INFO:root:[159,  1050] training loss: 0.00002581
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch159
INFO:root:[160,    50] training loss: 0.00827427
INFO:root:[160,   100] training loss: 0.00878099
INFO:root:[160,   150] training loss: 0.00830831
INFO:root:[160,   200] training loss: 0.00712413
INFO:root:[160,   250] training loss: 0.00630941
INFO:root:[160,   300] training loss: 0.00765176
INFO:root:[160,   350] training loss: 0.00669986
INFO:root:[160,   400] training loss: 0.00001901
INFO:root:[160,   450] training loss: 0.00001672
INFO:root:[160,   500] training loss: 0.00004089
INFO:root:[160,   550] training loss: 0.00038635
INFO:root:[160,   600] training loss: 0.00016536
INFO:root:[160,   650] training loss: 0.00002153
INFO:root:[160,   700] training loss: 0.00003114
INFO:root:[160,   750] training loss: 0.00038062
INFO:root:[160,   800] training loss: 0.00063660
INFO:root:[160,   850] training loss: 0.00044910
INFO:root:[160,   900] training loss: 0.00530205
INFO:root:[160,   950] training loss: 0.00153475
INFO:root:[160,  1000] training loss: 0.00002821
INFO:root:[160,  1050] training loss: 0.00002158
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch160
INFO:root:[161,    50] training loss: 0.00853218
INFO:root:[161,   100] training loss: 0.00789512
INFO:root:[161,   150] training loss: 0.00772529
INFO:root:[161,   200] training loss: 0.00686418
INFO:root:[161,   250] training loss: 0.00689832
INFO:root:[161,   300] training loss: 0.00796049
INFO:root:[161,   350] training loss: 0.00685876
INFO:root:[161,   400] training loss: 0.00001570
INFO:root:[161,   450] training loss: 0.00001796
INFO:root:[161,   500] training loss: 0.00002509
INFO:root:[161,   550] training loss: 0.00034790
INFO:root:[161,   600] training loss: 0.00015938
INFO:root:[161,   650] training loss: 0.00003285
INFO:root:[161,   700] training loss: 0.00003389
INFO:root:[161,   750] training loss: 0.00034270
INFO:root:[161,   800] training loss: 0.00039526
INFO:root:[161,   850] training loss: 0.00063132
INFO:root:[161,   900] training loss: 0.00489300
INFO:root:[161,   950] training loss: 0.00178093
INFO:root:[161,  1000] training loss: 0.00002679
INFO:root:[161,  1050] training loss: 0.00002353
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch161
INFO:root:[162,    50] training loss: 0.00856074
INFO:root:[162,   100] training loss: 0.00780630
INFO:root:[162,   150] training loss: 0.00792684
INFO:root:[162,   200] training loss: 0.00664273
INFO:root:[162,   250] training loss: 0.00659200
INFO:root:[162,   300] training loss: 0.00828301
INFO:root:[162,   350] training loss: 0.00667881
INFO:root:[162,   400] training loss: 0.00002044
INFO:root:[162,   450] training loss: 0.00001815
INFO:root:[162,   500] training loss: 0.00002413
INFO:root:[162,   550] training loss: 0.00031983
INFO:root:[162,   600] training loss: 0.00018227
INFO:root:[162,   650] training loss: 0.00002326
INFO:root:[162,   700] training loss: 0.00002377
INFO:root:[162,   750] training loss: 0.00033167
INFO:root:[162,   800] training loss: 0.00047832
INFO:root:[162,   850] training loss: 0.00051830
INFO:root:[162,   900] training loss: 0.00540170
INFO:root:[162,   950] training loss: 0.00145394
INFO:root:[162,  1000] training loss: 0.00002832
INFO:root:[162,  1050] training loss: 0.00002312
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch162
INFO:root:[163,    50] training loss: 0.00832944
INFO:root:[163,   100] training loss: 0.00771622
INFO:root:[163,   150] training loss: 0.00761001
INFO:root:[163,   200] training loss: 0.00701036
INFO:root:[163,   250] training loss: 0.00609439
INFO:root:[163,   300] training loss: 0.00759862
INFO:root:[163,   350] training loss: 0.00625125
INFO:root:[163,   400] training loss: 0.00001566
INFO:root:[163,   450] training loss: 0.00003629
INFO:root:[163,   500] training loss: 0.00004393
INFO:root:[163,   550] training loss: 0.00031680
INFO:root:[163,   600] training loss: 0.00018215
INFO:root:[163,   650] training loss: 0.00002961
INFO:root:[163,   700] training loss: 0.00002286
INFO:root:[163,   750] training loss: 0.00028953
INFO:root:[163,   800] training loss: 0.00044996
INFO:root:[163,   850] training loss: 0.00049983
INFO:root:[163,   900] training loss: 0.00556689
INFO:root:[163,   950] training loss: 0.00138114
INFO:root:[163,  1000] training loss: 0.00003128
INFO:root:[163,  1050] training loss: 0.00002715
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch163
INFO:root:[164,    50] training loss: 0.00838881
INFO:root:[164,   100] training loss: 0.00748229
INFO:root:[164,   150] training loss: 0.00762916
INFO:root:[164,   200] training loss: 0.00679122
INFO:root:[164,   250] training loss: 0.00712044
INFO:root:[164,   300] training loss: 0.00799068
INFO:root:[164,   350] training loss: 0.00625390
INFO:root:[164,   400] training loss: 0.00005144
INFO:root:[164,   450] training loss: 0.00001693
INFO:root:[164,   500] training loss: 0.00002539
INFO:root:[164,   550] training loss: 0.00025110
INFO:root:[164,   600] training loss: 0.00011439
INFO:root:[164,   650] training loss: 0.00002271
INFO:root:[164,   700] training loss: 0.00001759
INFO:root:[164,   750] training loss: 0.00029099
INFO:root:[164,   800] training loss: 0.00046442
INFO:root:[164,   850] training loss: 0.00050805
INFO:root:[164,   900] training loss: 0.00603247
INFO:root:[164,   950] training loss: 0.00150115
INFO:root:[164,  1000] training loss: 0.00003199
INFO:root:[164,  1050] training loss: 0.00002124
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch164
INFO:root:[165,    50] training loss: 0.00821280
INFO:root:[165,   100] training loss: 0.00744518
INFO:root:[165,   150] training loss: 0.00786598
INFO:root:[165,   200] training loss: 0.00684318
INFO:root:[165,   250] training loss: 0.00641779
INFO:root:[165,   300] training loss: 0.00815127
INFO:root:[165,   350] training loss: 0.00667044
INFO:root:[165,   400] training loss: 0.00001602
INFO:root:[165,   450] training loss: 0.00001681
INFO:root:[165,   500] training loss: 0.00002351
INFO:root:[165,   550] training loss: 0.00028005
INFO:root:[165,   600] training loss: 0.00019412
INFO:root:[165,   650] training loss: 0.00001957
INFO:root:[165,   700] training loss: 0.00002267
INFO:root:[165,   750] training loss: 0.00031766
INFO:root:[165,   800] training loss: 0.00045158
INFO:root:[165,   850] training loss: 0.00055004
INFO:root:[165,   900] training loss: 0.00596748
INFO:root:[165,   950] training loss: 0.00155274
INFO:root:[165,  1000] training loss: 0.00004347
INFO:root:[165,  1050] training loss: 0.00001961
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch165
INFO:root:[166,    50] training loss: 0.00836974
INFO:root:[166,   100] training loss: 0.00746143
INFO:root:[166,   150] training loss: 0.00774538
INFO:root:[166,   200] training loss: 0.00682291
INFO:root:[166,   250] training loss: 0.00615862
INFO:root:[166,   300] training loss: 0.00752124
INFO:root:[166,   350] training loss: 0.00652241
INFO:root:[166,   400] training loss: 0.00001659
INFO:root:[166,   450] training loss: 0.00001640
INFO:root:[166,   500] training loss: 0.00002711
INFO:root:[166,   550] training loss: 0.00023055
INFO:root:[166,   600] training loss: 0.00012412
INFO:root:[166,   650] training loss: 0.00002718
INFO:root:[166,   700] training loss: 0.00001858
INFO:root:[166,   750] training loss: 0.00031328
INFO:root:[166,   800] training loss: 0.00042059
INFO:root:[166,   850] training loss: 0.00057032
INFO:root:[166,   900] training loss: 0.00589654
INFO:root:[166,   950] training loss: 0.00170452
INFO:root:[166,  1000] training loss: 0.00002879
INFO:root:[166,  1050] training loss: 0.00002190
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch166
INFO:root:[167,    50] training loss: 0.00827689
INFO:root:[167,   100] training loss: 0.00731985
INFO:root:[167,   150] training loss: 0.00757635
INFO:root:[167,   200] training loss: 0.00672064
INFO:root:[167,   250] training loss: 0.00652876
INFO:root:[167,   300] training loss: 0.00849959
INFO:root:[167,   350] training loss: 0.00664095
INFO:root:[167,   400] training loss: 0.00001653
INFO:root:[167,   450] training loss: 0.00002586
INFO:root:[167,   500] training loss: 0.00002917
INFO:root:[167,   550] training loss: 0.00024456
INFO:root:[167,   600] training loss: 0.00016063
INFO:root:[167,   650] training loss: 0.00002092
INFO:root:[167,   700] training loss: 0.00001788
INFO:root:[167,   750] training loss: 0.00037255
INFO:root:[167,   800] training loss: 0.00045337
INFO:root:[167,   850] training loss: 0.00072387
INFO:root:[167,   900] training loss: 0.00616285
INFO:root:[167,   950] training loss: 0.00174898
INFO:root:[167,  1000] training loss: 0.00002760
INFO:root:[167,  1050] training loss: 0.00002420
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch167
INFO:root:[168,    50] training loss: 0.00837330
INFO:root:[168,   100] training loss: 0.00751860
INFO:root:[168,   150] training loss: 0.00807654
INFO:root:[168,   200] training loss: 0.00674290
INFO:root:[168,   250] training loss: 0.00628851
INFO:root:[168,   300] training loss: 0.00803079
INFO:root:[168,   350] training loss: 0.00644574
INFO:root:[168,   400] training loss: 0.00004346
INFO:root:[168,   450] training loss: 0.00002119
INFO:root:[168,   500] training loss: 0.00002299
INFO:root:[168,   550] training loss: 0.00029050
INFO:root:[168,   600] training loss: 0.00013205
INFO:root:[168,   650] training loss: 0.00002164
INFO:root:[168,   700] training loss: 0.00001791
INFO:root:[168,   750] training loss: 0.00039604
INFO:root:[168,   800] training loss: 0.00046114
INFO:root:[168,   850] training loss: 0.00055563
INFO:root:[168,   900] training loss: 0.00604656
INFO:root:[168,   950] training loss: 0.00182348
INFO:root:[168,  1000] training loss: 0.00003272
INFO:root:[168,  1050] training loss: 0.00002620
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch168
INFO:root:[169,    50] training loss: 0.00830728
INFO:root:[169,   100] training loss: 0.00748770
INFO:root:[169,   150] training loss: 0.00764312
INFO:root:[169,   200] training loss: 0.00675924
INFO:root:[169,   250] training loss: 0.00799685
INFO:root:[169,   300] training loss: 0.00799748
INFO:root:[169,   350] training loss: 0.00637768
INFO:root:[169,   400] training loss: 0.00001985
INFO:root:[169,   450] training loss: 0.00001908
INFO:root:[169,   500] training loss: 0.00003655
INFO:root:[169,   550] training loss: 0.00026730
INFO:root:[169,   600] training loss: 0.00020758
INFO:root:[169,   650] training loss: 0.00003849
INFO:root:[169,   700] training loss: 0.00001990
INFO:root:[169,   750] training loss: 0.00032399
INFO:root:[169,   800] training loss: 0.00063713
INFO:root:[169,   850] training loss: 0.00051373
INFO:root:[169,   900] training loss: 0.00538586
INFO:root:[169,   950] training loss: 0.00180651
INFO:root:[169,  1000] training loss: 0.00002850
INFO:root:[169,  1050] training loss: 0.00002227
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch169
INFO:root:[170,    50] training loss: 0.00797891
INFO:root:[170,   100] training loss: 0.00721117
INFO:root:[170,   150] training loss: 0.00793663
INFO:root:[170,   200] training loss: 0.00877027
INFO:root:[170,   250] training loss: 0.00696885
INFO:root:[170,   300] training loss: 0.00770569
INFO:root:[170,   350] training loss: 0.00638264
INFO:root:[170,   400] training loss: 0.00001682
INFO:root:[170,   450] training loss: 0.00002032
INFO:root:[170,   500] training loss: 0.00003098
INFO:root:[170,   550] training loss: 0.00027283
INFO:root:[170,   600] training loss: 0.00014956
INFO:root:[170,   650] training loss: 0.00002908
INFO:root:[170,   700] training loss: 0.00003081
INFO:root:[170,   750] training loss: 0.00031522
INFO:root:[170,   800] training loss: 0.00045302
INFO:root:[170,   850] training loss: 0.00066293
INFO:root:[170,   900] training loss: 0.00484890
INFO:root:[170,   950] training loss: 0.00152829
INFO:root:[170,  1000] training loss: 0.00002814
INFO:root:[170,  1050] training loss: 0.00002452
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch170
INFO:root:[171,    50] training loss: 0.00827561
INFO:root:[171,   100] training loss: 0.00780617
INFO:root:[171,   150] training loss: 0.00780147
INFO:root:[171,   200] training loss: 0.00650906
INFO:root:[171,   250] training loss: 0.00653365
INFO:root:[171,   300] training loss: 0.00785202
INFO:root:[171,   350] training loss: 0.00668720
INFO:root:[171,   400] training loss: 0.00002028
INFO:root:[171,   450] training loss: 0.00001709
INFO:root:[171,   500] training loss: 0.00004636
INFO:root:[171,   550] training loss: 0.00035121
INFO:root:[171,   600] training loss: 0.00019920
INFO:root:[171,   650] training loss: 0.00003044
INFO:root:[171,   700] training loss: 0.00001973
INFO:root:[171,   750] training loss: 0.00035381
INFO:root:[171,   800] training loss: 0.00051067
INFO:root:[171,   850] training loss: 0.00045182
INFO:root:[171,   900] training loss: 0.00523180
INFO:root:[171,   950] training loss: 0.00145622
INFO:root:[171,  1000] training loss: 0.00003800
INFO:root:[171,  1050] training loss: 0.00002141
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch171
INFO:root:[172,    50] training loss: 0.00793144
INFO:root:[172,   100] training loss: 0.00735672
INFO:root:[172,   150] training loss: 0.00745695
INFO:root:[172,   200] training loss: 0.00683430
INFO:root:[172,   250] training loss: 0.00613081
INFO:root:[172,   300] training loss: 0.00804752
INFO:root:[172,   350] training loss: 0.00626052
INFO:root:[172,   400] training loss: 0.00002234
INFO:root:[172,   450] training loss: 0.00001886
INFO:root:[172,   500] training loss: 0.00002574
INFO:root:[172,   550] training loss: 0.00038367
INFO:root:[172,   600] training loss: 0.00013804
INFO:root:[172,   650] training loss: 0.00002430
INFO:root:[172,   700] training loss: 0.00002250
INFO:root:[172,   750] training loss: 0.00039489
INFO:root:[172,   800] training loss: 0.00049284
INFO:root:[172,   850] training loss: 0.00061931
INFO:root:[172,   900] training loss: 0.00577654
INFO:root:[172,   950] training loss: 0.00174392
INFO:root:[172,  1000] training loss: 0.00002735
INFO:root:[172,  1050] training loss: 0.00002308
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch172
INFO:root:[173,    50] training loss: 0.00854342
INFO:root:[173,   100] training loss: 0.00759706
INFO:root:[173,   150] training loss: 0.00781452
INFO:root:[173,   200] training loss: 0.00711411
INFO:root:[173,   250] training loss: 0.00835263
INFO:root:[173,   300] training loss: 0.00783481
INFO:root:[173,   350] training loss: 0.00658185
INFO:root:[173,   400] training loss: 0.00001630
INFO:root:[173,   450] training loss: 0.00001879
INFO:root:[173,   500] training loss: 0.00002842
INFO:root:[173,   550] training loss: 0.00026594
INFO:root:[173,   600] training loss: 0.00017308
INFO:root:[173,   650] training loss: 0.00002107
INFO:root:[173,   700] training loss: 0.00002203
INFO:root:[173,   750] training loss: 0.00038638
INFO:root:[173,   800] training loss: 0.00051015
INFO:root:[173,   850] training loss: 0.00050671
INFO:root:[173,   900] training loss: 0.00531395
INFO:root:[173,   950] training loss: 0.00133094
INFO:root:[173,  1000] training loss: 0.00004451
INFO:root:[173,  1050] training loss: 0.00002643
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch173
INFO:root:[174,    50] training loss: 0.00811016
INFO:root:[174,   100] training loss: 0.00765097
INFO:root:[174,   150] training loss: 0.00772975
INFO:root:[174,   200] training loss: 0.00667004
INFO:root:[174,   250] training loss: 0.00642014
INFO:root:[174,   300] training loss: 0.00774150
INFO:root:[174,   350] training loss: 0.00657050
INFO:root:[174,   400] training loss: 0.00002711
INFO:root:[174,   450] training loss: 0.00001893
INFO:root:[174,   500] training loss: 0.00002367
INFO:root:[174,   550] training loss: 0.00031353
INFO:root:[174,   600] training loss: 0.00015977
INFO:root:[174,   650] training loss: 0.00002801
INFO:root:[174,   700] training loss: 0.00001902
INFO:root:[174,   750] training loss: 0.00025703
INFO:root:[174,   800] training loss: 0.00038710
INFO:root:[174,   850] training loss: 0.00051090
INFO:root:[174,   900] training loss: 0.00564629
INFO:root:[174,   950] training loss: 0.00136609
INFO:root:[174,  1000] training loss: 0.00003667
INFO:root:[174,  1050] training loss: 0.00002253
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch174
INFO:root:[175,    50] training loss: 0.00814755
INFO:root:[175,   100] training loss: 0.00903135
INFO:root:[175,   150] training loss: 0.00845193
INFO:root:[175,   200] training loss: 0.00689876
INFO:root:[175,   250] training loss: 0.00627864
INFO:root:[175,   300] training loss: 0.00778664
INFO:root:[175,   350] training loss: 0.00628164
INFO:root:[175,   400] training loss: 0.00001446
INFO:root:[175,   450] training loss: 0.00003244
INFO:root:[175,   500] training loss: 0.00002641
INFO:root:[175,   550] training loss: 0.00032722
INFO:root:[175,   600] training loss: 0.00019956
INFO:root:[175,   650] training loss: 0.00002223
INFO:root:[175,   700] training loss: 0.00002416
INFO:root:[175,   750] training loss: 0.00030889
INFO:root:[175,   800] training loss: 0.00043012
INFO:root:[175,   850] training loss: 0.00047853
INFO:root:[175,   900] training loss: 0.00529429
INFO:root:[175,   950] training loss: 0.00171168
INFO:root:[175,  1000] training loss: 0.00003216
INFO:root:[175,  1050] training loss: 0.00002831
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch175
INFO:root:[176,    50] training loss: 0.00810362
INFO:root:[176,   100] training loss: 0.00749862
INFO:root:[176,   150] training loss: 0.00797058
INFO:root:[176,   200] training loss: 0.00715622
INFO:root:[176,   250] training loss: 0.00612152
INFO:root:[176,   300] training loss: 0.00795402
INFO:root:[176,   350] training loss: 0.00691400
INFO:root:[176,   400] training loss: 0.00001693
INFO:root:[176,   450] training loss: 0.00001811
INFO:root:[176,   500] training loss: 0.00002684
INFO:root:[176,   550] training loss: 0.00023251
INFO:root:[176,   600] training loss: 0.00019484
INFO:root:[176,   650] training loss: 0.00002029
INFO:root:[176,   700] training loss: 0.00001872
INFO:root:[176,   750] training loss: 0.00028437
INFO:root:[176,   800] training loss: 0.00057704
INFO:root:[176,   850] training loss: 0.00053042
INFO:root:[176,   900] training loss: 0.00584835
INFO:root:[176,   950] training loss: 0.00137199
INFO:root:[176,  1000] training loss: 0.00002677
INFO:root:[176,  1050] training loss: 0.00001930
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch176
INFO:root:[177,    50] training loss: 0.00871918
INFO:root:[177,   100] training loss: 0.00738219
INFO:root:[177,   150] training loss: 0.00835641
INFO:root:[177,   200] training loss: 0.00691384
INFO:root:[177,   250] training loss: 0.00643003
INFO:root:[177,   300] training loss: 0.00821544
INFO:root:[177,   350] training loss: 0.00639077
INFO:root:[177,   400] training loss: 0.00001569
INFO:root:[177,   450] training loss: 0.00001941
INFO:root:[177,   500] training loss: 0.00002402
INFO:root:[177,   550] training loss: 0.00028971
INFO:root:[177,   600] training loss: 0.00017577
INFO:root:[177,   650] training loss: 0.00002157
INFO:root:[177,   700] training loss: 0.00002834
INFO:root:[177,   750] training loss: 0.00043867
INFO:root:[177,   800] training loss: 0.00056364
INFO:root:[177,   850] training loss: 0.00051537
INFO:root:[177,   900] training loss: 0.00578829
INFO:root:[177,   950] training loss: 0.00150828
INFO:root:[177,  1000] training loss: 0.00005085
INFO:root:[177,  1050] training loss: 0.00002341
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch177
INFO:root:[178,    50] training loss: 0.00870276
INFO:root:[178,   100] training loss: 0.00751785
INFO:root:[178,   150] training loss: 0.00748553
INFO:root:[178,   200] training loss: 0.00687840
INFO:root:[178,   250] training loss: 0.00955235
INFO:root:[178,   300] training loss: 0.00772673
INFO:root:[178,   350] training loss: 0.00650364
INFO:root:[178,   400] training loss: 0.00001493
INFO:root:[178,   450] training loss: 0.00002049
INFO:root:[178,   500] training loss: 0.00002458
INFO:root:[178,   550] training loss: 0.00028795
INFO:root:[178,   600] training loss: 0.00018778
INFO:root:[178,   650] training loss: 0.00002969
INFO:root:[178,   700] training loss: 0.00003433
INFO:root:[178,   750] training loss: 0.00027647
INFO:root:[178,   800] training loss: 0.00035803
INFO:root:[178,   850] training loss: 0.00053062
INFO:root:[178,   900] training loss: 0.00486867
INFO:root:[178,   950] training loss: 0.00133656
INFO:root:[178,  1000] training loss: 0.00003888
INFO:root:[178,  1050] training loss: 0.00002238
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch178
INFO:root:[179,    50] training loss: 0.00793069
INFO:root:[179,   100] training loss: 0.00738804
INFO:root:[179,   150] training loss: 0.00740103
INFO:root:[179,   200] training loss: 0.00695456
INFO:root:[179,   250] training loss: 0.00668886
INFO:root:[179,   300] training loss: 0.00797336
INFO:root:[179,   350] training loss: 0.00654262
INFO:root:[179,   400] training loss: 0.00001767
INFO:root:[179,   450] training loss: 0.00001713
INFO:root:[179,   500] training loss: 0.00002902
INFO:root:[179,   550] training loss: 0.00030113
INFO:root:[179,   600] training loss: 0.00015365
INFO:root:[179,   650] training loss: 0.00004469
INFO:root:[179,   700] training loss: 0.00002076
INFO:root:[179,   750] training loss: 0.00047789
INFO:root:[179,   800] training loss: 0.00056750
INFO:root:[179,   850] training loss: 0.00048277
INFO:root:[179,   900] training loss: 0.00511581
INFO:root:[179,   950] training loss: 0.00159915
INFO:root:[179,  1000] training loss: 0.00002678
INFO:root:[179,  1050] training loss: 0.00001974
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch179
INFO:root:[180,    50] training loss: 0.00819347
INFO:root:[180,   100] training loss: 0.00800593
INFO:root:[180,   150] training loss: 0.00785438
INFO:root:[180,   200] training loss: 0.00755932
INFO:root:[180,   250] training loss: 0.00635993
INFO:root:[180,   300] training loss: 0.00783300
INFO:root:[180,   350] training loss: 0.00640203
INFO:root:[180,   400] training loss: 0.00001859
INFO:root:[180,   450] training loss: 0.00003689
INFO:root:[180,   500] training loss: 0.00002524
INFO:root:[180,   550] training loss: 0.00028404
INFO:root:[180,   600] training loss: 0.00013651
INFO:root:[180,   650] training loss: 0.00002599
INFO:root:[180,   700] training loss: 0.00003000
INFO:root:[180,   750] training loss: 0.00035480
INFO:root:[180,   800] training loss: 0.00049831
INFO:root:[180,   850] training loss: 0.00051473
INFO:root:[180,   900] training loss: 0.00477669
INFO:root:[180,   950] training loss: 0.00179043
INFO:root:[180,  1000] training loss: 0.00002894
INFO:root:[180,  1050] training loss: 0.00002394
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch180
INFO:root:[181,    50] training loss: 0.00801733
INFO:root:[181,   100] training loss: 0.00847363
INFO:root:[181,   150] training loss: 0.00784503
INFO:root:[181,   200] training loss: 0.00714456
INFO:root:[181,   250] training loss: 0.00645463
INFO:root:[181,   300] training loss: 0.00790999
INFO:root:[181,   350] training loss: 0.00632784
INFO:root:[181,   400] training loss: 0.00001835
INFO:root:[181,   450] training loss: 0.00001652
INFO:root:[181,   500] training loss: 0.00002913
INFO:root:[181,   550] training loss: 0.00037375
INFO:root:[181,   600] training loss: 0.00014681
INFO:root:[181,   650] training loss: 0.00002781
INFO:root:[181,   700] training loss: 0.00001946
INFO:root:[181,   750] training loss: 0.00031848
INFO:root:[181,   800] training loss: 0.00051254
INFO:root:[181,   850] training loss: 0.00047370
INFO:root:[181,   900] training loss: 0.00528042
INFO:root:[181,   950] training loss: 0.00159425
INFO:root:[181,  1000] training loss: 0.00003935
INFO:root:[181,  1050] training loss: 0.00002228
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch181
INFO:root:[182,    50] training loss: 0.01032793
INFO:root:[182,   100] training loss: 0.00764819
INFO:root:[182,   150] training loss: 0.00790454
INFO:root:[182,   200] training loss: 0.00695529
INFO:root:[182,   250] training loss: 0.00593709
INFO:root:[182,   300] training loss: 0.00779799
INFO:root:[182,   350] training loss: 0.00631078
INFO:root:[182,   400] training loss: 0.00001484
INFO:root:[182,   450] training loss: 0.00001725
INFO:root:[182,   500] training loss: 0.00002895
INFO:root:[182,   550] training loss: 0.00026791
INFO:root:[182,   600] training loss: 0.00017432
INFO:root:[182,   650] training loss: 0.00003011
INFO:root:[182,   700] training loss: 0.00003492
INFO:root:[182,   750] training loss: 0.00030954
INFO:root:[182,   800] training loss: 0.00053605
INFO:root:[182,   850] training loss: 0.00072397
INFO:root:[182,   900] training loss: 0.00598828
INFO:root:[182,   950] training loss: 0.00148060
INFO:root:[182,  1000] training loss: 0.00002987
INFO:root:[182,  1050] training loss: 0.00002936
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch182
INFO:root:[183,    50] training loss: 0.00801579
INFO:root:[183,   100] training loss: 0.00766301
INFO:root:[183,   150] training loss: 0.00777542
INFO:root:[183,   200] training loss: 0.00728476
INFO:root:[183,   250] training loss: 0.00648346
INFO:root:[183,   300] training loss: 0.00819430
INFO:root:[183,   350] training loss: 0.00636988
INFO:root:[183,   400] training loss: 0.00001657
INFO:root:[183,   450] training loss: 0.00001955
INFO:root:[183,   500] training loss: 0.00003011
INFO:root:[183,   550] training loss: 0.00029842
INFO:root:[183,   600] training loss: 0.00011642
INFO:root:[183,   650] training loss: 0.00002142
INFO:root:[183,   700] training loss: 0.00001842
INFO:root:[183,   750] training loss: 0.00030608
INFO:root:[183,   800] training loss: 0.00055737
INFO:root:[183,   850] training loss: 0.00052628
INFO:root:[183,   900] training loss: 0.00468280
INFO:root:[183,   950] training loss: 0.00127306
INFO:root:[183,  1000] training loss: 0.00003171
INFO:root:[183,  1050] training loss: 0.00002457
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch183
INFO:root:[184,    50] training loss: 0.00781314
INFO:root:[184,   100] training loss: 0.00740695
INFO:root:[184,   150] training loss: 0.00831712
INFO:root:[184,   200] training loss: 0.00695228
INFO:root:[184,   250] training loss: 0.00665358
INFO:root:[184,   300] training loss: 0.00764058
INFO:root:[184,   350] training loss: 0.00637046
INFO:root:[184,   400] training loss: 0.00001813
INFO:root:[184,   450] training loss: 0.00003326
INFO:root:[184,   500] training loss: 0.00002509
INFO:root:[184,   550] training loss: 0.00028444
INFO:root:[184,   600] training loss: 0.00015595
INFO:root:[184,   650] training loss: 0.00002489
INFO:root:[184,   700] training loss: 0.00002075
INFO:root:[184,   750] training loss: 0.00041767
INFO:root:[184,   800] training loss: 0.00047041
INFO:root:[184,   850] training loss: 0.00040444
INFO:root:[184,   900] training loss: 0.00498923
INFO:root:[184,   950] training loss: 0.00172269
INFO:root:[184,  1000] training loss: 0.00002960
INFO:root:[184,  1050] training loss: 0.00003020
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch184
INFO:root:[185,    50] training loss: 0.00830829
INFO:root:[185,   100] training loss: 0.00736245
INFO:root:[185,   150] training loss: 0.00787860
INFO:root:[185,   200] training loss: 0.00922946
INFO:root:[185,   250] training loss: 0.00610858
INFO:root:[185,   300] training loss: 0.00778809
INFO:root:[185,   350] training loss: 0.00612442
INFO:root:[185,   400] training loss: 0.00001560
INFO:root:[185,   450] training loss: 0.00001640
INFO:root:[185,   500] training loss: 0.00003156
INFO:root:[185,   550] training loss: 0.00027878
INFO:root:[185,   600] training loss: 0.00014336
INFO:root:[185,   650] training loss: 0.00002124
INFO:root:[185,   700] training loss: 0.00001970
INFO:root:[185,   750] training loss: 0.00038343
INFO:root:[185,   800] training loss: 0.00045745
INFO:root:[185,   850] training loss: 0.00056373
INFO:root:[185,   900] training loss: 0.00570029
INFO:root:[185,   950] training loss: 0.00147466
INFO:root:[185,  1000] training loss: 0.00002887
INFO:root:[185,  1050] training loss: 0.00002173
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch185
INFO:root:[186,    50] training loss: 0.00823346
INFO:root:[186,   100] training loss: 0.00827085
INFO:root:[186,   150] training loss: 0.00787878
INFO:root:[186,   200] training loss: 0.00685972
INFO:root:[186,   250] training loss: 0.00594020
INFO:root:[186,   300] training loss: 0.00826272
INFO:root:[186,   350] training loss: 0.00666592
INFO:root:[186,   400] training loss: 0.00001745
INFO:root:[186,   450] training loss: 0.00001626
INFO:root:[186,   500] training loss: 0.00002785
INFO:root:[186,   550] training loss: 0.00030639
INFO:root:[186,   600] training loss: 0.00018976
INFO:root:[186,   650] training loss: 0.00003121
INFO:root:[186,   700] training loss: 0.00003226
INFO:root:[186,   750] training loss: 0.00035657
INFO:root:[186,   800] training loss: 0.00047725
INFO:root:[186,   850] training loss: 0.00045158
INFO:root:[186,   900] training loss: 0.00514819
INFO:root:[186,   950] training loss: 0.00146301
INFO:root:[186,  1000] training loss: 0.00004414
INFO:root:[186,  1050] training loss: 0.00002270
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch186
INFO:root:[187,    50] training loss: 0.00832023
INFO:root:[187,   100] training loss: 0.00749920
INFO:root:[187,   150] training loss: 0.00755821
INFO:root:[187,   200] training loss: 0.00692096
INFO:root:[187,   250] training loss: 0.00690210
INFO:root:[187,   300] training loss: 0.00801334
INFO:root:[187,   350] training loss: 0.00748163
INFO:root:[187,   400] training loss: 0.00001808
INFO:root:[187,   450] training loss: 0.00001836
INFO:root:[187,   500] training loss: 0.00002556
INFO:root:[187,   550] training loss: 0.00026694
INFO:root:[187,   600] training loss: 0.00013383
INFO:root:[187,   650] training loss: 0.00001828
INFO:root:[187,   700] training loss: 0.00001977
INFO:root:[187,   750] training loss: 0.00039137
INFO:root:[187,   800] training loss: 0.00039748
INFO:root:[187,   850] training loss: 0.00050119
INFO:root:[187,   900] training loss: 0.00461199
INFO:root:[187,   950] training loss: 0.00185619
INFO:root:[187,  1000] training loss: 0.00003016
INFO:root:[187,  1050] training loss: 0.00002555
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch187
INFO:root:[188,    50] training loss: 0.00863851
INFO:root:[188,   100] training loss: 0.00753072
INFO:root:[188,   150] training loss: 0.00809496
INFO:root:[188,   200] training loss: 0.00673955
INFO:root:[188,   250] training loss: 0.00665777
INFO:root:[188,   300] training loss: 0.00918304
INFO:root:[188,   350] training loss: 0.00666653
INFO:root:[188,   400] training loss: 0.00001644
INFO:root:[188,   450] training loss: 0.00001438
INFO:root:[188,   500] training loss: 0.00002692
INFO:root:[188,   550] training loss: 0.00026283
INFO:root:[188,   600] training loss: 0.00015329
INFO:root:[188,   650] training loss: 0.00002339
INFO:root:[188,   700] training loss: 0.00001914
INFO:root:[188,   750] training loss: 0.00029697
INFO:root:[188,   800] training loss: 0.00036819
INFO:root:[188,   850] training loss: 0.00049943
INFO:root:[188,   900] training loss: 0.00570907
INFO:root:[188,   950] training loss: 0.00152817
INFO:root:[188,  1000] training loss: 0.00002674
INFO:root:[188,  1050] training loss: 0.00001944
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch188
INFO:root:[189,    50] training loss: 0.00814608
INFO:root:[189,   100] training loss: 0.00746396
INFO:root:[189,   150] training loss: 0.00750663
INFO:root:[189,   200] training loss: 0.00687215
INFO:root:[189,   250] training loss: 0.00601544
INFO:root:[189,   300] training loss: 0.00752172
INFO:root:[189,   350] training loss: 0.00782939
INFO:root:[189,   400] training loss: 0.00005109
INFO:root:[189,   450] training loss: 0.00001703
INFO:root:[189,   500] training loss: 0.00003208
INFO:root:[189,   550] training loss: 0.00025745
INFO:root:[189,   600] training loss: 0.00013390
INFO:root:[189,   650] training loss: 0.00002789
INFO:root:[189,   700] training loss: 0.00002589
INFO:root:[189,   750] training loss: 0.00034287
INFO:root:[189,   800] training loss: 0.00039379
INFO:root:[189,   850] training loss: 0.00059084
INFO:root:[189,   900] training loss: 0.00533521
INFO:root:[189,   950] training loss: 0.00147518
INFO:root:[189,  1000] training loss: 0.00003049
INFO:root:[189,  1050] training loss: 0.00002692
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch189
INFO:root:[190,    50] training loss: 0.00793472
INFO:root:[190,   100] training loss: 0.00738982
INFO:root:[190,   150] training loss: 0.00786924
INFO:root:[190,   200] training loss: 0.00681182
INFO:root:[190,   250] training loss: 0.00643581
INFO:root:[190,   300] training loss: 0.00790816
INFO:root:[190,   350] training loss: 0.00660962
INFO:root:[190,   400] training loss: 0.00001489
INFO:root:[190,   450] training loss: 0.00001559
INFO:root:[190,   500] training loss: 0.00002764
INFO:root:[190,   550] training loss: 0.00028994
INFO:root:[190,   600] training loss: 0.00015852
INFO:root:[190,   650] training loss: 0.00002868
INFO:root:[190,   700] training loss: 0.00002111
INFO:root:[190,   750] training loss: 0.00027456
INFO:root:[190,   800] training loss: 0.00041827
INFO:root:[190,   850] training loss: 0.00046343
INFO:root:[190,   900] training loss: 0.00538330
INFO:root:[190,   950] training loss: 0.00161712
INFO:root:[190,  1000] training loss: 0.00003197
INFO:root:[190,  1050] training loss: 0.00001983
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch190
INFO:root:[191,    50] training loss: 0.00857286
INFO:root:[191,   100] training loss: 0.00793444
INFO:root:[191,   150] training loss: 0.00801769
INFO:root:[191,   200] training loss: 0.00658189
INFO:root:[191,   250] training loss: 0.00635726
INFO:root:[191,   300] training loss: 0.00824735
INFO:root:[191,   350] training loss: 0.00677826
INFO:root:[191,   400] training loss: 0.00001717
INFO:root:[191,   450] training loss: 0.00001947
INFO:root:[191,   500] training loss: 0.00003090
INFO:root:[191,   550] training loss: 0.00025729
INFO:root:[191,   600] training loss: 0.00022700
INFO:root:[191,   650] training loss: 0.00002146
INFO:root:[191,   700] training loss: 0.00002964
INFO:root:[191,   750] training loss: 0.00030392
INFO:root:[191,   800] training loss: 0.00055449
INFO:root:[191,   850] training loss: 0.00049044
INFO:root:[191,   900] training loss: 0.00506785
INFO:root:[191,   950] training loss: 0.00145575
INFO:root:[191,  1000] training loss: 0.00005398
INFO:root:[191,  1050] training loss: 0.00001786
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch191
INFO:root:[192,    50] training loss: 0.00865752
INFO:root:[192,   100] training loss: 0.00786333
INFO:root:[192,   150] training loss: 0.00821680
INFO:root:[192,   200] training loss: 0.00745819
INFO:root:[192,   250] training loss: 0.00623116
INFO:root:[192,   300] training loss: 0.00780095
INFO:root:[192,   350] training loss: 0.00666275
INFO:root:[192,   400] training loss: 0.00001790
INFO:root:[192,   450] training loss: 0.00001689
INFO:root:[192,   500] training loss: 0.00002923
INFO:root:[192,   550] training loss: 0.00026087
INFO:root:[192,   600] training loss: 0.00015406
INFO:root:[192,   650] training loss: 0.00002020
INFO:root:[192,   700] training loss: 0.00002431
INFO:root:[192,   750] training loss: 0.00045409
INFO:root:[192,   800] training loss: 0.00054278
INFO:root:[192,   850] training loss: 0.00054678
INFO:root:[192,   900] training loss: 0.00508773
INFO:root:[192,   950] training loss: 0.00151468
INFO:root:[192,  1000] training loss: 0.00004415
INFO:root:[192,  1050] training loss: 0.00002470
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch192
INFO:root:[193,    50] training loss: 0.00783900
INFO:root:[193,   100] training loss: 0.00759627
INFO:root:[193,   150] training loss: 0.00768357
INFO:root:[193,   200] training loss: 0.00682664
INFO:root:[193,   250] training loss: 0.00579932
INFO:root:[193,   300] training loss: 0.00784518
INFO:root:[193,   350] training loss: 0.00691851
INFO:root:[193,   400] training loss: 0.00001575
INFO:root:[193,   450] training loss: 0.00003500
INFO:root:[193,   500] training loss: 0.00002487
INFO:root:[193,   550] training loss: 0.00024099
INFO:root:[193,   600] training loss: 0.00019259
INFO:root:[193,   650] training loss: 0.00002763
INFO:root:[193,   700] training loss: 0.00002013
INFO:root:[193,   750] training loss: 0.00021619
INFO:root:[193,   800] training loss: 0.00048267
INFO:root:[193,   850] training loss: 0.00051109
INFO:root:[193,   900] training loss: 0.00555705
INFO:root:[193,   950] training loss: 0.00175203
INFO:root:[193,  1000] training loss: 0.00002535
INFO:root:[193,  1050] training loss: 0.00002126
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch193
INFO:root:[194,    50] training loss: 0.00829500
INFO:root:[194,   100] training loss: 0.00716633
INFO:root:[194,   150] training loss: 0.00844071
INFO:root:[194,   200] training loss: 0.00703228
INFO:root:[194,   250] training loss: 0.00679731
INFO:root:[194,   300] training loss: 0.00814914
INFO:root:[194,   350] training loss: 0.00647877
INFO:root:[194,   400] training loss: 0.00003651
INFO:root:[194,   450] training loss: 0.00001832
INFO:root:[194,   500] training loss: 0.00002641
INFO:root:[194,   550] training loss: 0.00030731
INFO:root:[194,   600] training loss: 0.00015161
INFO:root:[194,   650] training loss: 0.00002215
INFO:root:[194,   700] training loss: 0.00002265
INFO:root:[194,   750] training loss: 0.00041484
INFO:root:[194,   800] training loss: 0.00045471
INFO:root:[194,   850] training loss: 0.00055839
INFO:root:[194,   900] training loss: 0.00536760
INFO:root:[194,   950] training loss: 0.00209074
INFO:root:[194,  1000] training loss: 0.00003125
INFO:root:[194,  1050] training loss: 0.00002322
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch194
INFO:root:[195,    50] training loss: 0.00843933
INFO:root:[195,   100] training loss: 0.00777579
INFO:root:[195,   150] training loss: 0.00792532
INFO:root:[195,   200] training loss: 0.00702622
INFO:root:[195,   250] training loss: 0.00648569
INFO:root:[195,   300] training loss: 0.00806402
INFO:root:[195,   350] training loss: 0.00619481
INFO:root:[195,   400] training loss: 0.00001887
INFO:root:[195,   450] training loss: 0.00001885
INFO:root:[195,   500] training loss: 0.00002384
INFO:root:[195,   550] training loss: 0.00038359
INFO:root:[195,   600] training loss: 0.00014056
INFO:root:[195,   650] training loss: 0.00002129
INFO:root:[195,   700] training loss: 0.00002378
INFO:root:[195,   750] training loss: 0.00033299
INFO:root:[195,   800] training loss: 0.00044902
INFO:root:[195,   850] training loss: 0.00059927
INFO:root:[195,   900] training loss: 0.00489271
INFO:root:[195,   950] training loss: 0.00162601
INFO:root:[195,  1000] training loss: 0.00002729
INFO:root:[195,  1050] training loss: 0.00001968
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch195
INFO:root:[196,    50] training loss: 0.00850822
INFO:root:[196,   100] training loss: 0.00720102
INFO:root:[196,   150] training loss: 0.00792347
INFO:root:[196,   200] training loss: 0.00690398
INFO:root:[196,   250] training loss: 0.00663384
INFO:root:[196,   300] training loss: 0.00776684
INFO:root:[196,   350] training loss: 0.00621901
INFO:root:[196,   400] training loss: 0.00001640
INFO:root:[196,   450] training loss: 0.00001629
INFO:root:[196,   500] training loss: 0.00002309
INFO:root:[196,   550] training loss: 0.00028346
INFO:root:[196,   600] training loss: 0.00018115
INFO:root:[196,   650] training loss: 0.00003103
INFO:root:[196,   700] training loss: 0.00002398
INFO:root:[196,   750] training loss: 0.00042647
INFO:root:[196,   800] training loss: 0.00044698
INFO:root:[196,   850] training loss: 0.00055650
INFO:root:[196,   900] training loss: 0.00648843
INFO:root:[196,   950] training loss: 0.00139951
INFO:root:[196,  1000] training loss: 0.00002697
INFO:root:[196,  1050] training loss: 0.00002487
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch196
INFO:root:[197,    50] training loss: 0.00861177
INFO:root:[197,   100] training loss: 0.00790927
INFO:root:[197,   150] training loss: 0.00808276
INFO:root:[197,   200] training loss: 0.00689293
INFO:root:[197,   250] training loss: 0.00654963
INFO:root:[197,   300] training loss: 0.00766461
INFO:root:[197,   350] training loss: 0.00628783
INFO:root:[197,   400] training loss: 0.00001575
INFO:root:[197,   450] training loss: 0.00002125
INFO:root:[197,   500] training loss: 0.00002405
INFO:root:[197,   550] training loss: 0.00033919
INFO:root:[197,   600] training loss: 0.00014726
INFO:root:[197,   650] training loss: 0.00002199
INFO:root:[197,   700] training loss: 0.00002291
INFO:root:[197,   750] training loss: 0.00029794
INFO:root:[197,   800] training loss: 0.00037801
INFO:root:[197,   850] training loss: 0.00049784
INFO:root:[197,   900] training loss: 0.00584251
INFO:root:[197,   950] training loss: 0.00151736
INFO:root:[197,  1000] training loss: 0.00004366
INFO:root:[197,  1050] training loss: 0.00002007
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch197
INFO:root:[198,    50] training loss: 0.00885383
INFO:root:[198,   100] training loss: 0.00754525
INFO:root:[198,   150] training loss: 0.00819069
INFO:root:[198,   200] training loss: 0.00765136
INFO:root:[198,   250] training loss: 0.00656530
INFO:root:[198,   300] training loss: 0.00770853
INFO:root:[198,   350] training loss: 0.00648589
INFO:root:[198,   400] training loss: 0.00001614
INFO:root:[198,   450] training loss: 0.00001807
INFO:root:[198,   500] training loss: 0.00003316
INFO:root:[198,   550] training loss: 0.00027079
INFO:root:[198,   600] training loss: 0.00015108
INFO:root:[198,   650] training loss: 0.00003298
INFO:root:[198,   700] training loss: 0.00002423
INFO:root:[198,   750] training loss: 0.00043215
INFO:root:[198,   800] training loss: 0.00056746
INFO:root:[198,   850] training loss: 0.00056392
INFO:root:[198,   900] training loss: 0.00543210
INFO:root:[198,   950] training loss: 0.00142008
INFO:root:[198,  1000] training loss: 0.00003130
INFO:root:[198,  1050] training loss: 0.00002054
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch198
INFO:root:[199,    50] training loss: 0.00840594
INFO:root:[199,   100] training loss: 0.00735678
INFO:root:[199,   150] training loss: 0.00758527
INFO:root:[199,   200] training loss: 0.00685904
INFO:root:[199,   250] training loss: 0.00637073
INFO:root:[199,   300] training loss: 0.00797034
INFO:root:[199,   350] training loss: 0.00644061
INFO:root:[199,   400] training loss: 0.00001555
INFO:root:[199,   450] training loss: 0.00001660
INFO:root:[199,   500] training loss: 0.00002394
INFO:root:[199,   550] training loss: 0.00026884
INFO:root:[199,   600] training loss: 0.00016497
INFO:root:[199,   650] training loss: 0.00002119
INFO:root:[199,   700] training loss: 0.00002116
INFO:root:[199,   750] training loss: 0.00037018
INFO:root:[199,   800] training loss: 0.00039862
INFO:root:[199,   850] training loss: 0.00061299
INFO:root:[199,   900] training loss: 0.00491491
INFO:root:[199,   950] training loss: 0.00156811
INFO:root:[199,  1000] training loss: 0.00003619
INFO:root:[199,  1050] training loss: 0.00002717
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch199
INFO:root:[200,    50] training loss: 0.00772182
INFO:root:[200,   100] training loss: 0.00742355
INFO:root:[200,   150] training loss: 0.00761453
INFO:root:[200,   200] training loss: 0.00681573
INFO:root:[200,   250] training loss: 0.00696516
INFO:root:[200,   300] training loss: 0.00818437
INFO:root:[200,   350] training loss: 0.00673209
INFO:root:[200,   400] training loss: 0.00001601
INFO:root:[200,   450] training loss: 0.00001903
INFO:root:[200,   500] training loss: 0.00003062
INFO:root:[200,   550] training loss: 0.00027930
INFO:root:[200,   600] training loss: 0.00023598
INFO:root:[200,   650] training loss: 0.00002180
INFO:root:[200,   700] training loss: 0.00002109
INFO:root:[200,   750] training loss: 0.00038580
INFO:root:[200,   800] training loss: 0.00043014
INFO:root:[200,   850] training loss: 0.00048857
INFO:root:[200,   900] training loss: 0.00501126
INFO:root:[200,   950] training loss: 0.00145299
INFO:root:[200,  1000] training loss: 0.00002909
INFO:root:[200,  1050] training loss: 0.00016011
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch200
INFO:root:[201,    50] training loss: 0.00850210
INFO:root:[201,   100] training loss: 0.00771259
INFO:root:[201,   150] training loss: 0.00767508
INFO:root:[201,   200] training loss: 0.00751740
INFO:root:[201,   250] training loss: 0.00626212
INFO:root:[201,   300] training loss: 0.00746160
INFO:root:[201,   350] training loss: 0.00638162
INFO:root:[201,   400] training loss: 0.00001554
INFO:root:[201,   450] training loss: 0.00001796
INFO:root:[201,   500] training loss: 0.00002437
INFO:root:[201,   550] training loss: 0.00030996
INFO:root:[201,   600] training loss: 0.00012663
INFO:root:[201,   650] training loss: 0.00002948
INFO:root:[201,   700] training loss: 0.00001958
INFO:root:[201,   750] training loss: 0.00036085
INFO:root:[201,   800] training loss: 0.00047501
INFO:root:[201,   850] training loss: 0.00057285
INFO:root:[201,   900] training loss: 0.00537944
INFO:root:[201,   950] training loss: 0.00147794
INFO:root:[201,  1000] training loss: 0.00003220
INFO:root:[201,  1050] training loss: 0.00002344
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch201
INFO:root:[202,    50] training loss: 0.00827721
INFO:root:[202,   100] training loss: 0.00792739
INFO:root:[202,   150] training loss: 0.00756212
INFO:root:[202,   200] training loss: 0.00698146
INFO:root:[202,   250] training loss: 0.00629375
INFO:root:[202,   300] training loss: 0.00775905
INFO:root:[202,   350] training loss: 0.00691349
INFO:root:[202,   400] training loss: 0.00002013
INFO:root:[202,   450] training loss: 0.00001812
INFO:root:[202,   500] training loss: 0.00002686
INFO:root:[202,   550] training loss: 0.00043665
INFO:root:[202,   600] training loss: 0.00015292
INFO:root:[202,   650] training loss: 0.00002401
INFO:root:[202,   700] training loss: 0.00001706
INFO:root:[202,   750] training loss: 0.00034522
INFO:root:[202,   800] training loss: 0.00049574
INFO:root:[202,   850] training loss: 0.00062097
INFO:root:[202,   900] training loss: 0.00504968
INFO:root:[202,   950] training loss: 0.00165927
INFO:root:[202,  1000] training loss: 0.00002797
INFO:root:[202,  1050] training loss: 0.00002639
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch202
INFO:root:[203,    50] training loss: 0.00832957
INFO:root:[203,   100] training loss: 0.00752779
INFO:root:[203,   150] training loss: 0.00732197
INFO:root:[203,   200] training loss: 0.00739248
INFO:root:[203,   250] training loss: 0.00732305
INFO:root:[203,   300] training loss: 0.00784212
INFO:root:[203,   350] training loss: 0.00631596
INFO:root:[203,   400] training loss: 0.00004050
INFO:root:[203,   450] training loss: 0.00001638
INFO:root:[203,   500] training loss: 0.00002484
INFO:root:[203,   550] training loss: 0.00038767
INFO:root:[203,   600] training loss: 0.00016375
INFO:root:[203,   650] training loss: 0.00002036
INFO:root:[203,   700] training loss: 0.00002085
INFO:root:[203,   750] training loss: 0.00034607
INFO:root:[203,   800] training loss: 0.00050551
INFO:root:[203,   850] training loss: 0.00057614
INFO:root:[203,   900] training loss: 0.00736913
INFO:root:[203,   950] training loss: 0.00146956
INFO:root:[203,  1000] training loss: 0.00003216
INFO:root:[203,  1050] training loss: 0.00002231
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch203
INFO:root:[204,    50] training loss: 0.00809061
INFO:root:[204,   100] training loss: 0.00734828
INFO:root:[204,   150] training loss: 0.00792986
INFO:root:[204,   200] training loss: 0.00697827
INFO:root:[204,   250] training loss: 0.00627277
INFO:root:[204,   300] training loss: 0.00804423
INFO:root:[204,   350] training loss: 0.00617004
INFO:root:[204,   400] training loss: 0.00001462
INFO:root:[204,   450] training loss: 0.00001882
INFO:root:[204,   500] training loss: 0.00002408
INFO:root:[204,   550] training loss: 0.00027771
INFO:root:[204,   600] training loss: 0.00020144
INFO:root:[204,   650] training loss: 0.00002031
INFO:root:[204,   700] training loss: 0.00002318
INFO:root:[204,   750] training loss: 0.00041352
INFO:root:[204,   800] training loss: 0.00044813
INFO:root:[204,   850] training loss: 0.00058962
INFO:root:[204,   900] training loss: 0.00536632
INFO:root:[204,   950] training loss: 0.00157766
INFO:root:[204,  1000] training loss: 0.00003204
INFO:root:[204,  1050] training loss: 0.00002025
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch204
INFO:root:[205,    50] training loss: 0.00811227
INFO:root:[205,   100] training loss: 0.00747283
INFO:root:[205,   150] training loss: 0.00750802
INFO:root:[205,   200] training loss: 0.00679816
INFO:root:[205,   250] training loss: 0.00687433
INFO:root:[205,   300] training loss: 0.00814656
INFO:root:[205,   350] training loss: 0.00657856
INFO:root:[205,   400] training loss: 0.00001477
INFO:root:[205,   450] training loss: 0.00001987
INFO:root:[205,   500] training loss: 0.00003063
INFO:root:[205,   550] training loss: 0.00024928
INFO:root:[205,   600] training loss: 0.00015092
INFO:root:[205,   650] training loss: 0.00001782
INFO:root:[205,   700] training loss: 0.00002081
INFO:root:[205,   750] training loss: 0.00028889
INFO:root:[205,   800] training loss: 0.00041872
INFO:root:[205,   850] training loss: 0.00054595
INFO:root:[205,   900] training loss: 0.00616757
INFO:root:[205,   950] training loss: 0.00136007
INFO:root:[205,  1000] training loss: 0.00002781
INFO:root:[205,  1050] training loss: 0.00002449
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch205
INFO:root:[206,    50] training loss: 0.00781514
INFO:root:[206,   100] training loss: 0.00746013
INFO:root:[206,   150] training loss: 0.00857177
INFO:root:[206,   200] training loss: 0.00694066
INFO:root:[206,   250] training loss: 0.00663065
INFO:root:[206,   300] training loss: 0.00794849
INFO:root:[206,   350] training loss: 0.00646082
INFO:root:[206,   400] training loss: 0.00001642
INFO:root:[206,   450] training loss: 0.00001942
INFO:root:[206,   500] training loss: 0.00002315
INFO:root:[206,   550] training loss: 0.00031960
INFO:root:[206,   600] training loss: 0.00014963
INFO:root:[206,   650] training loss: 0.00002707
INFO:root:[206,   700] training loss: 0.00001726
INFO:root:[206,   750] training loss: 0.00030206
INFO:root:[206,   800] training loss: 0.00047392
INFO:root:[206,   850] training loss: 0.00044285
INFO:root:[206,   900] training loss: 0.00587970
INFO:root:[206,   950] training loss: 0.00155732
INFO:root:[206,  1000] training loss: 0.00002874
INFO:root:[206,  1050] training loss: 0.00003677
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch206
INFO:root:[207,    50] training loss: 0.00817443
INFO:root:[207,   100] training loss: 0.00744649
INFO:root:[207,   150] training loss: 0.00782282
INFO:root:[207,   200] training loss: 0.00692271
INFO:root:[207,   250] training loss: 0.00612646
INFO:root:[207,   300] training loss: 0.00837835
INFO:root:[207,   350] training loss: 0.00694693
INFO:root:[207,   400] training loss: 0.00005705
INFO:root:[207,   450] training loss: 0.00001765
INFO:root:[207,   500] training loss: 0.00002606
INFO:root:[207,   550] training loss: 0.00026217
INFO:root:[207,   600] training loss: 0.00014086
INFO:root:[207,   650] training loss: 0.00002490
INFO:root:[207,   700] training loss: 0.00001785
INFO:root:[207,   750] training loss: 0.00049129
INFO:root:[207,   800] training loss: 0.00052798
INFO:root:[207,   850] training loss: 0.00046869
INFO:root:[207,   900] training loss: 0.00556951
INFO:root:[207,   950] training loss: 0.00156847
INFO:root:[207,  1000] training loss: 0.00003320
INFO:root:[207,  1050] training loss: 0.00002722
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch207
INFO:root:[208,    50] training loss: 0.00800972
INFO:root:[208,   100] training loss: 0.00706120
INFO:root:[208,   150] training loss: 0.00882565
INFO:root:[208,   200] training loss: 0.00711231
INFO:root:[208,   250] training loss: 0.00621657
INFO:root:[208,   300] training loss: 0.00804800
INFO:root:[208,   350] training loss: 0.00757289
INFO:root:[208,   400] training loss: 0.00001737
INFO:root:[208,   450] training loss: 0.00001706
INFO:root:[208,   500] training loss: 0.00002976
INFO:root:[208,   550] training loss: 0.00028088
INFO:root:[208,   600] training loss: 0.00016468
INFO:root:[208,   650] training loss: 0.00002094
INFO:root:[208,   700] training loss: 0.00003544
INFO:root:[208,   750] training loss: 0.00042015
INFO:root:[208,   800] training loss: 0.00034580
INFO:root:[208,   850] training loss: 0.00058816
INFO:root:[208,   900] training loss: 0.00490845
INFO:root:[208,   950] training loss: 0.00156474
INFO:root:[208,  1000] training loss: 0.00007810
INFO:root:[208,  1050] training loss: 0.00002300
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch208
INFO:root:[209,    50] training loss: 0.00820795
INFO:root:[209,   100] training loss: 0.00721050
INFO:root:[209,   150] training loss: 0.00753744
INFO:root:[209,   200] training loss: 0.00686854
INFO:root:[209,   250] training loss: 0.00681129
INFO:root:[209,   300] training loss: 0.00836187
INFO:root:[209,   350] training loss: 0.00655245
INFO:root:[209,   400] training loss: 0.00001581
INFO:root:[209,   450] training loss: 0.00002140
INFO:root:[209,   500] training loss: 0.00003062
INFO:root:[209,   550] training loss: 0.00034529
INFO:root:[209,   600] training loss: 0.00013879
INFO:root:[209,   650] training loss: 0.00002058
INFO:root:[209,   700] training loss: 0.00002180
INFO:root:[209,   750] training loss: 0.00040676
INFO:root:[209,   800] training loss: 0.00051368
INFO:root:[209,   850] training loss: 0.00051095
INFO:root:[209,   900] training loss: 0.00579977
INFO:root:[209,   950] training loss: 0.00166395
INFO:root:[209,  1000] training loss: 0.00003048
INFO:root:[209,  1050] training loss: 0.00002070
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch209
INFO:root:[210,    50] training loss: 0.00824677
INFO:root:[210,   100] training loss: 0.00762439
INFO:root:[210,   150] training loss: 0.00765455
INFO:root:[210,   200] training loss: 0.00867910
INFO:root:[210,   250] training loss: 0.00705022
INFO:root:[210,   300] training loss: 0.00774338
INFO:root:[210,   350] training loss: 0.00671243
INFO:root:[210,   400] training loss: 0.00001833
INFO:root:[210,   450] training loss: 0.00001672
INFO:root:[210,   500] training loss: 0.00002659
INFO:root:[210,   550] training loss: 0.00027713
INFO:root:[210,   600] training loss: 0.00015089
INFO:root:[210,   650] training loss: 0.00003821
INFO:root:[210,   700] training loss: 0.00002127
INFO:root:[210,   750] training loss: 0.00033894
INFO:root:[210,   800] training loss: 0.00041680
INFO:root:[210,   850] training loss: 0.00050699
INFO:root:[210,   900] training loss: 0.00601344
INFO:root:[210,   950] training loss: 0.00132779
INFO:root:[210,  1000] training loss: 0.00007685
INFO:root:[210,  1050] training loss: 0.00003371
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch210
INFO:root:[211,    50] training loss: 0.00828671
INFO:root:[211,   100] training loss: 0.00809560
INFO:root:[211,   150] training loss: 0.00763993
INFO:root:[211,   200] training loss: 0.00706860
INFO:root:[211,   250] training loss: 0.00639219
INFO:root:[211,   300] training loss: 0.00772320
INFO:root:[211,   350] training loss: 0.00641184
INFO:root:[211,   400] training loss: 0.00001461
INFO:root:[211,   450] training loss: 0.00002188
INFO:root:[211,   500] training loss: 0.00002773
INFO:root:[211,   550] training loss: 0.00026853
INFO:root:[211,   600] training loss: 0.00014179
INFO:root:[211,   650] training loss: 0.00001792
INFO:root:[211,   700] training loss: 0.00002530
INFO:root:[211,   750] training loss: 0.00035109
INFO:root:[211,   800] training loss: 0.00047304
INFO:root:[211,   850] training loss: 0.00058662
INFO:root:[211,   900] training loss: 0.00471951
INFO:root:[211,   950] training loss: 0.00136016
INFO:root:[211,  1000] training loss: 0.00002741
INFO:root:[211,  1050] training loss: 0.00002140
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch211
INFO:root:[212,    50] training loss: 0.01111935
INFO:root:[212,   100] training loss: 0.00725190
INFO:root:[212,   150] training loss: 0.00801918
INFO:root:[212,   200] training loss: 0.00686143
INFO:root:[212,   250] training loss: 0.00632581
INFO:root:[212,   300] training loss: 0.00806190
INFO:root:[212,   350] training loss: 0.00620805
INFO:root:[212,   400] training loss: 0.00001604
INFO:root:[212,   450] training loss: 0.00001901
INFO:root:[212,   500] training loss: 0.00002604
INFO:root:[212,   550] training loss: 0.00029457
INFO:root:[212,   600] training loss: 0.00014181
INFO:root:[212,   650] training loss: 0.00002039
INFO:root:[212,   700] training loss: 0.00001980
INFO:root:[212,   750] training loss: 0.00028259
INFO:root:[212,   800] training loss: 0.00054910
INFO:root:[212,   850] training loss: 0.00062742
INFO:root:[212,   900] training loss: 0.00542076
INFO:root:[212,   950] training loss: 0.00168591
INFO:root:[212,  1000] training loss: 0.00003046
INFO:root:[212,  1050] training loss: 0.00003049
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch212
INFO:root:[213,    50] training loss: 0.00785984
INFO:root:[213,   100] training loss: 0.00786727
INFO:root:[213,   150] training loss: 0.00775593
INFO:root:[213,   200] training loss: 0.00677516
INFO:root:[213,   250] training loss: 0.00653290
INFO:root:[213,   300] training loss: 0.00866008
INFO:root:[213,   350] training loss: 0.00652686
INFO:root:[213,   400] training loss: 0.00001980
INFO:root:[213,   450] training loss: 0.00002073
INFO:root:[213,   500] training loss: 0.00006987
INFO:root:[213,   550] training loss: 0.00028900
INFO:root:[213,   600] training loss: 0.00014983
INFO:root:[213,   650] training loss: 0.00002129
INFO:root:[213,   700] training loss: 0.00002062
INFO:root:[213,   750] training loss: 0.00044232
INFO:root:[213,   800] training loss: 0.00053042
INFO:root:[213,   850] training loss: 0.00052023
INFO:root:[213,   900] training loss: 0.00505696
INFO:root:[213,   950] training loss: 0.00175519
INFO:root:[213,  1000] training loss: 0.00004416
INFO:root:[213,  1050] training loss: 0.00007288
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch213
INFO:root:[214,    50] training loss: 0.00844517
INFO:root:[214,   100] training loss: 0.00722605
INFO:root:[214,   150] training loss: 0.00794851
INFO:root:[214,   200] training loss: 0.00778333
INFO:root:[214,   250] training loss: 0.00731828
INFO:root:[214,   300] training loss: 0.00800944
INFO:root:[214,   350] training loss: 0.00660805
INFO:root:[214,   400] training loss: 0.00003112
INFO:root:[214,   450] training loss: 0.00003496
INFO:root:[214,   500] training loss: 0.00002951
INFO:root:[214,   550] training loss: 0.00025906
INFO:root:[214,   600] training loss: 0.00013218
INFO:root:[214,   650] training loss: 0.00003170
INFO:root:[214,   700] training loss: 0.00001806
INFO:root:[214,   750] training loss: 0.00032675
INFO:root:[214,   800] training loss: 0.00054484
INFO:root:[214,   850] training loss: 0.00049607
INFO:root:[214,   900] training loss: 0.00631868
INFO:root:[214,   950] training loss: 0.00159024
INFO:root:[214,  1000] training loss: 0.00003598
INFO:root:[214,  1050] training loss: 0.00002089
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch214
INFO:root:[215,    50] training loss: 0.00832214
INFO:root:[215,   100] training loss: 0.00747624
INFO:root:[215,   150] training loss: 0.00788187
INFO:root:[215,   200] training loss: 0.00709155
INFO:root:[215,   250] training loss: 0.00616344
INFO:root:[215,   300] training loss: 0.00863145
INFO:root:[215,   350] training loss: 0.00662054
INFO:root:[215,   400] training loss: 0.00001667
INFO:root:[215,   450] training loss: 0.00001620
INFO:root:[215,   500] training loss: 0.00002496
INFO:root:[215,   550] training loss: 0.00028818
INFO:root:[215,   600] training loss: 0.00019926
INFO:root:[215,   650] training loss: 0.00002019
INFO:root:[215,   700] training loss: 0.00001790
INFO:root:[215,   750] training loss: 0.00038280
INFO:root:[215,   800] training loss: 0.00047531
INFO:root:[215,   850] training loss: 0.00066319
INFO:root:[215,   900] training loss: 0.00500613
INFO:root:[215,   950] training loss: 0.00152379
INFO:root:[215,  1000] training loss: 0.00003447
INFO:root:[215,  1050] training loss: 0.00002309
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch215
INFO:root:[216,    50] training loss: 0.00864415
INFO:root:[216,   100] training loss: 0.00731285
INFO:root:[216,   150] training loss: 0.00793056
INFO:root:[216,   200] training loss: 0.00712373
INFO:root:[216,   250] training loss: 0.00612463
INFO:root:[216,   300] training loss: 0.00830147
INFO:root:[216,   350] training loss: 0.00686022
INFO:root:[216,   400] training loss: 0.00001670
INFO:root:[216,   450] training loss: 0.00001766
INFO:root:[216,   500] training loss: 0.00003042
INFO:root:[216,   550] training loss: 0.00027388
INFO:root:[216,   600] training loss: 0.00015913
INFO:root:[216,   650] training loss: 0.00002368
INFO:root:[216,   700] training loss: 0.00002089
INFO:root:[216,   750] training loss: 0.00033896
INFO:root:[216,   800] training loss: 0.00050132
INFO:root:[216,   850] training loss: 0.00051034
INFO:root:[216,   900] training loss: 0.00503443
INFO:root:[216,   950] training loss: 0.00147404
INFO:root:[216,  1000] training loss: 0.00004415
INFO:root:[216,  1050] training loss: 0.00003141
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch216
INFO:root:[217,    50] training loss: 0.00816089
INFO:root:[217,   100] training loss: 0.00739498
INFO:root:[217,   150] training loss: 0.00801885
INFO:root:[217,   200] training loss: 0.00700066
INFO:root:[217,   250] training loss: 0.00627942
INFO:root:[217,   300] training loss: 0.00785222
INFO:root:[217,   350] training loss: 0.00664397
INFO:root:[217,   400] training loss: 0.00002069
INFO:root:[217,   450] training loss: 0.00001811
INFO:root:[217,   500] training loss: 0.00005199
INFO:root:[217,   550] training loss: 0.00025101
INFO:root:[217,   600] training loss: 0.00014413
INFO:root:[217,   650] training loss: 0.00002265
INFO:root:[217,   700] training loss: 0.00001784
INFO:root:[217,   750] training loss: 0.00032006
INFO:root:[217,   800] training loss: 0.00055258
INFO:root:[217,   850] training loss: 0.00051912
INFO:root:[217,   900] training loss: 0.00597786
INFO:root:[217,   950] training loss: 0.00194151
INFO:root:[217,  1000] training loss: 0.00003082
INFO:root:[217,  1050] training loss: 0.00002034
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch217
INFO:root:[218,    50] training loss: 0.00838630
INFO:root:[218,   100] training loss: 0.00756023
INFO:root:[218,   150] training loss: 0.00755720
INFO:root:[218,   200] training loss: 0.00665876
INFO:root:[218,   250] training loss: 0.00616735
INFO:root:[218,   300] training loss: 0.00805641
INFO:root:[218,   350] training loss: 0.00650114
INFO:root:[218,   400] training loss: 0.00002593
INFO:root:[218,   450] training loss: 0.00001938
INFO:root:[218,   500] training loss: 0.00002578
INFO:root:[218,   550] training loss: 0.00029970
INFO:root:[218,   600] training loss: 0.00015553
INFO:root:[218,   650] training loss: 0.00003263
INFO:root:[218,   700] training loss: 0.00001937
INFO:root:[218,   750] training loss: 0.00037563
INFO:root:[218,   800] training loss: 0.00040318
INFO:root:[218,   850] training loss: 0.00048277
INFO:root:[218,   900] training loss: 0.00534575
INFO:root:[218,   950] training loss: 0.00135691
INFO:root:[218,  1000] training loss: 0.00003409
INFO:root:[218,  1050] training loss: 0.00002385
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch218
INFO:root:[219,    50] training loss: 0.00811453
INFO:root:[219,   100] training loss: 0.00737532
INFO:root:[219,   150] training loss: 0.00775078
INFO:root:[219,   200] training loss: 0.00709002
INFO:root:[219,   250] training loss: 0.00652477
INFO:root:[219,   300] training loss: 0.00854082
INFO:root:[219,   350] training loss: 0.00649789
INFO:root:[219,   400] training loss: 0.00001585
INFO:root:[219,   450] training loss: 0.00001758
INFO:root:[219,   500] training loss: 0.00002411
INFO:root:[219,   550] training loss: 0.00032909
INFO:root:[219,   600] training loss: 0.00015081
INFO:root:[219,   650] training loss: 0.00005387
INFO:root:[219,   700] training loss: 0.00002381
INFO:root:[219,   750] training loss: 0.00035679
INFO:root:[219,   800] training loss: 0.00034785
INFO:root:[219,   850] training loss: 0.00056790
INFO:root:[219,   900] training loss: 0.00504773
INFO:root:[219,   950] training loss: 0.00217658
INFO:root:[219,  1000] training loss: 0.00003401
INFO:root:[219,  1050] training loss: 0.00002101
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch219
INFO:root:[220,    50] training loss: 0.00857571
INFO:root:[220,   100] training loss: 0.00741407
INFO:root:[220,   150] training loss: 0.00856533
INFO:root:[220,   200] training loss: 0.00695010
INFO:root:[220,   250] training loss: 0.00635065
INFO:root:[220,   300] training loss: 0.00775778
INFO:root:[220,   350] training loss: 0.00646189
INFO:root:[220,   400] training loss: 0.00001520
INFO:root:[220,   450] training loss: 0.00001855
INFO:root:[220,   500] training loss: 0.00002942
INFO:root:[220,   550] training loss: 0.00034587
INFO:root:[220,   600] training loss: 0.00015105
INFO:root:[220,   650] training loss: 0.00002660
INFO:root:[220,   700] training loss: 0.00002321
INFO:root:[220,   750] training loss: 0.00044654
INFO:root:[220,   800] training loss: 0.00053225
INFO:root:[220,   850] training loss: 0.00051904
INFO:root:[220,   900] training loss: 0.00593786
INFO:root:[220,   950] training loss: 0.00149008
INFO:root:[220,  1000] training loss: 0.00002933
INFO:root:[220,  1050] training loss: 0.00002116
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch220
INFO:root:[221,    50] training loss: 0.00873331
INFO:root:[221,   100] training loss: 0.00734345
INFO:root:[221,   150] training loss: 0.00787509
INFO:root:[221,   200] training loss: 0.00723837
INFO:root:[221,   250] training loss: 0.00689021
INFO:root:[221,   300] training loss: 0.00808212
INFO:root:[221,   350] training loss: 0.00695229
INFO:root:[221,   400] training loss: 0.00001746
INFO:root:[221,   450] training loss: 0.00001616
INFO:root:[221,   500] training loss: 0.00002990
INFO:root:[221,   550] training loss: 0.00037961
INFO:root:[221,   600] training loss: 0.00012697
INFO:root:[221,   650] training loss: 0.00003298
INFO:root:[221,   700] training loss: 0.00002293
INFO:root:[221,   750] training loss: 0.00028774
INFO:root:[221,   800] training loss: 0.00046692
INFO:root:[221,   850] training loss: 0.00071158
INFO:root:[221,   900] training loss: 0.00568633
INFO:root:[221,   950] training loss: 0.00217387
INFO:root:[221,  1000] training loss: 0.00003322
INFO:root:[221,  1050] training loss: 0.00002365
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch221
INFO:root:[222,    50] training loss: 0.00787684
INFO:root:[222,   100] training loss: 0.00731833
INFO:root:[222,   150] training loss: 0.00764740
INFO:root:[222,   200] training loss: 0.00701883
INFO:root:[222,   250] training loss: 0.00613378
INFO:root:[222,   300] training loss: 0.00848455
INFO:root:[222,   350] training loss: 0.00607384
INFO:root:[222,   400] training loss: 0.00001749
INFO:root:[222,   450] training loss: 0.00001783
INFO:root:[222,   500] training loss: 0.00002605
INFO:root:[222,   550] training loss: 0.00027032
INFO:root:[222,   600] training loss: 0.00013350
INFO:root:[222,   650] training loss: 0.00002426
INFO:root:[222,   700] training loss: 0.00002157
INFO:root:[222,   750] training loss: 0.00033640
INFO:root:[222,   800] training loss: 0.00041571
INFO:root:[222,   850] training loss: 0.00055043
INFO:root:[222,   900] training loss: 0.00643400
INFO:root:[222,   950] training loss: 0.00157199
INFO:root:[222,  1000] training loss: 0.00002996
INFO:root:[222,  1050] training loss: 0.00002409
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch222
INFO:root:[223,    50] training loss: 0.00818749
INFO:root:[223,   100] training loss: 0.00737247
INFO:root:[223,   150] training loss: 0.00794877
INFO:root:[223,   200] training loss: 0.00696287
INFO:root:[223,   250] training loss: 0.00622730
INFO:root:[223,   300] training loss: 0.00816689
INFO:root:[223,   350] training loss: 0.00665886
INFO:root:[223,   400] training loss: 0.00001786
INFO:root:[223,   450] training loss: 0.00001602
INFO:root:[223,   500] training loss: 0.00002886
INFO:root:[223,   550] training loss: 0.00032810
INFO:root:[223,   600] training loss: 0.00014542
INFO:root:[223,   650] training loss: 0.00002553
INFO:root:[223,   700] training loss: 0.00002651
INFO:root:[223,   750] training loss: 0.00032835
INFO:root:[223,   800] training loss: 0.00041021
INFO:root:[223,   850] training loss: 0.00046194
INFO:root:[223,   900] training loss: 0.00537522
INFO:root:[223,   950] training loss: 0.00156961
INFO:root:[223,  1000] training loss: 0.00007142
INFO:root:[223,  1050] training loss: 0.00002751
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch223
INFO:root:[224,    50] training loss: 0.00831971
INFO:root:[224,   100] training loss: 0.00777689
INFO:root:[224,   150] training loss: 0.00764084
INFO:root:[224,   200] training loss: 0.00713049
INFO:root:[224,   250] training loss: 0.00809052
INFO:root:[224,   300] training loss: 0.00798623
INFO:root:[224,   350] training loss: 0.00695400
INFO:root:[224,   400] training loss: 0.00001676
INFO:root:[224,   450] training loss: 0.00001741
INFO:root:[224,   500] training loss: 0.00002946
INFO:root:[224,   550] training loss: 0.00027553
INFO:root:[224,   600] training loss: 0.00019138
INFO:root:[224,   650] training loss: 0.00002177
INFO:root:[224,   700] training loss: 0.00001880
INFO:root:[224,   750] training loss: 0.00035775
INFO:root:[224,   800] training loss: 0.00051123
INFO:root:[224,   850] training loss: 0.00057544
INFO:root:[224,   900] training loss: 0.00551906
INFO:root:[224,   950] training loss: 0.00153430
INFO:root:[224,  1000] training loss: 0.00004111
INFO:root:[224,  1050] training loss: 0.00003643
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch224
INFO:root:[225,    50] training loss: 0.00805480
INFO:root:[225,   100] training loss: 0.00770557
INFO:root:[225,   150] training loss: 0.00750134
INFO:root:[225,   200] training loss: 0.00700549
INFO:root:[225,   250] training loss: 0.00635834
INFO:root:[225,   300] training loss: 0.00802596
INFO:root:[225,   350] training loss: 0.00604968
INFO:root:[225,   400] training loss: 0.00001658
INFO:root:[225,   450] training loss: 0.00001839
INFO:root:[225,   500] training loss: 0.00003030
INFO:root:[225,   550] training loss: 0.00028690
INFO:root:[225,   600] training loss: 0.00015067
INFO:root:[225,   650] training loss: 0.00002205
INFO:root:[225,   700] training loss: 0.00002042
INFO:root:[225,   750] training loss: 0.00027220
INFO:root:[225,   800] training loss: 0.00045754
INFO:root:[225,   850] training loss: 0.00054723
INFO:root:[225,   900] training loss: 0.00521363
INFO:root:[225,   950] training loss: 0.00166943
INFO:root:[225,  1000] training loss: 0.00003651
INFO:root:[225,  1050] training loss: 0.00002346
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch225
INFO:root:[226,    50] training loss: 0.00807055
INFO:root:[226,   100] training loss: 0.00782222
INFO:root:[226,   150] training loss: 0.00753981
INFO:root:[226,   200] training loss: 0.00676209
INFO:root:[226,   250] training loss: 0.00641136
INFO:root:[226,   300] training loss: 0.00769982
INFO:root:[226,   350] training loss: 0.00674115
INFO:root:[226,   400] training loss: 0.00001870
INFO:root:[226,   450] training loss: 0.00001921
INFO:root:[226,   500] training loss: 0.00002789
INFO:root:[226,   550] training loss: 0.00029657
INFO:root:[226,   600] training loss: 0.00019946
INFO:root:[226,   650] training loss: 0.00002398
INFO:root:[226,   700] training loss: 0.00002296
INFO:root:[226,   750] training loss: 0.00033638
INFO:root:[226,   800] training loss: 0.00045221
INFO:root:[226,   850] training loss: 0.00049042
INFO:root:[226,   900] training loss: 0.00479834
INFO:root:[226,   950] training loss: 0.00165485
INFO:root:[226,  1000] training loss: 0.00003020
INFO:root:[226,  1050] training loss: 0.00003496
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch226
INFO:root:[227,    50] training loss: 0.00794735
INFO:root:[227,   100] training loss: 0.00722288
INFO:root:[227,   150] training loss: 0.00926626
INFO:root:[227,   200] training loss: 0.00712047
INFO:root:[227,   250] training loss: 0.00709254
INFO:root:[227,   300] training loss: 0.00807434
INFO:root:[227,   350] training loss: 0.00647481
INFO:root:[227,   400] training loss: 0.00001633
INFO:root:[227,   450] training loss: 0.00001694
INFO:root:[227,   500] training loss: 0.00002846
INFO:root:[227,   550] training loss: 0.00033628
INFO:root:[227,   600] training loss: 0.00012572
INFO:root:[227,   650] training loss: 0.00002035
INFO:root:[227,   700] training loss: 0.00002016
INFO:root:[227,   750] training loss: 0.00029348
INFO:root:[227,   800] training loss: 0.00044229
INFO:root:[227,   850] training loss: 0.00070952
INFO:root:[227,   900] training loss: 0.00548872
INFO:root:[227,   950] training loss: 0.00221161
INFO:root:[227,  1000] training loss: 0.00003725
INFO:root:[227,  1050] training loss: 0.00002408
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch227
INFO:root:[228,    50] training loss: 0.00911957
INFO:root:[228,   100] training loss: 0.00806142
INFO:root:[228,   150] training loss: 0.00843470
INFO:root:[228,   200] training loss: 0.00821242
INFO:root:[228,   250] training loss: 0.00635125
INFO:root:[228,   300] training loss: 0.00788142
INFO:root:[228,   350] training loss: 0.00646602
INFO:root:[228,   400] training loss: 0.00001686
INFO:root:[228,   450] training loss: 0.00001516
INFO:root:[228,   500] training loss: 0.00003423
INFO:root:[228,   550] training loss: 0.00026362
INFO:root:[228,   600] training loss: 0.00019216
INFO:root:[228,   650] training loss: 0.00002760
INFO:root:[228,   700] training loss: 0.00001762
INFO:root:[228,   750] training loss: 0.00066659
INFO:root:[228,   800] training loss: 0.00043001
INFO:root:[228,   850] training loss: 0.00063905
INFO:root:[228,   900] training loss: 0.00793810
INFO:root:[228,   950] training loss: 0.00148161
INFO:root:[228,  1000] training loss: 0.00003479
INFO:root:[228,  1050] training loss: 0.00002002
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch228
INFO:root:[229,    50] training loss: 0.00819167
INFO:root:[229,   100] training loss: 0.00738046
INFO:root:[229,   150] training loss: 0.00833490
INFO:root:[229,   200] training loss: 0.00701972
INFO:root:[229,   250] training loss: 0.00619687
INFO:root:[229,   300] training loss: 0.00781785
INFO:root:[229,   350] training loss: 0.00680307
INFO:root:[229,   400] training loss: 0.00001524
INFO:root:[229,   450] training loss: 0.00001687
INFO:root:[229,   500] training loss: 0.00002499
INFO:root:[229,   550] training loss: 0.00036865
INFO:root:[229,   600] training loss: 0.00018583
INFO:root:[229,   650] training loss: 0.00002256
INFO:root:[229,   700] training loss: 0.00002146
INFO:root:[229,   750] training loss: 0.00034531
INFO:root:[229,   800] training loss: 0.00047029
INFO:root:[229,   850] training loss: 0.00043239
INFO:root:[229,   900] training loss: 0.00478876
INFO:root:[229,   950] training loss: 0.00142161
INFO:root:[229,  1000] training loss: 0.00002419
INFO:root:[229,  1050] training loss: 0.00002136
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch229
INFO:root:[230,    50] training loss: 0.00870565
INFO:root:[230,   100] training loss: 0.00748368
INFO:root:[230,   150] training loss: 0.00780402
INFO:root:[230,   200] training loss: 0.00681324
INFO:root:[230,   250] training loss: 0.00646062
INFO:root:[230,   300] training loss: 0.00767611
INFO:root:[230,   350] training loss: 0.00653963
INFO:root:[230,   400] training loss: 0.00002218
INFO:root:[230,   450] training loss: 0.00001853
INFO:root:[230,   500] training loss: 0.00002586
INFO:root:[230,   550] training loss: 0.00035764
INFO:root:[230,   600] training loss: 0.00013648
INFO:root:[230,   650] training loss: 0.00003465
INFO:root:[230,   700] training loss: 0.00002347
INFO:root:[230,   750] training loss: 0.00039072
INFO:root:[230,   800] training loss: 0.00036617
INFO:root:[230,   850] training loss: 0.00044018
INFO:root:[230,   900] training loss: 0.00546788
INFO:root:[230,   950] training loss: 0.00128710
INFO:root:[230,  1000] training loss: 0.00003535
INFO:root:[230,  1050] training loss: 0.00002193
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch230
INFO:root:[231,    50] training loss: 0.00953213
INFO:root:[231,   100] training loss: 0.00728526
INFO:root:[231,   150] training loss: 0.00773107
INFO:root:[231,   200] training loss: 0.00726725
INFO:root:[231,   250] training loss: 0.00646176
INFO:root:[231,   300] training loss: 0.00773068
INFO:root:[231,   350] training loss: 0.00639010
INFO:root:[231,   400] training loss: 0.00001740
INFO:root:[231,   450] training loss: 0.00001833
INFO:root:[231,   500] training loss: 0.00002662
INFO:root:[231,   550] training loss: 0.00026285
INFO:root:[231,   600] training loss: 0.00017009
INFO:root:[231,   650] training loss: 0.00001996
INFO:root:[231,   700] training loss: 0.00002520
INFO:root:[231,   750] training loss: 0.00034430
INFO:root:[231,   800] training loss: 0.00050509
INFO:root:[231,   850] training loss: 0.00067487
INFO:root:[231,   900] training loss: 0.00668585
INFO:root:[231,   950] training loss: 0.00146722
INFO:root:[231,  1000] training loss: 0.00002942
INFO:root:[231,  1050] training loss: 0.00002198
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch231
INFO:root:[232,    50] training loss: 0.00798653
INFO:root:[232,   100] training loss: 0.00731835
INFO:root:[232,   150] training loss: 0.00826548
INFO:root:[232,   200] training loss: 0.00747911
INFO:root:[232,   250] training loss: 0.00607905
INFO:root:[232,   300] training loss: 0.00811250
INFO:root:[232,   350] training loss: 0.00635387
INFO:root:[232,   400] training loss: 0.00001691
INFO:root:[232,   450] training loss: 0.00001746
INFO:root:[232,   500] training loss: 0.00002876
INFO:root:[232,   550] training loss: 0.00029741
INFO:root:[232,   600] training loss: 0.00014779
INFO:root:[232,   650] training loss: 0.00002232
INFO:root:[232,   700] training loss: 0.00002490
INFO:root:[232,   750] training loss: 0.00036603
INFO:root:[232,   800] training loss: 0.00050484
INFO:root:[232,   850] training loss: 0.00036675
INFO:root:[232,   900] training loss: 0.00591564
INFO:root:[232,   950] training loss: 0.00139370
INFO:root:[232,  1000] training loss: 0.00003050
INFO:root:[232,  1050] training loss: 0.00002756
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch232
INFO:root:[233,    50] training loss: 0.00812165
INFO:root:[233,   100] training loss: 0.00725706
INFO:root:[233,   150] training loss: 0.00766460
INFO:root:[233,   200] training loss: 0.00678420
INFO:root:[233,   250] training loss: 0.00656646
INFO:root:[233,   300] training loss: 0.00799200
INFO:root:[233,   350] training loss: 0.00698130
INFO:root:[233,   400] training loss: 0.00001609
INFO:root:[233,   450] training loss: 0.00001903
INFO:root:[233,   500] training loss: 0.00002329
INFO:root:[233,   550] training loss: 0.00026069
INFO:root:[233,   600] training loss: 0.00018628
INFO:root:[233,   650] training loss: 0.00002194
INFO:root:[233,   700] training loss: 0.00001862
INFO:root:[233,   750] training loss: 0.00037626
INFO:root:[233,   800] training loss: 0.00044243
INFO:root:[233,   850] training loss: 0.00069257
INFO:root:[233,   900] training loss: 0.00541843
INFO:root:[233,   950] training loss: 0.00144679
INFO:root:[233,  1000] training loss: 0.00002859
INFO:root:[233,  1050] training loss: 0.00002289
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch233
INFO:root:[234,    50] training loss: 0.00836006
INFO:root:[234,   100] training loss: 0.00762161
INFO:root:[234,   150] training loss: 0.00813225
INFO:root:[234,   200] training loss: 0.00699739
INFO:root:[234,   250] training loss: 0.00609124
INFO:root:[234,   300] training loss: 0.00811616
INFO:root:[234,   350] training loss: 0.00672676
INFO:root:[234,   400] training loss: 0.00001791
INFO:root:[234,   450] training loss: 0.00001834
INFO:root:[234,   500] training loss: 0.00002861
INFO:root:[234,   550] training loss: 0.00028001
INFO:root:[234,   600] training loss: 0.00014665
INFO:root:[234,   650] training loss: 0.00001994
INFO:root:[234,   700] training loss: 0.00001710
INFO:root:[234,   750] training loss: 0.00030036
INFO:root:[234,   800] training loss: 0.00042109
INFO:root:[234,   850] training loss: 0.00049988
INFO:root:[234,   900] training loss: 0.00572364
INFO:root:[234,   950] training loss: 0.00142315
INFO:root:[234,  1000] training loss: 0.00003246
INFO:root:[234,  1050] training loss: 0.00002384
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch234
INFO:root:[235,    50] training loss: 0.00798014
INFO:root:[235,   100] training loss: 0.00782726
INFO:root:[235,   150] training loss: 0.00791343
INFO:root:[235,   200] training loss: 0.00662221
INFO:root:[235,   250] training loss: 0.00667438
INFO:root:[235,   300] training loss: 0.00789241
INFO:root:[235,   350] training loss: 0.00674731
INFO:root:[235,   400] training loss: 0.00001603
INFO:root:[235,   450] training loss: 0.00001677
INFO:root:[235,   500] training loss: 0.00002794
INFO:root:[235,   550] training loss: 0.00024014
INFO:root:[235,   600] training loss: 0.00013472
INFO:root:[235,   650] training loss: 0.00003758
INFO:root:[235,   700] training loss: 0.00001610
INFO:root:[235,   750] training loss: 0.00034773
INFO:root:[235,   800] training loss: 0.00059868
INFO:root:[235,   850] training loss: 0.00053077
INFO:root:[235,   900] training loss: 0.00555080
INFO:root:[235,   950] training loss: 0.00153089
INFO:root:[235,  1000] training loss: 0.00002562
INFO:root:[235,  1050] training loss: 0.00001873
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch235
INFO:root:[236,    50] training loss: 0.00811270
INFO:root:[236,   100] training loss: 0.00749776
INFO:root:[236,   150] training loss: 0.00753437
INFO:root:[236,   200] training loss: 0.00672696
INFO:root:[236,   250] training loss: 0.00642622
INFO:root:[236,   300] training loss: 0.00764791
INFO:root:[236,   350] training loss: 0.00649959
INFO:root:[236,   400] training loss: 0.00001615
INFO:root:[236,   450] training loss: 0.00001772
INFO:root:[236,   500] training loss: 0.00002746
INFO:root:[236,   550] training loss: 0.00036382
INFO:root:[236,   600] training loss: 0.00025268
INFO:root:[236,   650] training loss: 0.00002216
INFO:root:[236,   700] training loss: 0.00002091
INFO:root:[236,   750] training loss: 0.00026547
INFO:root:[236,   800] training loss: 0.00042447
INFO:root:[236,   850] training loss: 0.00051883
INFO:root:[236,   900] training loss: 0.00505146
INFO:root:[236,   950] training loss: 0.00147594
INFO:root:[236,  1000] training loss: 0.00002642
INFO:root:[236,  1050] training loss: 0.00002002
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch236
INFO:root:[237,    50] training loss: 0.00833101
INFO:root:[237,   100] training loss: 0.00736201
INFO:root:[237,   150] training loss: 0.00801631
INFO:root:[237,   200] training loss: 0.00755463
INFO:root:[237,   250] training loss: 0.00674318
INFO:root:[237,   300] training loss: 0.00781808
INFO:root:[237,   350] training loss: 0.00653101
INFO:root:[237,   400] training loss: 0.00001541
INFO:root:[237,   450] training loss: 0.00001840
INFO:root:[237,   500] training loss: 0.00002889
INFO:root:[237,   550] training loss: 0.00037430
INFO:root:[237,   600] training loss: 0.00015082
INFO:root:[237,   650] training loss: 0.00001806
INFO:root:[237,   700] training loss: 0.00001881
INFO:root:[237,   750] training loss: 0.00035169
INFO:root:[237,   800] training loss: 0.00042692
INFO:root:[237,   850] training loss: 0.00060005
INFO:root:[237,   900] training loss: 0.00531596
INFO:root:[237,   950] training loss: 0.00168275
INFO:root:[237,  1000] training loss: 0.00003608
INFO:root:[237,  1050] training loss: 0.00002460
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch237
INFO:root:[238,    50] training loss: 0.00872928
INFO:root:[238,   100] training loss: 0.00730434
INFO:root:[238,   150] training loss: 0.00786776
INFO:root:[238,   200] training loss: 0.00683799
INFO:root:[238,   250] training loss: 0.00680221
INFO:root:[238,   300] training loss: 0.00759774
INFO:root:[238,   350] training loss: 0.00655883
INFO:root:[238,   400] training loss: 0.00001960
INFO:root:[238,   450] training loss: 0.00003195
INFO:root:[238,   500] training loss: 0.00003789
INFO:root:[238,   550] training loss: 0.00027362
INFO:root:[238,   600] training loss: 0.00010991
INFO:root:[238,   650] training loss: 0.00002102
INFO:root:[238,   700] training loss: 0.00002122
INFO:root:[238,   750] training loss: 0.00038851
INFO:root:[238,   800] training loss: 0.00048902
INFO:root:[238,   850] training loss: 0.00052631
INFO:root:[238,   900] training loss: 0.00559642
INFO:root:[238,   950] training loss: 0.00152565
INFO:root:[238,  1000] training loss: 0.00003039
INFO:root:[238,  1050] training loss: 0.00002894
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch238
INFO:root:[239,    50] training loss: 0.00823516
INFO:root:[239,   100] training loss: 0.00748971
INFO:root:[239,   150] training loss: 0.00770334
INFO:root:[239,   200] training loss: 0.00662905
INFO:root:[239,   250] training loss: 0.00646191
INFO:root:[239,   300] training loss: 0.00808249
INFO:root:[239,   350] training loss: 0.00630628
INFO:root:[239,   400] training loss: 0.00002107
INFO:root:[239,   450] training loss: 0.00001968
INFO:root:[239,   500] training loss: 0.00003361
INFO:root:[239,   550] training loss: 0.00034992
INFO:root:[239,   600] training loss: 0.00016233
INFO:root:[239,   650] training loss: 0.00002503
INFO:root:[239,   700] training loss: 0.00002867
INFO:root:[239,   750] training loss: 0.00033851
INFO:root:[239,   800] training loss: 0.00047350
INFO:root:[239,   850] training loss: 0.00054199
INFO:root:[239,   900] training loss: 0.00629161
INFO:root:[239,   950] training loss: 0.00143550
INFO:root:[239,  1000] training loss: 0.00003113
INFO:root:[239,  1050] training loss: 0.00002001
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch239
INFO:root:[240,    50] training loss: 0.00881857
INFO:root:[240,   100] training loss: 0.00727805
INFO:root:[240,   150] training loss: 0.00775917
INFO:root:[240,   200] training loss: 0.00696704
INFO:root:[240,   250] training loss: 0.00651528
INFO:root:[240,   300] training loss: 0.00786578
INFO:root:[240,   350] training loss: 0.00658386
INFO:root:[240,   400] training loss: 0.00003062
INFO:root:[240,   450] training loss: 0.00002043
INFO:root:[240,   500] training loss: 0.00002410
INFO:root:[240,   550] training loss: 0.00037469
INFO:root:[240,   600] training loss: 0.00016470
INFO:root:[240,   650] training loss: 0.00003203
INFO:root:[240,   700] training loss: 0.00002209
INFO:root:[240,   750] training loss: 0.00061354
INFO:root:[240,   800] training loss: 0.00048617
INFO:root:[240,   850] training loss: 0.00057635
INFO:root:[240,   900] training loss: 0.00581929
INFO:root:[240,   950] training loss: 0.00165285
INFO:root:[240,  1000] training loss: 0.00004870
INFO:root:[240,  1050] training loss: 0.00002365
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch240
INFO:root:[241,    50] training loss: 0.00825126
INFO:root:[241,   100] training loss: 0.00781418
INFO:root:[241,   150] training loss: 0.00830021
INFO:root:[241,   200] training loss: 0.00671007
INFO:root:[241,   250] training loss: 0.00674096
INFO:root:[241,   300] training loss: 0.00826490
INFO:root:[241,   350] training loss: 0.00630389
INFO:root:[241,   400] training loss: 0.00001746
INFO:root:[241,   450] training loss: 0.00002034
INFO:root:[241,   500] training loss: 0.00002742
INFO:root:[241,   550] training loss: 0.00030693
INFO:root:[241,   600] training loss: 0.00013348
INFO:root:[241,   650] training loss: 0.00001841
INFO:root:[241,   700] training loss: 0.00002969
INFO:root:[241,   750] training loss: 0.00022913
INFO:root:[241,   800] training loss: 0.00040897
INFO:root:[241,   850] training loss: 0.00054092
INFO:root:[241,   900] training loss: 0.00638173
INFO:root:[241,   950] training loss: 0.00193491
INFO:root:[241,  1000] training loss: 0.00003303
INFO:root:[241,  1050] training loss: 0.00002193
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch241
INFO:root:[242,    50] training loss: 0.00847260
INFO:root:[242,   100] training loss: 0.00754295
INFO:root:[242,   150] training loss: 0.00774612
INFO:root:[242,   200] training loss: 0.00691663
INFO:root:[242,   250] training loss: 0.00842415
INFO:root:[242,   300] training loss: 0.00781810
INFO:root:[242,   350] training loss: 0.00595086
INFO:root:[242,   400] training loss: 0.00001692
INFO:root:[242,   450] training loss: 0.00001769
INFO:root:[242,   500] training loss: 0.00002521
INFO:root:[242,   550] training loss: 0.00027243
INFO:root:[242,   600] training loss: 0.00015513
INFO:root:[242,   650] training loss: 0.00002094
INFO:root:[242,   700] training loss: 0.00001628
INFO:root:[242,   750] training loss: 0.00033472
INFO:root:[242,   800] training loss: 0.00044416
INFO:root:[242,   850] training loss: 0.00055081
INFO:root:[242,   900] training loss: 0.00557301
INFO:root:[242,   950] training loss: 0.00172887
INFO:root:[242,  1000] training loss: 0.00002625
INFO:root:[242,  1050] training loss: 0.00002056
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch242
INFO:root:[243,    50] training loss: 0.00844542
INFO:root:[243,   100] training loss: 0.00754703
INFO:root:[243,   150] training loss: 0.00771547
INFO:root:[243,   200] training loss: 0.00710588
INFO:root:[243,   250] training loss: 0.00644277
INFO:root:[243,   300] training loss: 0.00785829
INFO:root:[243,   350] training loss: 0.00648220
INFO:root:[243,   400] training loss: 0.00001512
INFO:root:[243,   450] training loss: 0.00001860
INFO:root:[243,   500] training loss: 0.00003255
INFO:root:[243,   550] training loss: 0.00028171
INFO:root:[243,   600] training loss: 0.00016645
INFO:root:[243,   650] training loss: 0.00002338
INFO:root:[243,   700] training loss: 0.00002305
INFO:root:[243,   750] training loss: 0.00033376
INFO:root:[243,   800] training loss: 0.00044656
INFO:root:[243,   850] training loss: 0.00050807
INFO:root:[243,   900] training loss: 0.00605909
INFO:root:[243,   950] training loss: 0.00137006
INFO:root:[243,  1000] training loss: 0.00003127
INFO:root:[243,  1050] training loss: 0.00002237
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch243
INFO:root:[244,    50] training loss: 0.00887822
INFO:root:[244,   100] training loss: 0.00796290
INFO:root:[244,   150] training loss: 0.00797678
INFO:root:[244,   200] training loss: 0.00706528
INFO:root:[244,   250] training loss: 0.00627167
INFO:root:[244,   300] training loss: 0.00801453
INFO:root:[244,   350] training loss: 0.00697053
INFO:root:[244,   400] training loss: 0.00001706
INFO:root:[244,   450] training loss: 0.00001719
INFO:root:[244,   500] training loss: 0.00002817
INFO:root:[244,   550] training loss: 0.00027502
INFO:root:[244,   600] training loss: 0.00016816
INFO:root:[244,   650] training loss: 0.00002115
INFO:root:[244,   700] training loss: 0.00001972
INFO:root:[244,   750] training loss: 0.00041351
INFO:root:[244,   800] training loss: 0.00050410
INFO:root:[244,   850] training loss: 0.00051279
INFO:root:[244,   900] training loss: 0.00555708
INFO:root:[244,   950] training loss: 0.00128517
INFO:root:[244,  1000] training loss: 0.00002904
INFO:root:[244,  1050] training loss: 0.00002781
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch244
INFO:root:[245,    50] training loss: 0.00840823
INFO:root:[245,   100] training loss: 0.00735438
INFO:root:[245,   150] training loss: 0.00810212
INFO:root:[245,   200] training loss: 0.00667244
INFO:root:[245,   250] training loss: 0.00656609
INFO:root:[245,   300] training loss: 0.00792229
INFO:root:[245,   350] training loss: 0.00614252
INFO:root:[245,   400] training loss: 0.00003219
INFO:root:[245,   450] training loss: 0.00001751
INFO:root:[245,   500] training loss: 0.00002565
INFO:root:[245,   550] training loss: 0.00025466
INFO:root:[245,   600] training loss: 0.00011116
INFO:root:[245,   650] training loss: 0.00002108
INFO:root:[245,   700] training loss: 0.00002267
INFO:root:[245,   750] training loss: 0.00045857
INFO:root:[245,   800] training loss: 0.00051862
INFO:root:[245,   850] training loss: 0.00063590
INFO:root:[245,   900] training loss: 0.00503371
INFO:root:[245,   950] training loss: 0.00144248
INFO:root:[245,  1000] training loss: 0.00002622
INFO:root:[245,  1050] training loss: 0.00002028
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch245
INFO:root:[246,    50] training loss: 0.00793721
INFO:root:[246,   100] training loss: 0.00820092
INFO:root:[246,   150] training loss: 0.00888515
INFO:root:[246,   200] training loss: 0.00713361
INFO:root:[246,   250] training loss: 0.00644252
INFO:root:[246,   300] training loss: 0.00780904
INFO:root:[246,   350] training loss: 0.00660106
INFO:root:[246,   400] training loss: 0.00001737
INFO:root:[246,   450] training loss: 0.00002864
INFO:root:[246,   500] training loss: 0.00002496
INFO:root:[246,   550] training loss: 0.00047552
INFO:root:[246,   600] training loss: 0.00014383
INFO:root:[246,   650] training loss: 0.00001918
INFO:root:[246,   700] training loss: 0.00003003
INFO:root:[246,   750] training loss: 0.00031784
INFO:root:[246,   800] training loss: 0.00049212
INFO:root:[246,   850] training loss: 0.00052810
INFO:root:[246,   900] training loss: 0.00601629
INFO:root:[246,   950] training loss: 0.00144494
INFO:root:[246,  1000] training loss: 0.00002785
INFO:root:[246,  1050] training loss: 0.00001984
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch246
INFO:root:[247,    50] training loss: 0.00797102
INFO:root:[247,   100] training loss: 0.00761408
INFO:root:[247,   150] training loss: 0.00753590
INFO:root:[247,   200] training loss: 0.00674961
INFO:root:[247,   250] training loss: 0.00625857
INFO:root:[247,   300] training loss: 0.00773678
INFO:root:[247,   350] training loss: 0.00622031
INFO:root:[247,   400] training loss: 0.00002038
INFO:root:[247,   450] training loss: 0.00002137
INFO:root:[247,   500] training loss: 0.00002625
INFO:root:[247,   550] training loss: 0.00031206
INFO:root:[247,   600] training loss: 0.00017430
INFO:root:[247,   650] training loss: 0.00003518
INFO:root:[247,   700] training loss: 0.00002270
INFO:root:[247,   750] training loss: 0.00024510
INFO:root:[247,   800] training loss: 0.00052979
INFO:root:[247,   850] training loss: 0.00055114
INFO:root:[247,   900] training loss: 0.00515945
INFO:root:[247,   950] training loss: 0.00158595
INFO:root:[247,  1000] training loss: 0.00002993
INFO:root:[247,  1050] training loss: 0.00002236
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch247
INFO:root:[248,    50] training loss: 0.00773781
INFO:root:[248,   100] training loss: 0.00782515
INFO:root:[248,   150] training loss: 0.00749167
INFO:root:[248,   200] training loss: 0.00798764
INFO:root:[248,   250] training loss: 0.00647554
INFO:root:[248,   300] training loss: 0.00757251
INFO:root:[248,   350] training loss: 0.00625163
INFO:root:[248,   400] training loss: 0.00001473
INFO:root:[248,   450] training loss: 0.00001748
INFO:root:[248,   500] training loss: 0.00002620
INFO:root:[248,   550] training loss: 0.00024217
INFO:root:[248,   600] training loss: 0.00016149
INFO:root:[248,   650] training loss: 0.00004050
INFO:root:[248,   700] training loss: 0.00002390
INFO:root:[248,   750] training loss: 0.00032696
INFO:root:[248,   800] training loss: 0.00050029
INFO:root:[248,   850] training loss: 0.00047100
INFO:root:[248,   900] training loss: 0.00487790
INFO:root:[248,   950] training loss: 0.00159881
INFO:root:[248,  1000] training loss: 0.00003044
INFO:root:[248,  1050] training loss: 0.00002814
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch248
INFO:root:[249,    50] training loss: 0.00804306
INFO:root:[249,   100] training loss: 0.00783695
INFO:root:[249,   150] training loss: 0.00759683
INFO:root:[249,   200] training loss: 0.00742581
INFO:root:[249,   250] training loss: 0.00639502
INFO:root:[249,   300] training loss: 0.00782444
INFO:root:[249,   350] training loss: 0.00698257
INFO:root:[249,   400] training loss: 0.00001774
INFO:root:[249,   450] training loss: 0.00001787
INFO:root:[249,   500] training loss: 0.00002597
INFO:root:[249,   550] training loss: 0.00032268
INFO:root:[249,   600] training loss: 0.00015535
INFO:root:[249,   650] training loss: 0.00002332
INFO:root:[249,   700] training loss: 0.00002136
INFO:root:[249,   750] training loss: 0.00033271
INFO:root:[249,   800] training loss: 0.00053661
INFO:root:[249,   850] training loss: 0.00058416
INFO:root:[249,   900] training loss: 0.00541049
INFO:root:[249,   950] training loss: 0.00136262
INFO:root:[249,  1000] training loss: 0.00004511
INFO:root:[249,  1050] training loss: 0.00002124
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch249
INFO:root:[250,    50] training loss: 0.00875827
INFO:root:[250,   100] training loss: 0.00794890
INFO:root:[250,   150] training loss: 0.00728475
INFO:root:[250,   200] training loss: 0.00722421
INFO:root:[250,   250] training loss: 0.00627324
INFO:root:[250,   300] training loss: 0.00755014
INFO:root:[250,   350] training loss: 0.00639670
INFO:root:[250,   400] training loss: 0.00001664
INFO:root:[250,   450] training loss: 0.00001854
INFO:root:[250,   500] training loss: 0.00002661
INFO:root:[250,   550] training loss: 0.00032610
INFO:root:[250,   600] training loss: 0.00016675
INFO:root:[250,   650] training loss: 0.00003216
INFO:root:[250,   700] training loss: 0.00002296
INFO:root:[250,   750] training loss: 0.00036762
INFO:root:[250,   800] training loss: 0.00053319
INFO:root:[250,   850] training loss: 0.00070532
INFO:root:[250,   900] training loss: 0.00529129
INFO:root:[250,   950] training loss: 0.00161116
INFO:root:[250,  1000] training loss: 0.00003821
INFO:root:[250,  1050] training loss: 0.00003312
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch250
INFO:root:[251,    50] training loss: 0.00797958
INFO:root:[251,   100] training loss: 0.00743156
INFO:root:[251,   150] training loss: 0.00780904
INFO:root:[251,   200] training loss: 0.00772362
INFO:root:[251,   250] training loss: 0.00647035
INFO:root:[251,   300] training loss: 0.00782055
INFO:root:[251,   350] training loss: 0.00616250
INFO:root:[251,   400] training loss: 0.00002046
INFO:root:[251,   450] training loss: 0.00001619
INFO:root:[251,   500] training loss: 0.00002508
INFO:root:[251,   550] training loss: 0.00030790
INFO:root:[251,   600] training loss: 0.00016137
INFO:root:[251,   650] training loss: 0.00002192
INFO:root:[251,   700] training loss: 0.00001806
INFO:root:[251,   750] training loss: 0.00032422
INFO:root:[251,   800] training loss: 0.00042692
INFO:root:[251,   850] training loss: 0.00055705
INFO:root:[251,   900] training loss: 0.00516143
INFO:root:[251,   950] training loss: 0.00143202
INFO:root:[251,  1000] training loss: 0.00002639
INFO:root:[251,  1050] training loss: 0.00002650
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch251
INFO:root:[252,    50] training loss: 0.00754247
INFO:root:[252,   100] training loss: 0.00751593
INFO:root:[252,   150] training loss: 0.00747656
INFO:root:[252,   200] training loss: 0.00683760
INFO:root:[252,   250] training loss: 0.00646156
INFO:root:[252,   300] training loss: 0.00791279
INFO:root:[252,   350] training loss: 0.00689108
INFO:root:[252,   400] training loss: 0.00001615
INFO:root:[252,   450] training loss: 0.00001600
INFO:root:[252,   500] training loss: 0.00002564
INFO:root:[252,   550] training loss: 0.00029277
INFO:root:[252,   600] training loss: 0.00013104
INFO:root:[252,   650] training loss: 0.00002680
INFO:root:[252,   700] training loss: 0.00002949
INFO:root:[252,   750] training loss: 0.00036206
INFO:root:[252,   800] training loss: 0.00047250
INFO:root:[252,   850] training loss: 0.00056519
INFO:root:[252,   900] training loss: 0.00513144
INFO:root:[252,   950] training loss: 0.00172914
INFO:root:[252,  1000] training loss: 0.00004184
INFO:root:[252,  1050] training loss: 0.00002697
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch252
INFO:root:[253,    50] training loss: 0.00782844
INFO:root:[253,   100] training loss: 0.00742093
INFO:root:[253,   150] training loss: 0.00847141
INFO:root:[253,   200] training loss: 0.01205787
INFO:root:[253,   250] training loss: 0.00680418
INFO:root:[253,   300] training loss: 0.00779481
INFO:root:[253,   350] training loss: 0.00656998
INFO:root:[253,   400] training loss: 0.00001600
INFO:root:[253,   450] training loss: 0.00001928
INFO:root:[253,   500] training loss: 0.00003088
INFO:root:[253,   550] training loss: 0.00031950
INFO:root:[253,   600] training loss: 0.00016563
INFO:root:[253,   650] training loss: 0.00002125
INFO:root:[253,   700] training loss: 0.00002115
INFO:root:[253,   750] training loss: 0.00026843
INFO:root:[253,   800] training loss: 0.00043045
INFO:root:[253,   850] training loss: 0.00054018
INFO:root:[253,   900] training loss: 0.00499779
INFO:root:[253,   950] training loss: 0.00148867
INFO:root:[253,  1000] training loss: 0.00002578
INFO:root:[253,  1050] training loss: 0.00003009
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch253
INFO:root:[254,    50] training loss: 0.00888749
INFO:root:[254,   100] training loss: 0.00732149
INFO:root:[254,   150] training loss: 0.00745657
INFO:root:[254,   200] training loss: 0.00704790
INFO:root:[254,   250] training loss: 0.00671409
INFO:root:[254,   300] training loss: 0.00808212
INFO:root:[254,   350] training loss: 0.00656949
INFO:root:[254,   400] training loss: 0.00001539
INFO:root:[254,   450] training loss: 0.00001791
INFO:root:[254,   500] training loss: 0.00003121
INFO:root:[254,   550] training loss: 0.00025066
INFO:root:[254,   600] training loss: 0.00016298
INFO:root:[254,   650] training loss: 0.00002834
INFO:root:[254,   700] training loss: 0.00002445
INFO:root:[254,   750] training loss: 0.00040695
INFO:root:[254,   800] training loss: 0.00056701
INFO:root:[254,   850] training loss: 0.00063381
INFO:root:[254,   900] training loss: 0.00844075
INFO:root:[254,   950] training loss: 0.00140214
INFO:root:[254,  1000] training loss: 0.00003030
INFO:root:[254,  1050] training loss: 0.00002000
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch254
INFO:root:[255,    50] training loss: 0.00875110
INFO:root:[255,   100] training loss: 0.00750662
INFO:root:[255,   150] training loss: 0.00842524
INFO:root:[255,   200] training loss: 0.00698318
INFO:root:[255,   250] training loss: 0.00719559
INFO:root:[255,   300] training loss: 0.00810334
INFO:root:[255,   350] training loss: 0.00669286
INFO:root:[255,   400] training loss: 0.00001463
INFO:root:[255,   450] training loss: 0.00002257
INFO:root:[255,   500] training loss: 0.00002980
INFO:root:[255,   550] training loss: 0.00032604
INFO:root:[255,   600] training loss: 0.00016721
INFO:root:[255,   650] training loss: 0.00002144
INFO:root:[255,   700] training loss: 0.00001752
INFO:root:[255,   750] training loss: 0.00040923
INFO:root:[255,   800] training loss: 0.00055616
INFO:root:[255,   850] training loss: 0.00045934
INFO:root:[255,   900] training loss: 0.00558336
INFO:root:[255,   950] training loss: 0.00153563
INFO:root:[255,  1000] training loss: 0.00003170
INFO:root:[255,  1050] training loss: 0.00002027
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch255
INFO:root:[256,    50] training loss: 0.00887614
INFO:root:[256,   100] training loss: 0.00737645
INFO:root:[256,   150] training loss: 0.00757322
INFO:root:[256,   200] training loss: 0.00889186
INFO:root:[256,   250] training loss: 0.00642140
INFO:root:[256,   300] training loss: 0.00762848
INFO:root:[256,   350] training loss: 0.00610726
INFO:root:[256,   400] training loss: 0.00001769
INFO:root:[256,   450] training loss: 0.00001783
INFO:root:[256,   500] training loss: 0.00003912
INFO:root:[256,   550] training loss: 0.00027842
INFO:root:[256,   600] training loss: 0.00013268
INFO:root:[256,   650] training loss: 0.00002102
INFO:root:[256,   700] training loss: 0.00002053
INFO:root:[256,   750] training loss: 0.00040315
INFO:root:[256,   800] training loss: 0.00051762
INFO:root:[256,   850] training loss: 0.00059463
INFO:root:[256,   900] training loss: 0.00491498
INFO:root:[256,   950] training loss: 0.00147575
INFO:root:[256,  1000] training loss: 0.00003516
INFO:root:[256,  1050] training loss: 0.00002150
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch256
INFO:root:[257,    50] training loss: 0.00818571
INFO:root:[257,   100] training loss: 0.00747812
INFO:root:[257,   150] training loss: 0.00745833
INFO:root:[257,   200] training loss: 0.00742822
INFO:root:[257,   250] training loss: 0.00716169
INFO:root:[257,   300] training loss: 0.00785064
INFO:root:[257,   350] training loss: 0.00687269
INFO:root:[257,   400] training loss: 0.00001483
INFO:root:[257,   450] training loss: 0.00002946
INFO:root:[257,   500] training loss: 0.00002960
INFO:root:[257,   550] training loss: 0.00031245
INFO:root:[257,   600] training loss: 0.00012803
INFO:root:[257,   650] training loss: 0.00003547
INFO:root:[257,   700] training loss: 0.00002146
INFO:root:[257,   750] training loss: 0.00032542
INFO:root:[257,   800] training loss: 0.00054006
INFO:root:[257,   850] training loss: 0.00051655
INFO:root:[257,   900] training loss: 0.00505860
INFO:root:[257,   950] training loss: 0.00184684
INFO:root:[257,  1000] training loss: 0.00003056
INFO:root:[257,  1050] training loss: 0.00002430
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch257
INFO:root:[258,    50] training loss: 0.00940141
INFO:root:[258,   100] training loss: 0.00743825
INFO:root:[258,   150] training loss: 0.00823451
INFO:root:[258,   200] training loss: 0.00772864
INFO:root:[258,   250] training loss: 0.00636765
INFO:root:[258,   300] training loss: 0.00797196
INFO:root:[258,   350] training loss: 0.00656655
INFO:root:[258,   400] training loss: 0.00002420
INFO:root:[258,   450] training loss: 0.00001662
INFO:root:[258,   500] training loss: 0.00002777
INFO:root:[258,   550] training loss: 0.00029184
INFO:root:[258,   600] training loss: 0.00015479
INFO:root:[258,   650] training loss: 0.00002490
INFO:root:[258,   700] training loss: 0.00001860
INFO:root:[258,   750] training loss: 0.00033517
INFO:root:[258,   800] training loss: 0.00058641
INFO:root:[258,   850] training loss: 0.00057495
INFO:root:[258,   900] training loss: 0.00514781
INFO:root:[258,   950] training loss: 0.00148322
INFO:root:[258,  1000] training loss: 0.00004884
INFO:root:[258,  1050] training loss: 0.00002048
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch258
INFO:root:[259,    50] training loss: 0.00819125
INFO:root:[259,   100] training loss: 0.00743272
INFO:root:[259,   150] training loss: 0.00759932
INFO:root:[259,   200] training loss: 0.00674460
INFO:root:[259,   250] training loss: 0.00668234
INFO:root:[259,   300] training loss: 0.00774371
INFO:root:[259,   350] training loss: 0.00649595
INFO:root:[259,   400] training loss: 0.00001520
INFO:root:[259,   450] training loss: 0.00001811
INFO:root:[259,   500] training loss: 0.00002611
INFO:root:[259,   550] training loss: 0.00050644
INFO:root:[259,   600] training loss: 0.00017317
INFO:root:[259,   650] training loss: 0.00002771
INFO:root:[259,   700] training loss: 0.00001917
INFO:root:[259,   750] training loss: 0.00040729
INFO:root:[259,   800] training loss: 0.00052115
INFO:root:[259,   850] training loss: 0.00054594
INFO:root:[259,   900] training loss: 0.00565247
INFO:root:[259,   950] training loss: 0.00171041
INFO:root:[259,  1000] training loss: 0.00003759
INFO:root:[259,  1050] training loss: 0.00003306
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch259
INFO:root:[260,    50] training loss: 0.00820115
INFO:root:[260,   100] training loss: 0.00721250
INFO:root:[260,   150] training loss: 0.00781261
INFO:root:[260,   200] training loss: 0.00752085
INFO:root:[260,   250] training loss: 0.00942008
INFO:root:[260,   300] training loss: 0.00827548
INFO:root:[260,   350] training loss: 0.00630066
INFO:root:[260,   400] training loss: 0.00001688
INFO:root:[260,   450] training loss: 0.00001625
INFO:root:[260,   500] training loss: 0.00002509
INFO:root:[260,   550] training loss: 0.00031872
INFO:root:[260,   600] training loss: 0.00016824
INFO:root:[260,   650] training loss: 0.00003814
INFO:root:[260,   700] training loss: 0.00002390
INFO:root:[260,   750] training loss: 0.00037346
INFO:root:[260,   800] training loss: 0.00042799
INFO:root:[260,   850] training loss: 0.00051944
INFO:root:[260,   900] training loss: 0.00496870
INFO:root:[260,   950] training loss: 0.00138250
INFO:root:[260,  1000] training loss: 0.00002909
INFO:root:[260,  1050] training loss: 0.00002688
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch260
INFO:root:[261,    50] training loss: 0.00794330
INFO:root:[261,   100] training loss: 0.00733408
INFO:root:[261,   150] training loss: 0.00747694
INFO:root:[261,   200] training loss: 0.00701557
INFO:root:[261,   250] training loss: 0.00606069
INFO:root:[261,   300] training loss: 0.00787506
INFO:root:[261,   350] training loss: 0.00658099
INFO:root:[261,   400] training loss: 0.00001716
INFO:root:[261,   450] training loss: 0.00006467
INFO:root:[261,   500] training loss: 0.00002717
INFO:root:[261,   550] training loss: 0.00030405
INFO:root:[261,   600] training loss: 0.00014339
INFO:root:[261,   650] training loss: 0.00003226
INFO:root:[261,   700] training loss: 0.00002222
INFO:root:[261,   750] training loss: 0.00041498
INFO:root:[261,   800] training loss: 0.00040939
INFO:root:[261,   850] training loss: 0.00059449
INFO:root:[261,   900] training loss: 0.00598912
INFO:root:[261,   950] training loss: 0.00167866
INFO:root:[261,  1000] training loss: 0.00003139
INFO:root:[261,  1050] training loss: 0.00002938
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch261
INFO:root:[262,    50] training loss: 0.00894828
INFO:root:[262,   100] training loss: 0.00786854
INFO:root:[262,   150] training loss: 0.00806231
INFO:root:[262,   200] training loss: 0.00881255
INFO:root:[262,   250] training loss: 0.00621274
INFO:root:[262,   300] training loss: 0.00788477
INFO:root:[262,   350] training loss: 0.00628073
INFO:root:[262,   400] training loss: 0.00001875
INFO:root:[262,   450] training loss: 0.00001915
INFO:root:[262,   500] training loss: 0.00002547
INFO:root:[262,   550] training loss: 0.00033365
INFO:root:[262,   600] training loss: 0.00016301
INFO:root:[262,   650] training loss: 0.00002313
INFO:root:[262,   700] training loss: 0.00001934
INFO:root:[262,   750] training loss: 0.00026448
INFO:root:[262,   800] training loss: 0.00034612
INFO:root:[262,   850] training loss: 0.00045341
INFO:root:[262,   900] training loss: 0.00522044
INFO:root:[262,   950] training loss: 0.00153849
INFO:root:[262,  1000] training loss: 0.00002614
INFO:root:[262,  1050] training loss: 0.00002315
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch262
INFO:root:[263,    50] training loss: 0.00821307
INFO:root:[263,   100] training loss: 0.00749939
INFO:root:[263,   150] training loss: 0.00775468
INFO:root:[263,   200] training loss: 0.00758634
INFO:root:[263,   250] training loss: 0.00659662
INFO:root:[263,   300] training loss: 0.00744118
INFO:root:[263,   350] training loss: 0.00671304
INFO:root:[263,   400] training loss: 0.00002561
INFO:root:[263,   450] training loss: 0.00001685
INFO:root:[263,   500] training loss: 0.00002698
INFO:root:[263,   550] training loss: 0.00027417
INFO:root:[263,   600] training loss: 0.00012148
INFO:root:[263,   650] training loss: 0.00001985
INFO:root:[263,   700] training loss: 0.00002309
INFO:root:[263,   750] training loss: 0.00036569
INFO:root:[263,   800] training loss: 0.00045457
INFO:root:[263,   850] training loss: 0.00039514
INFO:root:[263,   900] training loss: 0.00565025
INFO:root:[263,   950] training loss: 0.00123055
INFO:root:[263,  1000] training loss: 0.00002796
INFO:root:[263,  1050] training loss: 0.00002790
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch263
INFO:root:[264,    50] training loss: 0.00864741
INFO:root:[264,   100] training loss: 0.00824439
INFO:root:[264,   150] training loss: 0.00783097
INFO:root:[264,   200] training loss: 0.00687270
INFO:root:[264,   250] training loss: 0.00666821
INFO:root:[264,   300] training loss: 0.00799766
INFO:root:[264,   350] training loss: 0.00650271
INFO:root:[264,   400] training loss: 0.00001633
INFO:root:[264,   450] training loss: 0.00001683
INFO:root:[264,   500] training loss: 0.00002666
INFO:root:[264,   550] training loss: 0.00029614
INFO:root:[264,   600] training loss: 0.00014111
INFO:root:[264,   650] training loss: 0.00002142
INFO:root:[264,   700] training loss: 0.00004617
INFO:root:[264,   750] training loss: 0.00029251
INFO:root:[264,   800] training loss: 0.00041982
INFO:root:[264,   850] training loss: 0.00039907
INFO:root:[264,   900] training loss: 0.00575802
INFO:root:[264,   950] training loss: 0.00175089
INFO:root:[264,  1000] training loss: 0.00003908
INFO:root:[264,  1050] training loss: 0.00002147
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch264
INFO:root:[265,    50] training loss: 0.00824071
INFO:root:[265,   100] training loss: 0.00723213
INFO:root:[265,   150] training loss: 0.00865569
INFO:root:[265,   200] training loss: 0.00699504
INFO:root:[265,   250] training loss: 0.00635763
INFO:root:[265,   300] training loss: 0.00789748
INFO:root:[265,   350] training loss: 0.00607309
INFO:root:[265,   400] training loss: 0.00001965
INFO:root:[265,   450] training loss: 0.00001744
INFO:root:[265,   500] training loss: 0.00002746
INFO:root:[265,   550] training loss: 0.00028789
INFO:root:[265,   600] training loss: 0.00014720
INFO:root:[265,   650] training loss: 0.00003833
INFO:root:[265,   700] training loss: 0.00002363
INFO:root:[265,   750] training loss: 0.00033779
INFO:root:[265,   800] training loss: 0.00052463
INFO:root:[265,   850] training loss: 0.00048168
INFO:root:[265,   900] training loss: 0.00513536
INFO:root:[265,   950] training loss: 0.00190119
INFO:root:[265,  1000] training loss: 0.00002894
INFO:root:[265,  1050] training loss: 0.00002247
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch265
INFO:root:[266,    50] training loss: 0.00821956
INFO:root:[266,   100] training loss: 0.00741548
INFO:root:[266,   150] training loss: 0.00786912
INFO:root:[266,   200] training loss: 0.00700485
INFO:root:[266,   250] training loss: 0.00594339
INFO:root:[266,   300] training loss: 0.00788723
INFO:root:[266,   350] training loss: 0.00686672
INFO:root:[266,   400] training loss: 0.00002466
INFO:root:[266,   450] training loss: 0.00004404
INFO:root:[266,   500] training loss: 0.00003582
INFO:root:[266,   550] training loss: 0.00028288
INFO:root:[266,   600] training loss: 0.00016097
INFO:root:[266,   650] training loss: 0.00002221
INFO:root:[266,   700] training loss: 0.00002110
INFO:root:[266,   750] training loss: 0.00031882
INFO:root:[266,   800] training loss: 0.00066693
INFO:root:[266,   850] training loss: 0.00051324
INFO:root:[266,   900] training loss: 0.00503081
INFO:root:[266,   950] training loss: 0.00149717
INFO:root:[266,  1000] training loss: 0.00003483
INFO:root:[266,  1050] training loss: 0.00001940
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch266
INFO:root:[267,    50] training loss: 0.00830733
INFO:root:[267,   100] training loss: 0.00796369
INFO:root:[267,   150] training loss: 0.00753741
INFO:root:[267,   200] training loss: 0.00678188
INFO:root:[267,   250] training loss: 0.00636412
INFO:root:[267,   300] training loss: 0.00797459
INFO:root:[267,   350] training loss: 0.00743538
INFO:root:[267,   400] training loss: 0.00001611
INFO:root:[267,   450] training loss: 0.00002025
INFO:root:[267,   500] training loss: 0.00002502
INFO:root:[267,   550] training loss: 0.00030168
INFO:root:[267,   600] training loss: 0.00014971
INFO:root:[267,   650] training loss: 0.00004987
INFO:root:[267,   700] training loss: 0.00001962
INFO:root:[267,   750] training loss: 0.00026571
INFO:root:[267,   800] training loss: 0.00043781
INFO:root:[267,   850] training loss: 0.00051824
INFO:root:[267,   900] training loss: 0.00723397
INFO:root:[267,   950] training loss: 0.00144651
INFO:root:[267,  1000] training loss: 0.00002941
INFO:root:[267,  1050] training loss: 0.00003117
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch267
INFO:root:[268,    50] training loss: 0.00789133
INFO:root:[268,   100] training loss: 0.00818269
INFO:root:[268,   150] training loss: 0.00773505
INFO:root:[268,   200] training loss: 0.00822732
INFO:root:[268,   250] training loss: 0.00632958
INFO:root:[268,   300] training loss: 0.00771514
INFO:root:[268,   350] training loss: 0.00620960
INFO:root:[268,   400] training loss: 0.00001643
INFO:root:[268,   450] training loss: 0.00001479
INFO:root:[268,   500] training loss: 0.00002993
INFO:root:[268,   550] training loss: 0.00030533
INFO:root:[268,   600] training loss: 0.00015449
INFO:root:[268,   650] training loss: 0.00002020
INFO:root:[268,   700] training loss: 0.00002478
INFO:root:[268,   750] training loss: 0.00037098
INFO:root:[268,   800] training loss: 0.00039661
INFO:root:[268,   850] training loss: 0.00065827
INFO:root:[268,   900] training loss: 0.00519693
INFO:root:[268,   950] training loss: 0.00152344
INFO:root:[268,  1000] training loss: 0.00003083
INFO:root:[268,  1050] training loss: 0.00002419
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch268
INFO:root:[269,    50] training loss: 0.00817148
INFO:root:[269,   100] training loss: 0.00729150
INFO:root:[269,   150] training loss: 0.00717240
INFO:root:[269,   200] training loss: 0.00679582
INFO:root:[269,   250] training loss: 0.00602943
INFO:root:[269,   300] training loss: 0.00754921
INFO:root:[269,   350] training loss: 0.00634243
INFO:root:[269,   400] training loss: 0.00001641
INFO:root:[269,   450] training loss: 0.00001942
INFO:root:[269,   500] training loss: 0.00003240
INFO:root:[269,   550] training loss: 0.00028810
INFO:root:[269,   600] training loss: 0.00020566
INFO:root:[269,   650] training loss: 0.00005496
INFO:root:[269,   700] training loss: 0.00001981
INFO:root:[269,   750] training loss: 0.00035812
INFO:root:[269,   800] training loss: 0.00048381
INFO:root:[269,   850] training loss: 0.00036601
INFO:root:[269,   900] training loss: 0.00580324
INFO:root:[269,   950] training loss: 0.00166530
INFO:root:[269,  1000] training loss: 0.00003332
INFO:root:[269,  1050] training loss: 0.00002541
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch269
INFO:root:[270,    50] training loss: 0.00899957
INFO:root:[270,   100] training loss: 0.00799365
INFO:root:[270,   150] training loss: 0.00814162
INFO:root:[270,   200] training loss: 0.00676525
INFO:root:[270,   250] training loss: 0.00620047
INFO:root:[270,   300] training loss: 0.00840007
INFO:root:[270,   350] training loss: 0.00645983
INFO:root:[270,   400] training loss: 0.00001766
INFO:root:[270,   450] training loss: 0.00001924
INFO:root:[270,   500] training loss: 0.00002508
INFO:root:[270,   550] training loss: 0.00021846
INFO:root:[270,   600] training loss: 0.00014864
INFO:root:[270,   650] training loss: 0.00004177
INFO:root:[270,   700] training loss: 0.00001921
INFO:root:[270,   750] training loss: 0.00038427
INFO:root:[270,   800] training loss: 0.00046071
INFO:root:[270,   850] training loss: 0.00062407
INFO:root:[270,   900] training loss: 0.00530768
INFO:root:[270,   950] training loss: 0.00175734
INFO:root:[270,  1000] training loss: 0.00003120
INFO:root:[270,  1050] training loss: 0.00002127
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch270
INFO:root:[271,    50] training loss: 0.00889688
INFO:root:[271,   100] training loss: 0.00734579
INFO:root:[271,   150] training loss: 0.00785246
INFO:root:[271,   200] training loss: 0.00675276
INFO:root:[271,   250] training loss: 0.00612406
INFO:root:[271,   300] training loss: 0.00838675
INFO:root:[271,   350] training loss: 0.00698800
INFO:root:[271,   400] training loss: 0.00001777
INFO:root:[271,   450] training loss: 0.00001799
INFO:root:[271,   500] training loss: 0.00002810
INFO:root:[271,   550] training loss: 0.00023733
INFO:root:[271,   600] training loss: 0.00013602
INFO:root:[271,   650] training loss: 0.00002609
INFO:root:[271,   700] training loss: 0.00002696
INFO:root:[271,   750] training loss: 0.00031933
INFO:root:[271,   800] training loss: 0.00052607
INFO:root:[271,   850] training loss: 0.00049481
INFO:root:[271,   900] training loss: 0.00616508
INFO:root:[271,   950] training loss: 0.00154622
INFO:root:[271,  1000] training loss: 0.00002565
INFO:root:[271,  1050] training loss: 0.00006421
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch271
INFO:root:[272,    50] training loss: 0.00798214
INFO:root:[272,   100] training loss: 0.00736949
INFO:root:[272,   150] training loss: 0.00774326
INFO:root:[272,   200] training loss: 0.00683649
INFO:root:[272,   250] training loss: 0.00625882
INFO:root:[272,   300] training loss: 0.00804235
INFO:root:[272,   350] training loss: 0.00629423
INFO:root:[272,   400] training loss: 0.00001638
INFO:root:[272,   450] training loss: 0.00002151
INFO:root:[272,   500] training loss: 0.00002692
INFO:root:[272,   550] training loss: 0.00027979
INFO:root:[272,   600] training loss: 0.00017961
INFO:root:[272,   650] training loss: 0.00002947
INFO:root:[272,   700] training loss: 0.00001973
INFO:root:[272,   750] training loss: 0.00034100
INFO:root:[272,   800] training loss: 0.00039544
INFO:root:[272,   850] training loss: 0.00065081
INFO:root:[272,   900] training loss: 0.00647618
INFO:root:[272,   950] training loss: 0.00204405
INFO:root:[272,  1000] training loss: 0.00003392
INFO:root:[272,  1050] training loss: 0.00001912
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch272
INFO:root:[273,    50] training loss: 0.00868708
INFO:root:[273,   100] training loss: 0.00726471
INFO:root:[273,   150] training loss: 0.00781083
INFO:root:[273,   200] training loss: 0.00749467
INFO:root:[273,   250] training loss: 0.00656689
INFO:root:[273,   300] training loss: 0.00798728
INFO:root:[273,   350] training loss: 0.00647412
INFO:root:[273,   400] training loss: 0.00001566
INFO:root:[273,   450] training loss: 0.00006323
INFO:root:[273,   500] training loss: 0.00004035
INFO:root:[273,   550] training loss: 0.00030902
INFO:root:[273,   600] training loss: 0.00018724
INFO:root:[273,   650] training loss: 0.00002933
INFO:root:[273,   700] training loss: 0.00002784
INFO:root:[273,   750] training loss: 0.00030732
INFO:root:[273,   800] training loss: 0.00050344
INFO:root:[273,   850] training loss: 0.00060237
INFO:root:[273,   900] training loss: 0.00482451
INFO:root:[273,   950] training loss: 0.00152231
INFO:root:[273,  1000] training loss: 0.00003680
INFO:root:[273,  1050] training loss: 0.00002268
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch273
INFO:root:[274,    50] training loss: 0.00835261
INFO:root:[274,   100] training loss: 0.00743454
INFO:root:[274,   150] training loss: 0.00797452
INFO:root:[274,   200] training loss: 0.00659310
INFO:root:[274,   250] training loss: 0.00637225
INFO:root:[274,   300] training loss: 0.00861776
INFO:root:[274,   350] training loss: 0.00628772
INFO:root:[274,   400] training loss: 0.00001743
INFO:root:[274,   450] training loss: 0.00001626
INFO:root:[274,   500] training loss: 0.00002840
INFO:root:[274,   550] training loss: 0.00029291
INFO:root:[274,   600] training loss: 0.00015085
INFO:root:[274,   650] training loss: 0.00002476
INFO:root:[274,   700] training loss: 0.00002372
INFO:root:[274,   750] training loss: 0.00025932
INFO:root:[274,   800] training loss: 0.00044577
INFO:root:[274,   850] training loss: 0.00041538
INFO:root:[274,   900] training loss: 0.00561227
INFO:root:[274,   950] training loss: 0.00157243
INFO:root:[274,  1000] training loss: 0.00002842
INFO:root:[274,  1050] training loss: 0.00002420
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch274
INFO:root:[275,    50] training loss: 0.00814742
INFO:root:[275,   100] training loss: 0.00788705
INFO:root:[275,   150] training loss: 0.00859378
INFO:root:[275,   200] training loss: 0.00798993
INFO:root:[275,   250] training loss: 0.00603347
INFO:root:[275,   300] training loss: 0.00840112
INFO:root:[275,   350] training loss: 0.00651396
INFO:root:[275,   400] training loss: 0.00002287
INFO:root:[275,   450] training loss: 0.00001747
INFO:root:[275,   500] training loss: 0.00002880
INFO:root:[275,   550] training loss: 0.00027606
INFO:root:[275,   600] training loss: 0.00014276
INFO:root:[275,   650] training loss: 0.00002211
INFO:root:[275,   700] training loss: 0.00003169
INFO:root:[275,   750] training loss: 0.00025253
INFO:root:[275,   800] training loss: 0.00040540
INFO:root:[275,   850] training loss: 0.00054254
INFO:root:[275,   900] training loss: 0.00564208
INFO:root:[275,   950] training loss: 0.00147361
INFO:root:[275,  1000] training loss: 0.00002760
INFO:root:[275,  1050] training loss: 0.00002364
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch275
INFO:root:[276,    50] training loss: 0.00996883
INFO:root:[276,   100] training loss: 0.00734874
INFO:root:[276,   150] training loss: 0.00779152
INFO:root:[276,   200] training loss: 0.00883040
INFO:root:[276,   250] training loss: 0.00630492
INFO:root:[276,   300] training loss: 0.00768217
INFO:root:[276,   350] training loss: 0.00649399
INFO:root:[276,   400] training loss: 0.00001584
INFO:root:[276,   450] training loss: 0.00001990
INFO:root:[276,   500] training loss: 0.00002473
INFO:root:[276,   550] training loss: 0.00030803
INFO:root:[276,   600] training loss: 0.00012796
INFO:root:[276,   650] training loss: 0.00001972
INFO:root:[276,   700] training loss: 0.00001925
INFO:root:[276,   750] training loss: 0.00032441
INFO:root:[276,   800] training loss: 0.00048552
INFO:root:[276,   850] training loss: 0.00055844
INFO:root:[276,   900] training loss: 0.00502154
INFO:root:[276,   950] training loss: 0.00140612
INFO:root:[276,  1000] training loss: 0.00002844
INFO:root:[276,  1050] training loss: 0.00002212
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch276
INFO:root:[277,    50] training loss: 0.00811153
INFO:root:[277,   100] training loss: 0.00758551
INFO:root:[277,   150] training loss: 0.00774116
INFO:root:[277,   200] training loss: 0.00701292
INFO:root:[277,   250] training loss: 0.00646774
INFO:root:[277,   300] training loss: 0.00798441
INFO:root:[277,   350] training loss: 0.00683452
INFO:root:[277,   400] training loss: 0.00001634
INFO:root:[277,   450] training loss: 0.00002242
INFO:root:[277,   500] training loss: 0.00003027
INFO:root:[277,   550] training loss: 0.00032836
INFO:root:[277,   600] training loss: 0.00014119
INFO:root:[277,   650] training loss: 0.00002860
INFO:root:[277,   700] training loss: 0.00002041
INFO:root:[277,   750] training loss: 0.00031454
INFO:root:[277,   800] training loss: 0.00046645
INFO:root:[277,   850] training loss: 0.00052519
INFO:root:[277,   900] training loss: 0.00441820
INFO:root:[277,   950] training loss: 0.00146583
INFO:root:[277,  1000] training loss: 0.00002992
INFO:root:[277,  1050] training loss: 0.00001858
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch277
INFO:root:[278,    50] training loss: 0.00921822
INFO:root:[278,   100] training loss: 0.00787056
INFO:root:[278,   150] training loss: 0.00823676
INFO:root:[278,   200] training loss: 0.00717585
INFO:root:[278,   250] training loss: 0.00630895
INFO:root:[278,   300] training loss: 0.00789233
INFO:root:[278,   350] training loss: 0.00678612
INFO:root:[278,   400] training loss: 0.00001583
INFO:root:[278,   450] training loss: 0.00001699
INFO:root:[278,   500] training loss: 0.00002535
INFO:root:[278,   550] training loss: 0.00027990
INFO:root:[278,   600] training loss: 0.00015390
INFO:root:[278,   650] training loss: 0.00001976
INFO:root:[278,   700] training loss: 0.00003175
INFO:root:[278,   750] training loss: 0.00029766
INFO:root:[278,   800] training loss: 0.00045633
INFO:root:[278,   850] training loss: 0.00063696
INFO:root:[278,   900] training loss: 0.00490587
INFO:root:[278,   950] training loss: 0.00149962
INFO:root:[278,  1000] training loss: 0.00002800
INFO:root:[278,  1050] training loss: 0.00002284
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch278
INFO:root:[279,    50] training loss: 0.00816367
INFO:root:[279,   100] training loss: 0.00762240
INFO:root:[279,   150] training loss: 0.00779858
INFO:root:[279,   200] training loss: 0.00699192
INFO:root:[279,   250] training loss: 0.00673666
INFO:root:[279,   300] training loss: 0.00782387
INFO:root:[279,   350] training loss: 0.00676272
INFO:root:[279,   400] training loss: 0.00001442
INFO:root:[279,   450] training loss: 0.00001804
INFO:root:[279,   500] training loss: 0.00002337
INFO:root:[279,   550] training loss: 0.00025419
INFO:root:[279,   600] training loss: 0.00011062
INFO:root:[279,   650] training loss: 0.00002141
INFO:root:[279,   700] training loss: 0.00002421
INFO:root:[279,   750] training loss: 0.00037357
INFO:root:[279,   800] training loss: 0.00041793
INFO:root:[279,   850] training loss: 0.00049832
INFO:root:[279,   900] training loss: 0.00517438
INFO:root:[279,   950] training loss: 0.00144717
INFO:root:[279,  1000] training loss: 0.00003203
INFO:root:[279,  1050] training loss: 0.00004144
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch279
INFO:root:[280,    50] training loss: 0.00825195
INFO:root:[280,   100] training loss: 0.00739476
INFO:root:[280,   150] training loss: 0.00791392
INFO:root:[280,   200] training loss: 0.00680861
INFO:root:[280,   250] training loss: 0.00669000
INFO:root:[280,   300] training loss: 0.00824145
INFO:root:[280,   350] training loss: 0.00638429
INFO:root:[280,   400] training loss: 0.00001666
INFO:root:[280,   450] training loss: 0.00001868
INFO:root:[280,   500] training loss: 0.00002724
INFO:root:[280,   550] training loss: 0.00027276
INFO:root:[280,   600] training loss: 0.00017951
INFO:root:[280,   650] training loss: 0.00002139
INFO:root:[280,   700] training loss: 0.00002753
INFO:root:[280,   750] training loss: 0.00043569
INFO:root:[280,   800] training loss: 0.00052317
INFO:root:[280,   850] training loss: 0.00052189
INFO:root:[280,   900] training loss: 0.00507642
INFO:root:[280,   950] training loss: 0.00154970
INFO:root:[280,  1000] training loss: 0.00002784
INFO:root:[280,  1050] training loss: 0.00002064
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch280
INFO:root:[281,    50] training loss: 0.00831475
INFO:root:[281,   100] training loss: 0.00753414
INFO:root:[281,   150] training loss: 0.00782784
INFO:root:[281,   200] training loss: 0.00662776
INFO:root:[281,   250] training loss: 0.00621373
INFO:root:[281,   300] training loss: 0.00893128
INFO:root:[281,   350] training loss: 0.00668214
INFO:root:[281,   400] training loss: 0.00001639
INFO:root:[281,   450] training loss: 0.00001915
INFO:root:[281,   500] training loss: 0.00002854
INFO:root:[281,   550] training loss: 0.00029018
INFO:root:[281,   600] training loss: 0.00013791
INFO:root:[281,   650] training loss: 0.00002127
INFO:root:[281,   700] training loss: 0.00002017
INFO:root:[281,   750] training loss: 0.00034900
INFO:root:[281,   800] training loss: 0.00049044
INFO:root:[281,   850] training loss: 0.00042528
INFO:root:[281,   900] training loss: 0.00511736
INFO:root:[281,   950] training loss: 0.00135488
INFO:root:[281,  1000] training loss: 0.00003130
INFO:root:[281,  1050] training loss: 0.00002253
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch281
INFO:root:[282,    50] training loss: 0.01023186
INFO:root:[282,   100] training loss: 0.00733170
INFO:root:[282,   150] training loss: 0.00773293
INFO:root:[282,   200] training loss: 0.00679091
INFO:root:[282,   250] training loss: 0.00658408
INFO:root:[282,   300] training loss: 0.00800967
INFO:root:[282,   350] training loss: 0.00744098
INFO:root:[282,   400] training loss: 0.00001696
INFO:root:[282,   450] training loss: 0.00001809
INFO:root:[282,   500] training loss: 0.00002756
INFO:root:[282,   550] training loss: 0.00029283
INFO:root:[282,   600] training loss: 0.00013671
INFO:root:[282,   650] training loss: 0.00002028
INFO:root:[282,   700] training loss: 0.00002508
INFO:root:[282,   750] training loss: 0.00036429
INFO:root:[282,   800] training loss: 0.00054022
INFO:root:[282,   850] training loss: 0.00055505
INFO:root:[282,   900] training loss: 0.00596476
INFO:root:[282,   950] training loss: 0.00143242
INFO:root:[282,  1000] training loss: 0.00003361
INFO:root:[282,  1050] training loss: 0.00002255
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch282
INFO:root:[283,    50] training loss: 0.00824272
INFO:root:[283,   100] training loss: 0.00778510
INFO:root:[283,   150] training loss: 0.00855305
INFO:root:[283,   200] training loss: 0.00693000
INFO:root:[283,   250] training loss: 0.00625912
INFO:root:[283,   300] training loss: 0.00782765
INFO:root:[283,   350] training loss: 0.00667316
INFO:root:[283,   400] training loss: 0.00001694
INFO:root:[283,   450] training loss: 0.00001953
INFO:root:[283,   500] training loss: 0.00002296
INFO:root:[283,   550] training loss: 0.00024871
INFO:root:[283,   600] training loss: 0.00018184
INFO:root:[283,   650] training loss: 0.00001857
INFO:root:[283,   700] training loss: 0.00002081
INFO:root:[283,   750] training loss: 0.00032438
INFO:root:[283,   800] training loss: 0.00038746
INFO:root:[283,   850] training loss: 0.00055134
INFO:root:[283,   900] training loss: 0.00486233
INFO:root:[283,   950] training loss: 0.00121458
INFO:root:[283,  1000] training loss: 0.00009157
INFO:root:[283,  1050] training loss: 0.00002084
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch283
INFO:root:[284,    50] training loss: 0.00817439
INFO:root:[284,   100] training loss: 0.00737289
INFO:root:[284,   150] training loss: 0.00810332
INFO:root:[284,   200] training loss: 0.00671266
INFO:root:[284,   250] training loss: 0.00643840
INFO:root:[284,   300] training loss: 0.00789698
INFO:root:[284,   350] training loss: 0.00658280
INFO:root:[284,   400] training loss: 0.00001629
INFO:root:[284,   450] training loss: 0.00001984
INFO:root:[284,   500] training loss: 0.00002416
INFO:root:[284,   550] training loss: 0.00027068
INFO:root:[284,   600] training loss: 0.00010830
INFO:root:[284,   650] training loss: 0.00002297
INFO:root:[284,   700] training loss: 0.00001766
INFO:root:[284,   750] training loss: 0.00033894
INFO:root:[284,   800] training loss: 0.00047675
INFO:root:[284,   850] training loss: 0.00072128
INFO:root:[284,   900] training loss: 0.00488204
INFO:root:[284,   950] training loss: 0.00144454
INFO:root:[284,  1000] training loss: 0.00002858
INFO:root:[284,  1050] training loss: 0.00002295
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch284
INFO:root:[285,    50] training loss: 0.00870258
INFO:root:[285,   100] training loss: 0.00764647
INFO:root:[285,   150] training loss: 0.00756860
INFO:root:[285,   200] training loss: 0.00690435
INFO:root:[285,   250] training loss: 0.00650125
INFO:root:[285,   300] training loss: 0.00766425
INFO:root:[285,   350] training loss: 0.00659593
INFO:root:[285,   400] training loss: 0.00001633
INFO:root:[285,   450] training loss: 0.00001690
INFO:root:[285,   500] training loss: 0.00002702
INFO:root:[285,   550] training loss: 0.00031943
INFO:root:[285,   600] training loss: 0.00014525
INFO:root:[285,   650] training loss: 0.00002611
INFO:root:[285,   700] training loss: 0.00002158
INFO:root:[285,   750] training loss: 0.00026386
INFO:root:[285,   800] training loss: 0.00053935
INFO:root:[285,   850] training loss: 0.00071429
INFO:root:[285,   900] training loss: 0.00519605
INFO:root:[285,   950] training loss: 0.00138916
INFO:root:[285,  1000] training loss: 0.00004101
INFO:root:[285,  1050] training loss: 0.00003192
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch285
INFO:root:[286,    50] training loss: 0.00826616
INFO:root:[286,   100] training loss: 0.00721314
INFO:root:[286,   150] training loss: 0.00816494
INFO:root:[286,   200] training loss: 0.00689711
INFO:root:[286,   250] training loss: 0.00672266
INFO:root:[286,   300] training loss: 0.00780603
INFO:root:[286,   350] training loss: 0.00648512
INFO:root:[286,   400] training loss: 0.00001351
INFO:root:[286,   450] training loss: 0.00001587
INFO:root:[286,   500] training loss: 0.00002715
INFO:root:[286,   550] training loss: 0.00025845
INFO:root:[286,   600] training loss: 0.00014991
INFO:root:[286,   650] training loss: 0.00002576
INFO:root:[286,   700] training loss: 0.00002223
INFO:root:[286,   750] training loss: 0.00034595
INFO:root:[286,   800] training loss: 0.00046980
INFO:root:[286,   850] training loss: 0.00043984
INFO:root:[286,   900] training loss: 0.00539880
INFO:root:[286,   950] training loss: 0.00136042
INFO:root:[286,  1000] training loss: 0.00003405
INFO:root:[286,  1050] training loss: 0.00001983
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch286
INFO:root:[287,    50] training loss: 0.00796878
INFO:root:[287,   100] training loss: 0.00733491
INFO:root:[287,   150] training loss: 0.00814343
INFO:root:[287,   200] training loss: 0.00714494
INFO:root:[287,   250] training loss: 0.00664570
INFO:root:[287,   300] training loss: 0.00767363
INFO:root:[287,   350] training loss: 0.00653888
INFO:root:[287,   400] training loss: 0.00001640
INFO:root:[287,   450] training loss: 0.00001590
INFO:root:[287,   500] training loss: 0.00002526
INFO:root:[287,   550] training loss: 0.00026018
INFO:root:[287,   600] training loss: 0.00016940
INFO:root:[287,   650] training loss: 0.00002213
INFO:root:[287,   700] training loss: 0.00002189
INFO:root:[287,   750] training loss: 0.00041213
INFO:root:[287,   800] training loss: 0.00037621
INFO:root:[287,   850] training loss: 0.00047372
INFO:root:[287,   900] training loss: 0.00545379
INFO:root:[287,   950] training loss: 0.00142620
INFO:root:[287,  1000] training loss: 0.00003197
INFO:root:[287,  1050] training loss: 0.00002355
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch287
INFO:root:[288,    50] training loss: 0.00839076
INFO:root:[288,   100] training loss: 0.00727583
INFO:root:[288,   150] training loss: 0.00795570
INFO:root:[288,   200] training loss: 0.00707658
INFO:root:[288,   250] training loss: 0.00654004
INFO:root:[288,   300] training loss: 0.00954860
INFO:root:[288,   350] training loss: 0.00653239
INFO:root:[288,   400] training loss: 0.00001567
INFO:root:[288,   450] training loss: 0.00001910
INFO:root:[288,   500] training loss: 0.00002741
INFO:root:[288,   550] training loss: 0.00026648
INFO:root:[288,   600] training loss: 0.00014483
INFO:root:[288,   650] training loss: 0.00002248
INFO:root:[288,   700] training loss: 0.00002116
INFO:root:[288,   750] training loss: 0.00023579
INFO:root:[288,   800] training loss: 0.00045191
INFO:root:[288,   850] training loss: 0.00057438
INFO:root:[288,   900] training loss: 0.00515777
INFO:root:[288,   950] training loss: 0.00170289
INFO:root:[288,  1000] training loss: 0.00004959
INFO:root:[288,  1050] training loss: 0.00001967
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch288
INFO:root:[289,    50] training loss: 0.00829934
INFO:root:[289,   100] training loss: 0.00738924
INFO:root:[289,   150] training loss: 0.00764080
INFO:root:[289,   200] training loss: 0.00695051
INFO:root:[289,   250] training loss: 0.00877922
INFO:root:[289,   300] training loss: 0.00787679
INFO:root:[289,   350] training loss: 0.00716260
INFO:root:[289,   400] training loss: 0.00001921
INFO:root:[289,   450] training loss: 0.00001746
INFO:root:[289,   500] training loss: 0.00003189
INFO:root:[289,   550] training loss: 0.00027776
INFO:root:[289,   600] training loss: 0.00014800
INFO:root:[289,   650] training loss: 0.00001871
INFO:root:[289,   700] training loss: 0.00002187
INFO:root:[289,   750] training loss: 0.00039422
INFO:root:[289,   800] training loss: 0.00049370
INFO:root:[289,   850] training loss: 0.00044285
INFO:root:[289,   900] training loss: 0.00481496
INFO:root:[289,   950] training loss: 0.00152958
INFO:root:[289,  1000] training loss: 0.00003117
INFO:root:[289,  1050] training loss: 0.00002221
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch289
INFO:root:[290,    50] training loss: 0.00839506
INFO:root:[290,   100] training loss: 0.00761272
INFO:root:[290,   150] training loss: 0.00761951
INFO:root:[290,   200] training loss: 0.00685493
INFO:root:[290,   250] training loss: 0.00664365
INFO:root:[290,   300] training loss: 0.00815056
INFO:root:[290,   350] training loss: 0.00597467
INFO:root:[290,   400] training loss: 0.00001890
INFO:root:[290,   450] training loss: 0.00001514
INFO:root:[290,   500] training loss: 0.00003180
INFO:root:[290,   550] training loss: 0.00027813
INFO:root:[290,   600] training loss: 0.00016406
INFO:root:[290,   650] training loss: 0.00002374
INFO:root:[290,   700] training loss: 0.00002105
INFO:root:[290,   750] training loss: 0.00031435
INFO:root:[290,   800] training loss: 0.00040354
INFO:root:[290,   850] training loss: 0.00043991
INFO:root:[290,   900] training loss: 0.00487455
INFO:root:[290,   950] training loss: 0.00157812
INFO:root:[290,  1000] training loss: 0.00003568
INFO:root:[290,  1050] training loss: 0.00001991
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch290
INFO:root:[291,    50] training loss: 0.00871711
INFO:root:[291,   100] training loss: 0.00791592
INFO:root:[291,   150] training loss: 0.00746238
INFO:root:[291,   200] training loss: 0.00683873
INFO:root:[291,   250] training loss: 0.00672193
INFO:root:[291,   300] training loss: 0.00879339
INFO:root:[291,   350] training loss: 0.00639541
INFO:root:[291,   400] training loss: 0.00001622
INFO:root:[291,   450] training loss: 0.00001825
INFO:root:[291,   500] training loss: 0.00002798
INFO:root:[291,   550] training loss: 0.00029971
INFO:root:[291,   600] training loss: 0.00017585
INFO:root:[291,   650] training loss: 0.00002319
INFO:root:[291,   700] training loss: 0.00002739
INFO:root:[291,   750] training loss: 0.00033312
INFO:root:[291,   800] training loss: 0.00043057
INFO:root:[291,   850] training loss: 0.00054916
INFO:root:[291,   900] training loss: 0.00576093
INFO:root:[291,   950] training loss: 0.00151947
INFO:root:[291,  1000] training loss: 0.00002815
INFO:root:[291,  1050] training loss: 0.00001995
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch291
INFO:root:[292,    50] training loss: 0.00801536
INFO:root:[292,   100] training loss: 0.00740914
INFO:root:[292,   150] training loss: 0.00816679
INFO:root:[292,   200] training loss: 0.00678003
INFO:root:[292,   250] training loss: 0.00668226
INFO:root:[292,   300] training loss: 0.00831177
INFO:root:[292,   350] training loss: 0.00622563
INFO:root:[292,   400] training loss: 0.00001713
INFO:root:[292,   450] training loss: 0.00003348
INFO:root:[292,   500] training loss: 0.00002615
INFO:root:[292,   550] training loss: 0.00031250
INFO:root:[292,   600] training loss: 0.00019030
INFO:root:[292,   650] training loss: 0.00002057
INFO:root:[292,   700] training loss: 0.00002317
INFO:root:[292,   750] training loss: 0.00031502
INFO:root:[292,   800] training loss: 0.00053497
INFO:root:[292,   850] training loss: 0.00052830
INFO:root:[292,   900] training loss: 0.00460479
INFO:root:[292,   950] training loss: 0.00161654
INFO:root:[292,  1000] training loss: 0.00002813
INFO:root:[292,  1050] training loss: 0.00002473
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch292
INFO:root:[293,    50] training loss: 0.00801030
INFO:root:[293,   100] training loss: 0.00760268
INFO:root:[293,   150] training loss: 0.00752279
INFO:root:[293,   200] training loss: 0.00679023
INFO:root:[293,   250] training loss: 0.00662888
INFO:root:[293,   300] training loss: 0.00758487
INFO:root:[293,   350] training loss: 0.00632698
INFO:root:[293,   400] training loss: 0.00002044
INFO:root:[293,   450] training loss: 0.00001513
INFO:root:[293,   500] training loss: 0.00002644
INFO:root:[293,   550] training loss: 0.00025070
INFO:root:[293,   600] training loss: 0.00017460
INFO:root:[293,   650] training loss: 0.00001977
INFO:root:[293,   700] training loss: 0.00002069
INFO:root:[293,   750] training loss: 0.00033601
INFO:root:[293,   800] training loss: 0.00051215
INFO:root:[293,   850] training loss: 0.00042868
INFO:root:[293,   900] training loss: 0.00503522
INFO:root:[293,   950] training loss: 0.00145202
INFO:root:[293,  1000] training loss: 0.00003264
INFO:root:[293,  1050] training loss: 0.00004355
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch293
INFO:root:[294,    50] training loss: 0.00883219
INFO:root:[294,   100] training loss: 0.00813670
INFO:root:[294,   150] training loss: 0.00836864
INFO:root:[294,   200] training loss: 0.00679245
INFO:root:[294,   250] training loss: 0.00656203
INFO:root:[294,   300] training loss: 0.00811755
INFO:root:[294,   350] training loss: 0.00853512
INFO:root:[294,   400] training loss: 0.00001617
INFO:root:[294,   450] training loss: 0.00001945
INFO:root:[294,   500] training loss: 0.00002407
INFO:root:[294,   550] training loss: 0.00028995
INFO:root:[294,   600] training loss: 0.00015781
INFO:root:[294,   650] training loss: 0.00002254
INFO:root:[294,   700] training loss: 0.00002677
INFO:root:[294,   750] training loss: 0.00026248
INFO:root:[294,   800] training loss: 0.00050373
INFO:root:[294,   850] training loss: 0.00049298
INFO:root:[294,   900] training loss: 0.00550623
INFO:root:[294,   950] training loss: 0.00152121
INFO:root:[294,  1000] training loss: 0.00002791
INFO:root:[294,  1050] training loss: 0.00002212
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch294
INFO:root:[295,    50] training loss: 0.00841936
INFO:root:[295,   100] training loss: 0.00761203
INFO:root:[295,   150] training loss: 0.00744364
INFO:root:[295,   200] training loss: 0.00688904
INFO:root:[295,   250] training loss: 0.00675136
INFO:root:[295,   300] training loss: 0.00787507
INFO:root:[295,   350] training loss: 0.00623923
INFO:root:[295,   400] training loss: 0.00001469
INFO:root:[295,   450] training loss: 0.00002065
INFO:root:[295,   500] training loss: 0.00003379
INFO:root:[295,   550] training loss: 0.00028645
INFO:root:[295,   600] training loss: 0.00013870
INFO:root:[295,   650] training loss: 0.00001928
INFO:root:[295,   700] training loss: 0.00002080
INFO:root:[295,   750] training loss: 0.00031029
INFO:root:[295,   800] training loss: 0.00047814
INFO:root:[295,   850] training loss: 0.00057002
INFO:root:[295,   900] training loss: 0.00504059
INFO:root:[295,   950] training loss: 0.00156737
INFO:root:[295,  1000] training loss: 0.00002794
INFO:root:[295,  1050] training loss: 0.00002257
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch295
INFO:root:[296,    50] training loss: 0.00804331
INFO:root:[296,   100] training loss: 0.00733707
INFO:root:[296,   150] training loss: 0.00767035
INFO:root:[296,   200] training loss: 0.00730938
INFO:root:[296,   250] training loss: 0.00609186
INFO:root:[296,   300] training loss: 0.00765937
INFO:root:[296,   350] training loss: 0.00634694
INFO:root:[296,   400] training loss: 0.00001849
INFO:root:[296,   450] training loss: 0.00002628
INFO:root:[296,   500] training loss: 0.00002493
INFO:root:[296,   550] training loss: 0.00023925
INFO:root:[296,   600] training loss: 0.00016540
INFO:root:[296,   650] training loss: 0.00002576
INFO:root:[296,   700] training loss: 0.00002994
INFO:root:[296,   750] training loss: 0.00038593
INFO:root:[296,   800] training loss: 0.00062810
INFO:root:[296,   850] training loss: 0.00050449
INFO:root:[296,   900] training loss: 0.00574170
INFO:root:[296,   950] training loss: 0.00151937
INFO:root:[296,  1000] training loss: 0.00003273
INFO:root:[296,  1050] training loss: 0.00001981
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch296
INFO:root:[297,    50] training loss: 0.00790564
INFO:root:[297,   100] training loss: 0.00754317
INFO:root:[297,   150] training loss: 0.00816473
INFO:root:[297,   200] training loss: 0.00663423
INFO:root:[297,   250] training loss: 0.00702860
INFO:root:[297,   300] training loss: 0.00799192
INFO:root:[297,   350] training loss: 0.00794531
INFO:root:[297,   400] training loss: 0.00002166
INFO:root:[297,   450] training loss: 0.00001975
INFO:root:[297,   500] training loss: 0.00002936
INFO:root:[297,   550] training loss: 0.00025711
INFO:root:[297,   600] training loss: 0.00013193
INFO:root:[297,   650] training loss: 0.00002009
INFO:root:[297,   700] training loss: 0.00001859
INFO:root:[297,   750] training loss: 0.00035257
INFO:root:[297,   800] training loss: 0.00045444
INFO:root:[297,   850] training loss: 0.00049106
INFO:root:[297,   900] training loss: 0.00543619
INFO:root:[297,   950] training loss: 0.00136208
INFO:root:[297,  1000] training loss: 0.00006139
INFO:root:[297,  1050] training loss: 0.00002120
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch297
INFO:root:[298,    50] training loss: 0.00845716
INFO:root:[298,   100] training loss: 0.00731809
INFO:root:[298,   150] training loss: 0.00862781
INFO:root:[298,   200] training loss: 0.00729324
INFO:root:[298,   250] training loss: 0.00624546
INFO:root:[298,   300] training loss: 0.00800138
INFO:root:[298,   350] training loss: 0.00615308
INFO:root:[298,   400] training loss: 0.00001802
INFO:root:[298,   450] training loss: 0.00001724
INFO:root:[298,   500] training loss: 0.00002424
INFO:root:[298,   550] training loss: 0.00029965
INFO:root:[298,   600] training loss: 0.00021517
INFO:root:[298,   650] training loss: 0.00002335
INFO:root:[298,   700] training loss: 0.00001878
INFO:root:[298,   750] training loss: 0.00032211
INFO:root:[298,   800] training loss: 0.00049888
INFO:root:[298,   850] training loss: 0.00057990
INFO:root:[298,   900] training loss: 0.00535928
INFO:root:[298,   950] training loss: 0.00158368
INFO:root:[298,  1000] training loss: 0.00003367
INFO:root:[298,  1050] training loss: 0.00001919
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch298
INFO:root:[299,    50] training loss: 0.00820414
INFO:root:[299,   100] training loss: 0.00792210
INFO:root:[299,   150] training loss: 0.00802361
INFO:root:[299,   200] training loss: 0.00731284
INFO:root:[299,   250] training loss: 0.00617890
INFO:root:[299,   300] training loss: 0.00802706
INFO:root:[299,   350] training loss: 0.00693484
INFO:root:[299,   400] training loss: 0.00001916
INFO:root:[299,   450] training loss: 0.00001720
INFO:root:[299,   500] training loss: 0.00003012
INFO:root:[299,   550] training loss: 0.00032883
INFO:root:[299,   600] training loss: 0.00015251
INFO:root:[299,   650] training loss: 0.00002035
INFO:root:[299,   700] training loss: 0.00002061
INFO:root:[299,   750] training loss: 0.00032354
INFO:root:[299,   800] training loss: 0.00045602
INFO:root:[299,   850] training loss: 0.00058260
INFO:root:[299,   900] training loss: 0.00656652
INFO:root:[299,   950] training loss: 0.00144105
INFO:root:[299,  1000] training loss: 0.00003086
INFO:root:[299,  1050] training loss: 0.00002040
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:epoch299
INFO:root:[300,    50] training loss: 0.00835007
INFO:root:[300,   100] training loss: 0.00848838
INFO:root:[300,   150] training loss: 0.00745102
INFO:root:[300,   200] training loss: 0.00728497
INFO:root:[300,   250] training loss: 0.00696018
INFO:root:[300,   300] training loss: 0.00803059
INFO:root:[300,   350] training loss: 0.00653404
INFO:root:[300,   400] training loss: 0.00001717
INFO:root:[300,   450] training loss: 0.00001801
INFO:root:[300,   500] training loss: 0.00003073
INFO:root:[300,   550] training loss: 0.00030534
INFO:root:[300,   600] training loss: 0.00019000
INFO:root:[300,   650] training loss: 0.00002845
INFO:root:[300,   700] training loss: 0.00001971
INFO:root:[300,   750] training loss: 0.00032077
INFO:root:[300,   800] training loss: 0.00042936
INFO:root:[300,   850] training loss: 0.00060563
INFO:root:[300,   900] training loss: 0.00733407
INFO:root:[300,   950] training loss: 0.00159768
INFO:root:[300,  1000] training loss: 0.00004831
INFO:root:[300,  1050] training loss: 0.00002874
INFO:root:              precision    recall  f1-score   support

   Metaphase     0.4000    0.6667    0.5000         3
    Prophase     0.9162    0.8380    0.8753      1722
           S     0.8500    0.8345    0.8422      1039
   Telophase     0.5455    0.6000    0.5714        10
          G2     0.4766    0.6892    0.5635        74
          G1     0.6576    0.7397    0.6963      1018
    Anaphase     0.6667    1.0000    0.8000         6

    accuracy                         0.8079      3872
   macro avg     0.6447    0.7669    0.6927      3872
weighted avg     0.8203    0.8079    0.8122      3872

INFO:root:Finished Training
INFO:root:Accuracy of the network on the 6454 test images: 82 %
